[{"authors":null,"categories":null,"content":"I have included a bunch of extra resources and guides related to causal inference, program evaluation, R, data, and other relevant topics. Enjoy!\n","date":1609718400,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1609718400,"objectID":"e5656835a9d5f0c69a44e56dc2b58101","permalink":"/resource/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/resource/","section":"resource","summary":"I have included a bunch of extra resources and guides related to causal inference, program evaluation, R, data, and other relevant topics. Enjoy!","tags":null,"title":"Helpful resources","type":"docs"},{"authors":null,"categories":null,"content":"Each class session has a set of required readings that you should complete before watching the lecture.\nEvery class session also has a YouTube playlist of short recorded videos for each of the lecture sections. The lecture slides are special HTML files made with the R package xaringan (R can do so much!). On each class session page you\u0026rsquo;ll see buttons for opening the presentation in a new tab or for downloading a PDF of the slides in case you want to print them or store them on your computer:\n View all slides in new window  Download PDF of all slides\nThe slides are also embedded on each page. You can click in the slides and navigate through them with ← and →. If you type ? (or shift + /) while viewing the slides you can see a list of slide-specific commands (like f for fullscreen or p for presenter mode if you want to see my notes).\n","date":1636329600,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1636329600,"objectID":"8899c927408853efa5f455eaa551e047","permalink":"/content/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/content/","section":"content","summary":"Each class session has a set of required readings that you should complete before watching the lecture.\nEvery class session also has a YouTube playlist of short recorded videos for each of the lecture sections.","tags":null,"title":"Clases, prácticos y materiales","type":"docs"},{"authors":null,"categories":null,"content":"The main goals of this class are to help you design, critique, code, and run rigorous, valid, and feasible evaluations of public sector programs. Each type of assignment in this class is designed to help you achieve one or more of these goals.\nWeekly check-in Every week, after you finish working through the content, I want to hear about what you learned and what questions you still have. To facilitate this, and to encourage engagement with the course content, you\u0026rsquo;ll need to fill out a short response on iCollege. This should be ≈150 words. That\u0026rsquo;s fairly short: there are ≈250 words on a typical double-spaced page in Microsoft Word (500 when single-spaced).\nYou should answer these two questions each week:\n What was the most exciting thing you learned from the session? Why? What was the muddiest thing from the session this week? What are you still wondering about?  I will grade these check-ins using a check system:\n ✔+: (11.5 points (115%) in gradebook) Response shows phenomenal thought and engagement with the course content. I will not assign these often. ✔: (10 points (100%) in gradebook) Response is thoughtful, well-written, and shows engagement with the course content. This is the expected level of performance. ✔−: (5 points (50%) in gradebook) Response is hastily composed, too short, and/or only cursorily engages with the course content. This grade signals that you need to improve next time. I will hopefully not assign these often.  Notice that is essentially a pass/fail or completion-based system. I\u0026rsquo;m not grading your writing ability, I\u0026rsquo;m not counting the exact number of words you\u0026rsquo;re writing, and I\u0026rsquo;m not looking for encyclopedic citations of every single reading to prove that you did indeed read everything. I\u0026rsquo;m looking for thoughtful engagement, that\u0026rsquo;s all. Do good work and you\u0026rsquo;ll get a ✓.\nYou will submit these responses via iCollege.\nProblem sets To practice writing R code, running inferential models, and thinking about causation, you will complete a series of problem sets.\nThere are 9 problem sets on the schedule. I will keep the highest grades for 8 of them. That is, I will drop the lowest score (even if it\u0026rsquo;s a zero). This means you can skip one of the problem sets. You need to show that you made a good faith effort to work each question. I will not grade these in detail. The problem sets will be graded using a check system:\n ✔+: (33 points (110%) in gradebook) Assignment is 100% completed. Every question was attempted and answered, and most answers are correct. Document is clean and easy to follow. Work is exceptional. I will not assign these often. ✔: (30 points (100%) in gradebook) Assignment is 70–99% complete and most answers are correct. This is the expected level of performance. ✔−: (15 points (50%) in gradebook) Assignment is less than 70% complete and/or most answers are incorrect. This indicates that you need to improve next time. I will hopefully not asisgn these often.  You may (and should!) work together on the problem sets, but you must turn in your own answers. You cannot work in groups of more than four people, and you must note who participated in the group in your assignment.\nEvaluation assignments For your final project, you will conduct a pre-registered evaluation of a social program using synthetic data. To (1) give you practice with the principles of program evaluation, research design, measurement, and causal diagrams, and (2) help you with the foundation of your final project, you will complete a set of four evaluation-related assignments.\nIdeally these will become major sections of your final project. However, there is no requirement that the programs you use in these assignments must be the same as the final project. If, through these assignments, you discover that your initially chosen program is too simple, too complex, too boring, etc., you can change at any time.\nThese assignments will be graded using a check system:\n ✔+: (33 points (110%) in gradebook) Assignment is 100% completed. Every question was attempted and answered, and most answers are correct. Document is clean and easy to follow. Work is exceptional. I will not assign these often. ✔: (30 points (100%) in gradebook) Assignment is 70–99% complete and most answers are correct. This is the expected level of performance. ✔−: (15 points (50%) in gradebook) Assignment is less than 70% complete and/or most answers are incorrect. This indicates that you need to improve next time. I will hopefully not asisgn these often.  Exams There will be two exams covering (1) program evaluation, design, and causation, and (2) the core statistical tools of program evaluation and causal inference.\nYou will take these exams online through iCollege. The exams will have a time limit, but you can use notes and readings and the Google. You must take the exams on your own though, and not talk to anyone about them.\nFinal project At the end of the course, you will demonstrate your knowledge of program evaluation and causal inference by completing a final project.\nComplete details for the final project are here.\nThere is no final exam. This project is your final exam.\n","date":1620000000,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1620000000,"objectID":"e18c399687bc0897ffd6503c7a1bbb8e","permalink":"/assignment/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/assignment/","section":"assignment","summary":"The main goals of this class are to help you design, critique, code, and run rigorous, valid, and feasible evaluations of public sector programs. Each type of assignment in this class is designed to help you achieve one or more of these goals.","tags":null,"title":"Assignment details","type":"docs"},{"authors":null,"categories":null,"content":"Evidence, causation, and evaluation You should understand…\n …the difference between experimental research and observational research …the sometimes conflicting roles of science and intuition in public administration and policy …the difference between the various types of evaluations and how they target specific parts of a logic model …the difference between identifying correlation (math) and identifying causation (philosophy and theory) …what it means for a relationship to be causal  Regression and inference You should understand…\n …the difference between correlation coefficients and regression coefficients …the difference between outcome/response/dependent and explanatory/predictor/independent variables …the two purposes of regression …what each of the components in a regression equation stand for, in both \u0026ldquo;flavors\u0026rdquo; of notation:  \\(y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\epsilon\\) for the statistical flavor \\(y = \\alpha + \\beta x_1 + \\gamma x_2 + \\epsilon\\) for the econometrics flavor   …how sliders and switches work as metaphors for regression coefficients …what it means to hold variables constant (or to control for variables) …the different elements of the grammar of graphics and be able to identify how variables are encoded in a graph (i.e. how columns in a dataset can be represented through x/y coordinates, through color, through size, through fill, etc.)  You should be able to…\n …write and interpret R code that calculates summary statistics for groups (i.e. group_by() %\u0026gt;% summarize()) …write and interpret R code that builds linear models (i.e. lm()) …interpret regression coefficients …interpret other regression diagnostics like \\(R^2\\) …use the %\u0026gt;% pipe in R to chain functions together …use ggplot() to visualize data  Helpful resources:\n  Garrett Grolemund and Hadley Wickham, R for Data Science  Kieran Healy, \u0026ldquo;How ggplot works,\u0026quot; chapter 3 in Data Visualization: A Practical Introduction  Theories of change and measurement You should understand…\n …how to describe a program\u0026rsquo;s theory of change …the difference between inputs, activities, outputs, and outcomes …the elements of a program\u0026rsquo;s impact theory: causes (activities) linked to effects (outcomes) …the elements of a program\u0026rsquo;s logic model: the explicit links between all its inputs, activities, outputs, and outcomes …the difference between implicit and articulated program theories …the purpose of smaller-scale mechanism testing …how indicators can be measured at different levels of abstraction …what makes an indicator a good indicator  You should be able to…\n …identify a program\u0026rsquo;s underlying theory based on its mission statement …draw a program impact theory chart that links activities to outcomes …draw a program logic model that links inputs to activities to outputs to outcomes …identify the most central elements of a potential outcome measurement  Counterfactuals and DAGs You should understand…\n …how a causal model encodes our understanding of a causal process …how to identify front door and back door paths between treatment/exposure and outcome …why we avoid closing front door paths …why we close back door paths …why adjusting for colliders can distort causal effects …the difference between logic models and DAGs …the difference between individual level causal effects, average treatment effects (ATE), conditional average treatment effect (CATE), average treatment on the treated effects (ATT), and average treatment on the untreated (ATU) …what the fundamental problem of causal inference is and how we can attempt to address it  You should be able to…\n …draw a possible DAG for a given causal relationship …identify all pathways between treatment/exposure and outcome …identify which nodes in the DAG need to be adjusted for (or closed) …identify colliders (which should not be adjusted for)  Helpful resources:\n  Malcom Barrett, \u0026ldquo;An Introduction to Directed Acyclic Graphs\u0026rdquo;  Malcom Barrett, \u0026ldquo;An Introduction to ggdag\u0026rdquo;  Judea Pearl, \u0026ldquo;A Crash Course in Good and Bad Control\u0026rdquo;: A quick summary of back doors, front doors, confounders, colliders, and when to control/not control for DAG nodes  Causal Inference Bootcamp, \u0026ldquo;Average Treatment Effects,\u0026quot; Duke University  Causal Inference Bootcamp, \u0026ldquo;Unit Level Effects,\u0026quot; Duke University  Causal Inference Bootcamp, \u0026ldquo;Conditional Average Treatment Effects,\u0026quot; Duke University  Causal Inference Bootcamp, \u0026ldquo;Counterfactuals,\u0026quot; Duke University  Neel Ocean, \u0026ldquo;Understanding Selection Bias\u0026rdquo;: explanation of how to identify selection bias from the ATT and the ATE, with an explanation of how ATE = ATT + selection bias under the potential outcomes framework  Paul Hünermund, \u0026ldquo;Sample Selection vs. Selection Into Treatment\u0026rdquo;  Threats to validity You should understand…\n …what it means when a study has internal validity and know how to identify the major threats to internal validity, including: omitted variable bias (selection and attrition), trend issues (maturation, secular trends, seasonality, testing, regression to the mean), study calibration issues (measurement error, time frame of study), and contamination issues (Hawthorne effects, John Henry effects, spillovers, and intervening events) …why selection bias is the most pernicious and difficult threat to internal validity and how we can account for it …what it means when a study has external validity …what it means when the measures used in a study have construct validity …what it means when the analysis used in a study has statistical conclusion validity  You should be able to…\n …identify existing and potential threats to validity in a study …suggest ways of addressing these threats  Helpful resources:\n Really, just google \u0026ldquo;threats to internal validity\u0026rdquo; or \u0026ldquo;threats to external validity\u0026rdquo; and you\u0026rsquo;ll find a billion different slide decks, articles, and lessons about these. They\u0026rsquo;re a pretty standard part of any research design class.  ","date":1601424000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1601424000,"objectID":"4a1429ce8efe85a9fbba7cfaf1ee88df","permalink":"/resource/exam1/","publishdate":"2020-09-30T00:00:00Z","relpermalink":"/resource/exam1/","section":"resource","summary":"Evidence, causation, and evaluation You should understand…\n …the difference between experimental research and observational research …the sometimes conflicting roles of science and intuition in public administration and policy …the difference between the various types of evaluations and how they target specific parts of a logic model …the difference between identifying correlation (math) and identifying causation (philosophy and theory) …what it means for a relationship to be causal  Regression and inference You should understand…","tags":null,"title":"Things you should know for Exam 1","type":"docs"},{"authors":null,"categories":null,"content":"You will do all of your work in this class with the open source (and free!) programming language R. You will use RStudio as the main program to access R. Think of R as an engine and RStudio as a car dashboard—R handles all the calculations and the actual statistics, while RStudio provides a nice interface for running R code.\nRStudio.cloud R is free, but it can sometimes be a pain to install and configure. To make life easier, you can (and should!) use the free RStudio.cloud service initially, which lets you run a full instance of RStudio in your web browser. This means you won\u0026rsquo;t have to install anything on your computer to get started with R! We will have a shared class workspace in RStudio.cloud that will let you quickly copy templates for labs and problem sets.\nGo to https://rstudio.cloud/ and create an account. You\u0026rsquo;ll receive a link to join the shared class workspace separately. If you don\u0026rsquo;t get this link, let me know and I will invite you.\nRStudio on your computer RStudio.cloud is convenient, but it can be slow and it is not designed to be able to handle larger datasets, more complicated analysis, or fancier graphics. Over the course of the semester, you should wean yourself off of RStudio.cloud and install all these things locally. This is also important if you want to customize fonts, since RStudio.cloud has extremely limited support for fonts other than Helvetica.\nHere\u0026rsquo;s how you install all these things\nInstall R First you need to install R itself (the engine).\n  Go to the CRAN (Collective R Archive Network)^[It\u0026rsquo;s a goofy name, but CRAN is where most R packages—and R itself—lives.] website: https://cran.r-project.org/\n  Click on \u0026ldquo;Download R for XXX\u0026rdquo;, where XXX is either Mac or Windows:\n  If you use macOS, scroll down to the first .pkg file in the list of files (in this picture, it\u0026rsquo;s R-4.0.0.pkg; as of right now, the current version is also 4.0.0) and download it.\n  If you use Windows, click \u0026ldquo;base\u0026rdquo; (or click on the bolded \u0026ldquo;install R for the first time\u0026rdquo; link) and download it.\n    Double click on the downloaded file (check your Downloads folder). Click yes through all the prompts to install like any other program.\n  If you use macOS, download and install XQuartz. You do not need to do this on Windows.\n  Install RStudio Next, you need to install RStudio, the nicer graphical user interface (GUI) for R (the dashboard). Once R and RStudio are both installed, you can ignore R and only use RStudio. RStudio will use R automatically and you won\u0026rsquo;t ever have to interact with it directly.\n  Go to the free download location on RStudio\u0026rsquo;s website: https://www.rstudio.com/products/rstudio/download/#download\n  The website should automatically detect your operating system (macOS or Windows) and show a big download button for it:\nIf not, scroll down a little to the large table and choose the version of RStudio that matches your operating system.\n  Double click on the downloaded file (again, check your Downloads folder). Click yes through all the prompts to install like any other program.\n  Double click on RStudio to run it (check your applications folder or start menu).\nInstall tidyverse R packages are easy to install with RStudio. Select the packages panel, click on \u0026ldquo;Install,\u0026rdquo; type the name of the package you want to install, and press enter.\nThis can sometimes be tedious when you\u0026rsquo;re installing lots of packages, though. The tidyverse, for instance, consists of dozens of packages (including ggplot2) that all work together. Rather than install each individually, you can install a single magical package and get them all at the same time.\nGo to the packages panel in RStudio, click on \u0026ldquo;Install,\u0026rdquo; type \u0026ldquo;tidyverse\u0026rdquo;, and press enter. You\u0026rsquo;ll see a bunch of output in the RStudio console as all the tidyverse packages are installed.\nNotice also that RStudio will generate a line of code for you and run it: install.packages(\u0026quot;tidyverse\u0026quot;). You can also just paste and run this instead of using the packages panel.\nInstall tinytex When you knit to PDF, R uses a special scientific typesetting program named LaTeX (pronounced \u0026ldquo;lay-tek\u0026rdquo; or \u0026ldquo;lah-tex\u0026rdquo;; for goofy nerdy reasons, the x is technically the \u0026ldquo;ch\u0026rdquo; sound in \u0026ldquo;Bach\u0026rdquo;, but most people just say it as \u0026ldquo;k\u0026rdquo;—saying \u0026ldquo;layteks\u0026rdquo; is frowned on for whatever reason).\nLaTeX is neat and makes pretty documents, but it\u0026rsquo;s a huge program—the macOS version, for instance, is nearly 4 GB! To make life easier, there\u0026rsquo;s an R package named tinytex that installs a minimal LaTeX program and that automatically deals with differences between macOS and Windows.\nHere\u0026rsquo;s how to install tinytex so you can knit to pretty PDFs:\n Use the Packages in panel in RStudio to install tinytex like you did above with tidyverse. Alternatively, run install.packages(\u0026quot;tinytex\u0026quot;) in the console. Run tinytex::install_tinytex() in the console. Wait for a bit while R downloads and installs everything you need. The end! You should now be able to knit to PDF.  ","date":1588291200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1588291200,"objectID":"c5e6c5f0d0baae071e282245bbba803c","permalink":"/resource/install/","publishdate":"2020-05-01T00:00:00Z","relpermalink":"/resource/install/","section":"resource","summary":"You will do all of your work in this class with the open source (and free!) programming language R. You will use RStudio as the main program to access R. Think of R as an engine and RStudio as a car dashboard—R handles all the calculations and the actual statistics, while RStudio provides a nice interface for running R code.","tags":null,"title":"Installing R, RStudio, tidyverse, and tinytex","type":"docs"},{"authors":null,"categories":null,"content":"Randomization  Understand why randomization is crucial for causal inference and counterfactuals Understand the process for analyzing a randomized controlled trial  Crucial resources:\n Readings, slides, and videos for randomization and matching Guide for RCTs Problem set 3 Task 1 in problem set 8  Matching and inverse probability weighting  Understand the intuition behind matching and inverse probability weighting Understand the process for adjusting for confounders and closing backdoors with both matching and inverse probability weighting  Crucial resources:\n Readings, slides, and videos for randomization and matching Guide for matching and inverse probability weighting Problem set 3 Task 2 in problem set 8  Difference-in-difference  Understand the intuition behind making causal inferences with difference-in-differences Understand the process for analyzing diff-in-diffs  Crucial resources:\n Readings, slides, and videos for diff-in-diff Guide for diff-in-diff Problem set 4 Problem set 5 Task 3 in problem set 8  Regression discontinuity  Understand the intuition behind making causal inferences with regression discontinuity Understand the process for analyzing regression discontinuities, both fuzzy and sharp Understand the difference between ATE and LATE  Crucial resources:\n Readings, slides, and videos for regression discontinuity I (sharp) Readings, slides, and videos for regression discontinuity II (fuzzy) Guide for sharp diff-in-diff Guide for fuzzy diff-in-diff Problem set 6 Task 4 in problem set 8  Instrumental variables  Understand the intuition behind using instruments for causal inference Understand the three characteristics of a good instrument Understand the process for analyzing data with instrumental variables and 2SLS Understand the difference between ATE and LATE  Crucial resources:\n Readings, slides, and videos for instrumental variables I Readings, slides, and videos for instrumental variables II Guide for instrumental variables Problem set 7 Task 5 in problem set 8  ","date":1605139200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1605139200,"objectID":"55763b3680f13e14f78659411d8e1ced","permalink":"/resource/exam2/","publishdate":"2020-11-12T00:00:00Z","relpermalink":"/resource/exam2/","section":"resource","summary":"Randomization  Understand why randomization is crucial for causal inference and counterfactuals Understand the process for analyzing a randomized controlled trial  Crucial resources:\n Readings, slides, and videos for randomization and matching Guide for RCTs Problem set 3 Task 1 in problem set 8  Matching and inverse probability weighting  Understand the intuition behind matching and inverse probability weighting Understand the process for adjusting for confounders and closing backdoors with both matching and inverse probability weighting  Crucial resources:","tags":null,"title":"Things you should know for Exam 2","type":"docs"},{"authors":null,"categories":null,"content":"Learning R I highly recommend subscribing to the R Weekly newsletter. This e-mail is sent every Monday and is full of helpful tutorials about how to do stuff with R.\nSearching for help with R on Google can sometimes be tricky because the program name is a single letter. Google is generally smart enough to figure out what you mean when you search for \u0026ldquo;r scatterplot\u0026rdquo;, but if it does struggle, try searching for \u0026ldquo;rstats\u0026rdquo; instead (e.g. \u0026ldquo;rstats scatterplot\u0026rdquo;).\nIf you use Twitter, post R-related questions and content with #rstats. The community there is exceptionally generous and helpful. Also check out StackOverflow (a Q\u0026amp;A site with hundreds of thousands of answers to all sorts of programming questions) and RStudio Community (a forum specifically designed for people using RStudio and the tidyverse (i.e. you)).\nThese resources are also really really helpful:\n R for Data Science: A free online book for learning the basics of R and the tidyverse. R and RStudio cheat sheets: A large collection of simple cheat sheets for RStudio, ggplot2, and other R-related things. Stat 545: Dr. Jenny Bryan at RStudio has an entire introductory course in R, visualization, and data analysis online. STA 112FS: Data Science: Dr. Mine Çetinkaya-Rundel at the University of Edinburgh / Duke University has an entire introductory course in R, visualization, and data science online. CSE 631: Principles \u0026amp; Practice of Data Visualization: Yet another introductory course for R and ggplot2 by Dr. Alison Presmanes Hill at RStudio.  R in the wild A popular (and increasingly standard) way for sharing your analyses and visualizations is to post an annotated explanation of your process somewhere online. RStudio allows you to publish knitted HTML files directly to RPubs, but you can also post your output to a blog or other type of website.^[If you want to be really fancy, you can use blogdown, which makes a complete website with R Markdown files. That\u0026rsquo;s actually how this site is built (see the source code). You can build your own site with this tutorial.] Reading these kinds of posts is one of the best ways to learn R, since they walk you through each step of the process and show the code and output.\nHere are some of the best examples I\u0026rsquo;ve come across:\n Text analysis of Trump\u0026rsquo;s tweets confirms he writes only the (angrier) Android half (with a follow-up) Bob Ross - Joy of Painting Bechdel analysis using the tidyverse: There are also a bunch of other examples using data from FiveThirtyEight. Sexism on the Silver Screen: Exploring film\u0026rsquo;s gender divide Comparison of Quentin Tarantino Movies by Box Office and the Bechdel Test Who came to vote in Utah\u0026rsquo;s caucuses? Health care indicators in Utah counties Song lyrics across the United States A decade (ish) of listening to Sigur Rós When is Tom peeping these days?: There are a also bunch of final projects from other R and data visualization classes here and here. Mapping Fall Foliage General (Attys) Distributions Disproving Approval  ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"fd33fdc527a88c760dba0f467bb3657d","permalink":"/resource/r/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/resource/r/","section":"resource","summary":"Learning R I highly recommend subscribing to the R Weekly newsletter. This e-mail is sent every Monday and is full of helpful tutorials about how to do stuff with R.","tags":null,"title":"R","type":"docs"},{"authors":null,"categories":null,"content":"R style conventions R is fairly forgiving about how you type code (unlike other languages like Python, where miscounting spaces can ruin your code!). All of these things will do exactly the same thing:\nmpg %\u0026gt;% filter(cty \u0026gt; 10, class == \u0026#34;compact\u0026#34;) mpg %\u0026gt;% filter(cty \u0026gt; 10, class == \u0026#34;compact\u0026#34;) mpg %\u0026gt;% filter(cty \u0026gt; 10, class == \u0026#34;compact\u0026#34;) mpg %\u0026gt;% filter(cty\u0026gt;10, class==\u0026#34;compact\u0026#34;) filter(mpg,cty\u0026gt;10,class==\u0026#34;compact\u0026#34;) mpg %\u0026gt;% filter(cty \u0026gt; 10, class == \u0026#34;compact\u0026#34;) filter ( mpg,cty\u0026gt;10, class==\u0026#34;compact\u0026#34; ) But you\u0026rsquo;ll notice that only a few of those iterations (the first three) are easily readable.\nTo help improve readability and make it easier to share code with others, there\u0026rsquo;s an unofficial style guide for writing R code. It\u0026rsquo;s fairly short and just has lots of examples of good and bad ways of writing code (naming variables, dealing with long lines, using proper indentation levels, etc.)—you should glance through it some time.\nRStudio has a built-in way of cleaning up your code. Select some code, press ctrl + i (on Windows) or ⌘ + i (on macOS), and R will reformat the code for you. It\u0026rsquo;s not always perfect, but it\u0026rsquo;s really helpful for getting indentation right without having to manually hit space a billion times.\nMain style things to pay attention to for this class  Important note: I won\u0026rsquo;t ever grade you on any of this! If you submit something like filter(mpg,cty\u0026gt;10,class==\u0026quot;compact\u0026quot;), I might recommend adding spaces, but it won\u0026rsquo;t affect your grade or points or anything.\n Spacing  See the \u0026ldquo;Spacing\u0026rdquo; section in the tidyverse style guide.\n Put spaces after commas (like in regular English):\n# Good filter(mpg, cty \u0026gt; 10) # Bad filter(mpg , cty \u0026gt; 10) filter(mpg ,cty \u0026gt; 10) filter(mpg,cty \u0026gt; 10) Put spaces around operators like +, -, \u0026gt;, =, etc.:\n# Good filter(mpg, cty \u0026gt; 10) # Bad filter(mpg, cty\u0026gt;10) filter(mpg, cty\u0026gt; 10) filter(mpg, cty \u0026gt;10) Don\u0026rsquo;t put spaces around parentheses that are parts of functions:\n# Good filter(mpg, cty \u0026gt; 10) # Bad filter (mpg, cty \u0026gt; 10) filter ( mpg, cty \u0026gt; 10) filter( mpg, cty \u0026gt; 10 ) Long lines  See the \u0026ldquo;Long lines\u0026rdquo; section in the tidyverse style guide.\n It\u0026rsquo;s generally good practice to not have really long lines of code. A good suggestion is to keep lines at a maximum of 80 characters. Instead of counting characters by hand (ew), in RStudio go to \u0026ldquo;Tools\u0026rdquo; \u0026gt; \u0026ldquo;Global Options\u0026rdquo; \u0026gt; \u0026ldquo;Code\u0026rdquo; \u0026gt; \u0026ldquo;Display\u0026rdquo; and check the box for \u0026ldquo;Show margin\u0026rdquo;. You should now see a really thin line indicating 80 characters. Again, you can go beyond this—that\u0026rsquo;s fine. It\u0026rsquo;s just good practice to avoid going too far past it.\nYou can add line breaks inside longer lines of code. Line breaks should come after commas, and things like function arguments should align within the function:\n# Good filter(mpg, cty \u0026gt; 10, class == \u0026#34;compact\u0026#34;) # Good filter(mpg, cty \u0026gt; 10, class == \u0026#34;compact\u0026#34;) # Good filter(mpg, cty \u0026gt; 10, class == \u0026#34;compact\u0026#34;) # Bad filter(mpg, cty \u0026gt; 10, class %in% c(\u0026#34;compact\u0026#34;, \u0026#34;pickup\u0026#34;, \u0026#34;midsize\u0026#34;, \u0026#34;subcompact\u0026#34;, \u0026#34;suv\u0026#34;, \u0026#34;2seater\u0026#34;, \u0026#34;minivan\u0026#34;)) # Good filter(mpg, cty \u0026gt; 10, class %in% c(\u0026#34;compact\u0026#34;, \u0026#34;pickup\u0026#34;, \u0026#34;midsize\u0026#34;, \u0026#34;subcompact\u0026#34;, \u0026#34;suv\u0026#34;, \u0026#34;2seater\u0026#34;, \u0026#34;minivan\u0026#34;)) Pipes (%\u0026gt;%) and ggplot layers (+) Put each layer of a ggplot plot on separate lines, with the + at the end of the line, indented with two spaces:\n# Good ggplot(mpg, aes(x = cty, y = hwy, color = class)) + geom_point() + geom_smooth() + theme_bw() # Bad ggplot(mpg, aes(x = cty, y = hwy, color = class)) + geom_point() + geom_smooth() + theme_bw() # Super bad ggplot(mpg, aes(x = cty, y = hwy, color = class)) + geom_point() + geom_smooth() + theme_bw() # Super bad and won\u0026#39;t even work ggplot(mpg, aes(x = cty, y = hwy, color = class)) + geom_point() + geom_smooth() + theme_bw() Put each step in a dplyr pipeline on separate lines, with the %\u0026gt;% at the end of the line, indented with two spaces:\n# Good mpg %\u0026gt;% filter(cty \u0026gt; 10) %\u0026gt;% group_by(class) %\u0026gt;% summarize(avg_hwy = mean(hwy)) # Bad mpg %\u0026gt;% filter(cty \u0026gt; 10) %\u0026gt;% group_by(class) %\u0026gt;% summarize(avg_hwy = mean(hwy)) # Super bad mpg %\u0026gt;% filter(cty \u0026gt; 10) %\u0026gt;% group_by(class) %\u0026gt;% summarize(avg_hwy = mean(hwy)) # Super bad and won\u0026#39;t even work mpg %\u0026gt;% filter(cty \u0026gt; 10) %\u0026gt;% group_by(class) %\u0026gt;% summarize(avg_hwy = mean(hwy)) Comments  See the \u0026ldquo;Comments\u0026rdquo; section in the tidyverse style guide.\n Comments should start with a comment symbol and a single space: # \n# Good #Bad #Bad If the comment is really short (and won\u0026rsquo;t cause you to go over 80 characters in the line), you can include it in the same line as the code, separated by at least two spaces (it works with one space, but using a couple can enhance readability):\nmpg %\u0026gt;% filter(cty \u0026gt; 10) %\u0026gt;% # Only rows where cty is 10 + group_by(class) %\u0026gt;% # Divide into class groups summarize(avg_hwy = mean(hwy)) # Find the average hwy in each group You can add extra spaces to get inline comments to align, if you want:\nmpg %\u0026gt;% filter(cty \u0026gt; 10) %\u0026gt;% # Only rows where cty is 10 + group_by(class) %\u0026gt;% # Divide into class groups summarize(avg_hwy = mean(hwy)) # Find the average hwy in each group If the comment is really long, you can break it into multiple lines. RStudio can do this for you if you go to \u0026ldquo;Code\u0026rdquo; \u0026gt; \u0026ldquo;Reflow comment\u0026rdquo;\n# Good # Happy families are all alike; every unhappy family is unhappy in its own way. # Everything was in confusion in the Oblonskys’ house. The wife had discovered # that the husband was carrying on an intrigue with a French girl, who had been # a governess in their family, and she had announced to her husband that she # could not go on living in the same house with him. This position of affairs # had now lasted three days, and not only the husband and wife themselves, but # all the members of their family and household, were painfully conscious of it. # Bad # Happy families are all alike; every unhappy family is unhappy in its own way. Everything was in confusion in the Oblonskys’ house. The wife had discovered that the husband was carrying on an intrigue with a French girl, who had been a governess in their family, and she had announced to her husband that she could not go on living in the same house with him. This position of affairs had now lasted three days, and not only the husband and wife themselves, but all the members of their family and household, were painfully conscious of it. Though, if you\u0026rsquo;re dealing with comments that are that long, consider putting the text in R Markdown instead and having it be actual prose.\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"844aea7aa9e8c205f7e898c3972e5c8f","permalink":"/resource/style/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/resource/style/","section":"resource","summary":"R style conventions R is fairly forgiving about how you type code (unlike other languages like Python, where miscounting spaces can ruin your code!). All of these things will do exactly the same thing:","tags":null,"title":"R style suggestions","type":"docs"},{"authors":null,"categories":null,"content":"Because RStudio projects typically consist of multiple files (R scripts, datasets, graphical output, etc.) the easiest way to distribute them to you for examples, assignments, and projects is to combine all the different files in to a single compressed collection called a zip file. When you unzip a zipped file, your operating system extracts all the files that are contained inside into a new folder on your computer.\nUnzipping files on macOS is trivial, but unzipping files on Windows can mess you up if you don\u0026rsquo;t pay careful attention. Here\u0026rsquo;s a helpful guide to unzipping files on both macOS and Windows.\nUnzipping files on macOS Double click on the downloaded .zip file. macOS will automatically create a new folder with the same name as the .zip file, and all the file\u0026rsquo;s contents will be inside. Double click on the RStudio Project file (.Rproj) to get started.\nUnzipping files on Windows tl;dr: Right click on the .zip file, select \u0026ldquo;Extract All…\u0026rdquo;, and work with the resulting unzipped folder.\nUnlike macOS, Windows does not automatically unzip things for you. If you double click on the .zip file, Windows will show you what\u0026rsquo;s inside, but it will do so without actually extracting anything. This can be is incredibly confusing! Here\u0026rsquo;s what it looks like—the only clues that this folder is really a .zip file are that there\u0026rsquo;s a \u0026ldquo;Compressed Folder Tools\u0026rdquo; tab at the top, and there\u0026rsquo;s a \u0026ldquo;Ratio\u0026rdquo; column that shows how much each file is compressed.\nIt is very tempting to try to open files from this view. However, if you do, things will break and you won\u0026rsquo;t be able to correctly work with any of the files in the zipped folder. If you open the R Project file, for instance, RStudio will point to a bizarre working directory buried deep in some temporary folder:\nYou most likely won\u0026rsquo;t be able to open any data files or save anything, which will be frustrating.\nInstead, you need to right click on the .zip file and select \u0026ldquo;Extract All…\u0026quot;:\nThen choose where you want to unzip all the files and click on \u0026ldquo;Extract\u0026rdquo;\nYou should then finally have a real folder with all the contents of the zipped file. Open the R Project file and RStudio will point to the correct working directory and everything will work.\n","date":1588723200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1588723200,"objectID":"f1b04d7939933ed26af8d5a9fe95387f","permalink":"/resource/unzipping/","publishdate":"2020-05-06T00:00:00Z","relpermalink":"/resource/unzipping/","section":"resource","summary":"Because RStudio projects typically consist of multiple files (R scripts, datasets, graphical output, etc.) the easiest way to distribute them to you for examples, assignments, and projects is to combine all the different files in to a single compressed collection called a zip file.","tags":null,"title":"Unzipping files","type":"docs"},{"authors":null,"categories":null,"content":"There are a ton of places to find data related to public policy and administration (as well as data on pretty much any topic you want) online:\n  Data is Plural newsletter: Jeremy Singer-Vine sends a weekly newsletter of the most interesting public datasets he\u0026rsquo;s found. You should subscribe to it. He also has an archive of all the datasets he\u0026rsquo;s highlighted.\n  Google Dataset Search: Google indexes thousands of public datasets; search for them here.\n  Kaggle: Kaggle hosts machine learning competitions where people compete to create the fastest, most efficient, most predictive algorithms. A byproduct of these competitions is a host of fascinating datasets that are generally free and open to the public. See, for example, the European Soccer Database, the Salem Witchcraft Dataset or results from an Oreo flavors taste test.\n  360Giving: Dozens of British foundations follow a standard file format for sharing grant data and have made that data available online.\n  US City Open Data Census: More than 100 US cities have committed to sharing dozens of types of data, including data about crime, budgets, campaign finance, lobbying, transit, and zoning. This site from the Sunlight Foundation and Code for America collects this data and rates cities by how well they\u0026rsquo;re doing.\n  Political science and economics datasets: There\u0026rsquo;s a wealth of data available for political science- and economics-related topics:\n François Briatte\u0026rsquo;s extensive curated lists: Includes data from/about intergovernmental organizations (IGOs), nongovernmental organizations (NGOs), public opinion surveys, parliaments and legislatures, wars, human rights, elections, and municipalities. Thomas Leeper\u0026rsquo;s list of political science datasets: Good short list of useful datasets, divided by type of data (country-level data, survey data, social media data, event data, text data, etc.). Erik Gahner\u0026rsquo;s list of political science datasets: Huge list of useful datasets, divided by topic (governance, elections, policy, political elites, etc.)    ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"4c57416e3690fb647bc91955a36b3a16","permalink":"/resource/data/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/resource/data/","section":"resource","summary":"There are a ton of places to find data related to public policy and administration (as well as data on pretty much any topic you want) online:\n  Data is Plural newsletter: Jeremy Singer-Vine sends a weekly newsletter of the most interesting public datasets he\u0026rsquo;s found.","tags":null,"title":"Data","type":"docs"},{"authors":null,"categories":null,"content":"\rCausal Inference: The Mixtape includes code examples in both Stata and R, which is so cool! The previous PDF-only version of the book only had Stata code, so it was hard to follow along with in this class.\nAs you\u0026rsquo;ll note throughout this class, there are lots of ways to do things in R. You\u0026rsquo;ll come across code in the RStudio forums and on StackOverflow that works, but looks slightly different than you\u0026rsquo;re used to. In this class, we use the tidyverse, or a set of packages that all work together nicely like ggplot2, dplyr, and others.\nThe Mixtape also uses tidyverse (yay!), but has a couple code quirks and differences. Because the HTML version of the book (with R code) only came out on January 3, I haven\u0026rsquo;t had time to find all the differences, so I\u0026rsquo;ll keep a running list here.\nLoading data Throughout The Mixtape, the code examples load data from external files stored online. The code provided for loading this data is a little more complicated than it needs to be. For instance, consider this from Section 4.0.1:\nlibrary(tidyverse) library(haven) read_data \u0026lt;- function(df) { full_path \u0026lt;- paste(\u0026#34;https://raw.github.com/scunning1975/mixtape/master/\u0026#34;, df, sep = \u0026#34;\u0026#34;) df \u0026lt;- read_dta(full_path) return(df) } yule \u0026lt;- read_data(\u0026#34;yule.dta\u0026#34;) %\u0026gt;% lm(paup ~ outrelief + old + pop, .) summary(yule) ##  ## Call: ## lm(formula = paup ~ outrelief + old + pop, data = .) ##  ## Residuals: ## Min 1Q Median 3Q Max  ## -17.48 -5.31 -1.83 3.13 25.34  ##  ## Coefficients: ## Estimate Std. Error t value Pr(\u0026gt;|t|)  ## (Intercept) 63.1877 27.1439 2.33 0.027 *  ## outrelief 0.7521 0.1350 5.57 5.8e-06 *** ## old 0.0556 0.2234 0.25 0.805  ## pop -0.3107 0.0669 -4.65 7.3e-05 *** ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ##  ## Residual standard error: 9.6 on 28 degrees of freedom ## Multiple R-squared: 0.697,\tAdjusted R-squared: 0.665  ## F-statistic: 21.5 on 3 and 28 DF, p-value: 2e-07 Here, The Mixtape authors created a custom function named read_data() that loads a Stata file from a URL at GitHub. They then run a model on that data and store the results as an object named yule. It works, but it\u0026rsquo;s not the most efficient way to do this for a few reasons:\n You don\u0026rsquo;t need that whole custom read_data() function. Instead you can feed read_dta() a full URL and load it like that It\u0026rsquo;s best if you save the data as an object by itself, then use it to create models. As it stands now, you can\u0026rsquo;t look at the yule.dta file or make scatterplots or do anything with the data, since the results that are stored as yule are the results from the model, not the data itself.  Instead, this is an easier way to load the data and run the model:\nTranslation # Mixtape version read_data \u0026lt;- function(df) { full_path \u0026lt;- paste(\u0026#34;https://raw.github.com/scunning1975/mixtape/master/\u0026#34;, df, sep = \u0026#34;\u0026#34;) df \u0026lt;- read_dta(full_path) return(df) } yule \u0026lt;- read_data(\u0026#34;yule.dta\u0026#34;) %\u0026gt;% lm(paup ~ outrelief + old + pop, .) summary(yule) # Our class version yule_data \u0026lt;- read_dta(\u0026#34;https://raw.github.com/scunning1975/mixtape/master/yule.dta\u0026#34;) yule_model \u0026lt;- lm(paup ~ outrelief + old + pop, data = yule_data) summary(yule_model) ##  ## Call: ## lm(formula = paup ~ outrelief + old + pop, data = yule_data) ##  ## Residuals: ## Min 1Q Median 3Q Max  ## -17.48 -5.31 -1.83 3.13 25.34  ##  ## Coefficients: ## Estimate Std. Error t value Pr(\u0026gt;|t|)  ## (Intercept) 63.1877 27.1439 2.33 0.027 *  ## outrelief 0.7521 0.1350 5.57 5.8e-06 *** ## old 0.0556 0.2234 0.25 0.805  ## pop -0.3107 0.0669 -4.65 7.3e-05 *** ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ##  ## Residual standard error: 9.6 on 28 degrees of freedom ## Multiple R-squared: 0.697,\tAdjusted R-squared: 0.665  ## F-statistic: 21.5 on 3 and 28 DF, p-value: 2e-07 Even better, instead of using summary(yule_model), you can use tidy() and glance() from the broom library, which convert any kind of model object into standard data frames:\nlibrary(broom) # You only need to run this once tidy(yule_model) ## # A tibble: 4 x 5 ## term estimate std.error statistic p.value ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 (Intercept) 63.2 27.1 2.33 0.0274  ## 2 outrelief 0.752 0.135 5.57 0.00000583 ## 3 old 0.0556 0.223 0.249 0.805  ## 4 pop -0.311 0.0669 -4.65 0.0000725 glance(yule_model) ## # A tibble: 1 x 12 ## r.squared adj.r.squared sigma statistic p.value df logLik AIC BIC deviance df.residual nobs ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; ## 1 0.697 0.665 9.55 21.5 0.000000200 3 -115. 241. 248. 2552. 28 32 Side-by-side regression tables: stargazer vs. modelsummary Side-by-side regression tables are a standard feature of economics, political science, and policy articles and reports. They conveniently let you see the results from multiple regresison models in one table. This blog post here goes into detail about how to read them.\nThere are several R packages for creating these tables automatically. One of the earliest is named stargazer, and tons of people still use it—including The Mixtape. For instance, Section 3.1.5 in The Mixtape shows an example of discrimination and collider bias and the code there shows three regression models side by side:\ncollider_discrimination.R:\nset.seed(1234) library(tidyverse) library(stargazer) tb \u0026lt;- tibble( female = ifelse(runif(10000) \u0026gt;= 0.5, 1, 0), ability = rnorm(10000), discrimination = female, occupation = 1 + 2*ability + 0*female - 2*discrimination + rnorm(10000), wage = 1 - 1*discrimination + 1*occupation + 2*ability + rnorm(10000) ) lm_1 \u0026lt;- lm(wage ~ female, tb) lm_2 \u0026lt;- lm(wage ~ female + occupation, tb) lm_3 \u0026lt;- lm(wage ~ female + occupation + ability, tb) stargazer(lm_1, lm_2, lm_3, type = \u0026#34;html\u0026#34;, column.labels = c(\u0026#34;Biased Unconditional\u0026#34;, \u0026#34;Biased\u0026#34;, \u0026#34;Unbiased Conditional\u0026#34;)) Dependent variable:\r\rwage\rBiased UnconditionalBiasedUnbiased Conditional\r(1)(2)(3)\rfemale-2.900***0.560***-0.980***\r(0.084)(0.029)(0.028)\r\roccupation1.800***0.990***\r(0.006)(0.010)\r\rability2.000***\r(0.023)\r\rConstant2.000***0.240***1.000***\r(0.060)(0.020)(0.017)\r\rObservations10,00010,00010,000\rR20.1100.9100.950\rAdjusted R20.1100.9100.950\rResidual Std. Error4.200 (df = 9998)1.300 (df = 9997)1.000 (df = 9996)\rF Statistic1,208.000*** (df = 1; 9998)50,089.000*** (df = 2; 9997)62,279.000*** (df = 3; 9996)\rNote:*p**p***p\r\rNeat. That works, and there are a ton of additional options you can feed to stargazer()—run ?stargazer or search in the Help pane for \u0026ldquo;stargazer\u0026rdquo; to see more.\nHowever, I\u0026rsquo;m not a fan of stargazer for a few reasons:\n Stargazer tables won\u0026rsquo;t knit to Word You have to use different syntax when you\u0026rsquo;re knitting to PDF vs. HTML (i.e. you have to change the type argument) Stargazer handles basic models like lm() and glm() well, but doesn\u0026rsquo;t support newer things since (1) it hasn\u0026rsquo;t been updated for a while, and (2) support for any newer models has to be built-in by hand by the developer.  In this class we won\u0026rsquo;t use stargazer. Instead, we\u0026rsquo;ll use the newer modelsummary package, which addresses all of the issues with Stargazer:\n modelsummary tables knit to Word You don\u0026rsquo;t have to use any special syntax when knitting documents with modelsummary tables—knit to PDF and they\u0026rsquo;ll show up in the PDF; knit to Word and they\u0026rsquo;ll show up in the Word file; knit to HTML and they\u0026rsquo;ll show up there modelsummary handles any model that is supported by the broom package (basically, if you ran run tidy() on it, you can stick it in a table with modelsummary). If a model doesn\u0026rsquo;t support broom yet, you can actually add that support yourself. For instance, nonparametric regression discontinuity models made with the rdrobust package don\u0026rsquo;t work in either stargazer or modelsummary. They\u0026rsquo;ll probably never be added to stargazer, but if you include the code here in your document, you can add rdrobust models to modelsummary(). Magic!  The syntax for modelsummary() is a little different from stargazer(). Here\u0026rsquo;s how to recreate the same table from before:\nTranslation # Stargazer version stargazer(lm_1, lm_2, lm_3, type = \u0026#34;text\u0026#34;, column.labels = c(\u0026#34;Biased Unconditional\u0026#34;, \u0026#34;Biased\u0026#34;, \u0026#34;Unbiased Conditional\u0026#34;)) # modelsummary version library(modelsummary) # You only need to run this once modelsummary(list(\u0026#34;Biased Unconditional\u0026#34; = lm_1, \u0026#34;Biased\u0026#34; = lm_2, \u0026#34;Unbiased Conditional\u0026#34; = lm_3), stars = TRUE) \r\r\rBiased Unconditional \rBiased \rUnbiased Conditional \r\r\r\r\r(Intercept) \r2.015*** \r0.245*** \r1.005*** \r\r\r\r(0.060) \r(0.020) \r(0.017) \r\r\rfemale \r-2.933*** \r0.561*** \r-0.984*** \r\r\r\r(0.084) \r(0.029) \r(0.028) \r\r\roccupation \r\r1.790*** \r0.995*** \r\r\r\r\r(0.006) \r(0.010) \r\r\rability \r\r\r2.007*** \r\r\r\r\r\r(0.023) \r\r\rNum.Obs. \r10000 \r10000 \r10000 \r\r\rR2 \r0.108 \r0.909 \r0.949 \r\r\rR2 Adj. \r0.108 \r0.909 \r0.949 \r\r\rAIC \r57175.7 \r34320.7 \r28518.8 \r\r\rBIC \r57197.3 \r34349.6 \r28554.9 \r\r\rLog.Lik. \r-28584.837 \r-17156.358 \r-14254.399 \r\r\rF \r1208.290 \r50089.046 \r62279.460 \r\r\r\r\r * p \u0026lt; 0.1, ** p \u0026lt; 0.05, *** p \u0026lt; 0.01\r\r\r\rThat\u0026rsquo;s it! You feed modelsummary() a list of models. If you name the elements of the list, you\u0026rsquo;ll get column headings.\nThe modelsummary() function has a ton of options for changing the appearance, adding extra rows, omitting rows, highlighting rows, etc. See this page for examples.\n","date":1609718400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609718400,"objectID":"0a9fcfbe8059aa8033e14d15a66ca307","permalink":"/resource/mixtape-diffs/","publishdate":"2021-01-04T00:00:00Z","relpermalink":"/resource/mixtape-diffs/","section":"resource","summary":"Causal Inference: The Mixtape includes code examples in both Stata and R, which is so cool! The previous PDF-only version of the book only had Stata code, so it was hard to follow along with in this class.","tags":null,"title":"Mixtape-style R code translations","type":"docs"},{"authors":null,"categories":null,"content":"You can download a BibTeX file of all the non-web-based readings in the course:\n  references.bib  You can open the file in BibDesk on macOS, JabRef on Windows, or Zotero or Mendeley online.\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"d599e8776d313634e5e55f089a22b902","permalink":"/resource/citations/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/resource/citations/","section":"resource","summary":"You can download a BibTeX file of all the non-web-based readings in the course:\n  references.bib  You can open the file in BibDesk on macOS, JabRef on Windows, or Zotero or Mendeley online.","tags":null,"title":"Citations and bibliography","type":"docs"},{"authors":null,"categories":null,"content":"Readings   Chapter 5 in Impact Evaluation in Practice1  Chapter 3 in Mastering ’Metrics2  Chapter 7, “Instrumental variables” in Causal Inference: The Mixtape3  Instrumental variables  The example page on instrumental variables shows how to use R to analyze and estimate causal effects with instrumental variables  Slides The slides for today’s lesson are available online as an HTML file. Use the buttons below to open the slides either as an interactive website or as a static PDF (for printing or storing for later). You can also click in the slides below and navigate through them with your left and right arrow keys.\n View all slides in new window  Download PDF of all slides\nIntroduction\r\rEndogeneity and exogeneity\r\rInstruments\r\rUsing instruments\r\r\r\r\r\r\r\r\r\r\r\r\r\r\r\rFun fact: If you type ? (or shift + /) while going through the slides, you can see a list of special slide-specific commands.\r\rVideos Videos for each section of the lecture are available at this YouTube playlist.\n Introduction Endogeneity and exogeneity Instruments Using instruments  You can also watch the playlist (and skip around to different sections) here:\n\r\r  Paul J. Gertler et al., Impact Evaluation in Practice, 2nd ed. (Inter-American Development Bank; World Bank, 2016), https://openknowledge.worldbank.org/handle/10986/25030. \u0026#x21a9;\u0026#xfe0e;\n Joshua D. Angrist and Jörn-Steffen Pischke, Mastering ’Metrics: The Path from Cause to Effect (Princeton, NJ: Princeton University Press, 2015). \u0026#x21a9;\u0026#xfe0e;\n Scott Cunningham, Causal Inference: The Mixtape (New Haven, CT: Yale University Press, 2021), https://mixtape.scunning.com/. \u0026#x21a9;\u0026#xfe0e;\n   ","date":1636329600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1636329600,"objectID":"b5fe8e1e81202db02e58eeaf682fa1b4","permalink":"/content/11-content/","publishdate":"2021-11-08T00:00:00Z","relpermalink":"/content/11-content/","section":"content","summary":"Readings   Chapter 5 in Impact Evaluation in Practice1  Chapter 3 in Mastering ’Metrics2  Chapter 7, “Instrumental variables” in Causal Inference: The Mixtape3  Instrumental variables  The example page on instrumental variables shows how to use R to analyze and estimate causal effects with instrumental variables  Slides The slides for today’s lesson are available online as an HTML file.","tags":null,"title":"Calidad de modelos y otras técnicas","type":"docs"},{"authors":null,"categories":null,"content":"DAGs with dagitty.net The easiest way to quickly build DAGs and find adjustment sets and testable implications is to use dagitty.net.\nThis video shows how to use it:\n\r\rDAGs with R, ggdag, and dagitty You can use the ggdag and dagitty packages in R to build and work with DAGs too. I typically draw an initial DAG in my browser with dagitty.net and then I rewrite it in code in R so that it\u0026rsquo;s more official and formal and reproducible.\nLive coding example \r\rBasic DAGs (This is a heavily cleaned up and annotated version of the code from the video.)\n# Load packages library(tidyverse) # For dplyr, ggplot, and friends library(ggdag) # For plotting DAGs library(dagitty) # For working with DAG logic The general process for making and working with DAGs in R is to create a DAG object with dagify() and then plot it with ggdag(). The documentation for ggdag is really good and helpful and full of examples. Check these pages for all sorts of additional details:\n \u0026ldquo;An introduction to ggdag\u0026rdquo; \u0026ldquo;An introduction to directed acyclic graphs\u0026rdquo; List of all ggdag-related functions  The syntax for dagify() is similar to the formula syntax you\u0026rsquo;ve been using for building regression models with lm(). The formulas you use in dagify() indicate the relationships between nodes.\nFor instance, in this DAG, y is caused by x, a, and b (y ~ x + a + b), and x is caused by a and b (x ~ a + b). This means that a and b are confounders. Use the exposure and outcome arguments to specify which nodes are the exposure (or treatment/intervention/program) and the outcome.\n# Create super basic DAG simple_dag \u0026lt;- dagify( y ~ x + a + b, x ~ a + b, exposure = \u0026#34;x\u0026#34;, outcome = \u0026#34;y\u0026#34; ) # Adding a theme_dag() layer to the plot makes it have a white background with no axis labels ggdag(simple_dag) + theme_dag() If you want to plot which nodes are the exposure and outcome, use ggdag_status() instead:\nggdag_status(simple_dag) + theme_dag() Layouts and manual coordinates Notice how the layout is different in both of those graphs. By default, ggdag() positions the nodes randomly every time using a network algorithm. You can change the algorithm by using the layout argument: ggdag(simple_dag, layout = \u0026quot;nicely\u0026quot;). You can see a full list of possible algorithms by running ?layout_tbl_graph_igraph in the console.\nAlternatively, you can specify your own coordinates so that the nodes are positioned in the same place every time. Do this with the coords argument in dagify().\nThe best way to figure out what these numbers should be is to draw the DAG on paper or on a whiteboard and add a grid to it and then figure out the coordinates. For instance, in this DAG there are three rows and three columns: x and y go in the middle row (row 2) while a and b go in the middle column (column 2). It can also be helpful to not include theme_dag() at first so you can see the numbers for the underlying grid. Once you have everything positioned correctly, add theme_dag() to clean it up.\nsimple_dag_with_coords \u0026lt;- dagify( y ~ x + a + b, x ~ a + b, exposure = \u0026#34;x\u0026#34;, outcome = \u0026#34;y\u0026#34;, coords = list(x = c(x = 1, a = 2, b = 2, y = 3), y = c(x = 2, a = 1, b = 3, y = 2)) ) ggdag_status(simple_dag_with_coords) + theme_dag() Node names and labels The variable names you use do not have to be limited to just x, y, and other lowercase letters. You can any names you want, as long as there are no spaces.\ndag_with_var_names \u0026lt;- dagify( outcome ~ treatment + confounder1 + confounder2, treatment ~ confounder1 + confounder2, exposure = \u0026#34;treatment\u0026#34;, outcome = \u0026#34;outcome\u0026#34; ) ggdag_status(dag_with_var_names) + theme_dag() However, unless you use very short names, it is likely that the text will not fit inside the nodes. To get around this, you can add labels to the nodes using the labels argument in dagify(). Plot the labels by setting use_labels = \u0026quot;label\u0026quot; in ggdag(). You can turn off the text in the nodes with text = FALSE in ggdag().\nsimple_dag_with_coords_and_labels \u0026lt;- dagify( y ~ x + a + b, x ~ a + b, exposure = \u0026#34;x\u0026#34;, outcome = \u0026#34;y\u0026#34;, labels = c(y = \u0026#34;Outcome\u0026#34;, x = \u0026#34;Treatment\u0026#34;, a = \u0026#34;Confounder 1\u0026#34;, b = \u0026#34;Confounder 2\u0026#34;), coords = list(x = c(x = 1, a = 2, b = 2, y = 3), y = c(x = 2, a = 1, b = 3, y = 2)) ) ggdag_status(simple_dag_with_coords_and_labels, use_labels = \u0026#34;label\u0026#34;, text = FALSE) + guides(fill = FALSE, color = FALSE) + # Disable the legend theme_dag() Paths and adjustment sets R can also perform analysis on DAG objects. For example, we can find all the testable implications from the DAG using the impliedConditionalIndependencies() function from the dagitty package. For this simple DAG, there is only one: a should be independent of b. If we had a dataset with columns for each of these variables, we could check if this is true by running cor(a, b) to see if the two are related.\nimpliedConditionalIndependencies(simple_dag) ## a _||_ b We can also find all the paths between x and y using the paths() function from the dagitty package. We can see that there are three open paths between x and y:\npaths(simple_dag) ## $paths ## [1] \u0026#34;x -\u0026gt; y\u0026#34; \u0026#34;x \u0026lt;- a -\u0026gt; y\u0026#34; \u0026#34;x \u0026lt;- b -\u0026gt; y\u0026#34; ##  ## $open ## [1] TRUE TRUE TRUE The first open path is fine—we want a single d-connected relationship between treatment and outcome—but the other two indicate that there is confounding from a and b. We can see what each of these paths are with the ggdag_paths() function from the ggdag package:\nggdag_paths(simple_dag_with_coords) + theme_dag() Instead of listing out all the possible paths and identifying backdoors by hand, you can use the adjustmentSets() function in the dagitty package to programmatically find all the nodes that need to be adjusted. Here we see that both a and b need to be controlled for to isolate the x -\u0026gt; y relationship.\nadjustmentSets(simple_dag) ## { a, b } You can also visualize the adjustment sets with ggdag_adjustment_set() in the ggdag package. Make sure you set shadow = TRUE to draw the arrows coming out of the adjusted nodes—by default, those are not included.\nggdag_adjustment_set(simple_dag_with_coords, shadow = TRUE) + theme_dag() R will find minimally sufficient adjustment sets, which includes the fewest number of adjustments needed to close all backdoors between x and y. In this example DAG there was only one set of variables (a and b), but in other situations there could be many possible sets, or none if the causal effect is not identifiable.\nPlot DAG from dagitty.net with ggdag() If you use dagitty.net to draw a DAG, you\u0026rsquo;ll notice that it generates some code for you in the model code section:\nYou can copy that dag{ ... } code and paste it into R to define a DAG object rather than use the dagify() function. To do this, use the dagitty() function from the dagitty library and include the whole generated model code in single quotes (''):\nmodel_from_dagitty \u0026lt;- dagitty(\u0026#39;dag { bb=\u0026#34;0,0,1,1\u0026#34; \u0026#34;A confounder\u0026#34; [pos=\u0026#34;0.809,0.306\u0026#34;] \u0026#34;Another confounder\u0026#34; [pos=\u0026#34;0.810,0.529\u0026#34;] \u0026#34;Some outcome\u0026#34; [outcome,pos=\u0026#34;0.918,0.432\u0026#34;] \u0026#34;Some treatment\u0026#34; [exposure,pos=\u0026#34;0.681,0.426\u0026#34;] \u0026#34;A confounder\u0026#34; -\u0026gt; \u0026#34;Some outcome\u0026#34; \u0026#34;A confounder\u0026#34; -\u0026gt; \u0026#34;Some treatment\u0026#34; \u0026#34;Another confounder\u0026#34; -\u0026gt; \u0026#34;Some outcome\u0026#34; \u0026#34;Another confounder\u0026#34; -\u0026gt; \u0026#34;Some treatment\u0026#34; \u0026#34;Some treatment\u0026#34; -\u0026gt; \u0026#34;Some outcome\u0026#34; } \u0026#39;) ggdag(model_from_dagitty) + theme_dag() By default it\u0026rsquo;s going to look ugly because (1) the node labels don\u0026rsquo;t fit, and (2) slight differences in the coordinates make it so the nodes don\u0026rsquo;t perfectly align with each other. To fix coordinate alignment, you can modify the numbers in the generated DAG code. Here I rounded the numbers so that they\u0026rsquo;re all 0.3, 0.8, etc.\nTo fix the label issue, you can add the use_labels argument like normally. Only here, you can\u0026rsquo;t specify use_labels = \u0026quot;label\u0026quot;. Instead, when you specify a DAG using dagitty\u0026rsquo;s code like this, the column in the underlying dataset that contains the labels is named name, so you need to use use_labels = \u0026quot;name\u0026quot;.\nOther ggdag() variations like ggdag_status() will still work fine.\nmodel_from_dagitty_rounded \u0026lt;- dagitty(\u0026#39;dag { bb=\u0026#34;0,0,1,1\u0026#34; \u0026#34;A confounder\u0026#34; [pos=\u0026#34;0.8,0.3\u0026#34;] \u0026#34;Another confounder\u0026#34; [pos=\u0026#34;0.8,0.5\u0026#34;] \u0026#34;Some outcome\u0026#34; [outcome,pos=\u0026#34;0.9,0.4\u0026#34;] \u0026#34;Some treatment\u0026#34; [exposure,pos=\u0026#34;0.7,0.4\u0026#34;] \u0026#34;A confounder\u0026#34; -\u0026gt; \u0026#34;Some outcome\u0026#34; \u0026#34;A confounder\u0026#34; -\u0026gt; \u0026#34;Some treatment\u0026#34; \u0026#34;Another confounder\u0026#34; -\u0026gt; \u0026#34;Some outcome\u0026#34; \u0026#34;Another confounder\u0026#34; -\u0026gt; \u0026#34;Some treatment\u0026#34; \u0026#34;Some treatment\u0026#34; -\u0026gt; \u0026#34;Some outcome\u0026#34; } \u0026#39;) ggdag_status(model_from_dagitty_rounded, text = FALSE, use_labels = \u0026#34;name\u0026#34;) + guides(color = FALSE) + # Turn off legend theme_dag() Mosquito net example Conditional independencies You can test if your stated relationships are correct by looking at the conditional independencies that are implied by the DAG. In dagitty.net, these appear in the sidebar in the \u0026ldquo;Testable implications\u0026rdquo; section. To show how this works, we\u0026rsquo;ll use a simulated dataset that I generated about a fictional mosquito net program. Download the data here if you want to follow along:\n  mosquito_nets.csv  Researchers are interested in whether using mosquito nets decreases an individual\u0026rsquo;s risk of contracting malaria. They have collected data from 1,752 households in an unnamed country and have variables related to environmental factors, individual health, and household characteristics. Additionally, this country has a special government program that provides free mosquito nets to households that meet specific requirements: to qualify for the program, there must be more than 4 members of the household, and the household\u0026rsquo;s monthly income must be lower than $700 a month. Households are not automatically enrolled in the program, and many do not use it. The data is not experimental—researchers have no control over who uses mosquito nets, and individual households make their own choices over whether to apply for free nets or buy their own nets, as well as whether they use the nets if they have them.\nmosquito_dag \u0026lt;- dagify( malaria_risk ~ net + income + health + temperature + resistance, net ~ income + health + temperature + eligible + household, eligible ~ income + household, health ~ income, exposure = \u0026#34;net\u0026#34;, outcome = \u0026#34;malaria_risk\u0026#34;, coords = list(x = c(malaria_risk = 7, net = 3, income = 4, health = 5, temperature = 6, resistance = 8.5, eligible = 2, household = 1), y = c(malaria_risk = 2, net = 2, income = 3, health = 1, temperature = 3, resistance = 2, eligible = 3, household = 2)), labels = c(malaria_risk = \u0026#34;Risk of malaria\u0026#34;, net = \u0026#34;Mosquito net\u0026#34;, income = \u0026#34;Income\u0026#34;, health = \u0026#34;Health\u0026#34;, temperature = \u0026#34;Nighttime temperatures\u0026#34;, resistance = \u0026#34;Insecticide resistance\u0026#34;, eligible = \u0026#34;Eligible for program\u0026#34;, household = \u0026#34;Number in household\u0026#34;) ) ggdag_status(mosquito_dag, use_labels = \u0026#34;label\u0026#34;, text = FALSE) + guides(fill = FALSE, color = FALSE) + # Disable the legend theme_dag() The causal graph above outlines the complete relationship between mosquito net use and risk of malaria. Each node in the DAG is a column in the dataset collected by the researchers, and includes the following:\n Malaria risk (malaria_risk): The likelihood that someone in the household will be infected with malaria. Measured on a scale of 0–100, with higher values indicating higher risk. Mosquito net (net and net_num): A binary variable indicating if the household used mosquito nets. Eligible for program (eligible): A binary variable indicating if the household is eligible for the free net program. Income (income): The household\u0026rsquo;s monthly income, in US dollars. Nighttime temperatures (temperature): The average temperature at night, in Celsius. Health (health): Self-reported healthiness in the household. Measured on a scale of 0–100, with higher values indicating better health. Number in household (household): Number of people living in the household. Insecticide resistance (resistance): Some strains of mosquitoes are more resistant to insecticide and thus pose a higher risk of infecting people with malaria. This is measured on a scale of 0–100, with higher values indicating higher resistance.  According to the DAG, malaria risk is caused by income, temperatures, health, insecticide resistance, and mosquito net use. People who live in hotter regions, have lower incomes, have worse health, are surrounded by mosquitoes with high resistance to insecticide, and who do not use mosquito nets are at higher risk of contracting malaria than those who do not. Mosquito net use is caused by income, nighttime temperatures, health, the number of people living in the house, and eligibility for the free net program. People who live in areas that are cooler at night, have higher incomes, have better health, have more people in the home, and are eligible for free government nets are more likely to regularly use nets than those who do not. The DAG also shows that eligibility for the free net program is caused by income and household size, since households must meet specific thresholds to qualify.\nFirst, let\u0026rsquo;s download the dataset, put in a folder named data, and load it:\n  mosquito_nets.csv  # Load the data. # It\u0026#39;d be a good idea to click on the \u0026#34;mosquito_nets.csv\u0026#34; object in the # Environment panel in RStudio to see what the data looks like after you load it mosquito_nets \u0026lt;- read_csv(\u0026#34;data/mosquito_nets.csv\u0026#34;) We can use this data to check if the relationships defined by our DAG reflect reality. Recall that d-separation implies that nodes are statistically independent of each other and do not transfer associational information. If you draw the mosquito net DAG with dagitty.net, or if you run impliedConditionalIndependencies() in R, you can see a list of all the implied conditional independencies.\nimpliedConditionalIndependencies(mosquito_dag) ## elgb _||_ hlth | incm ## elgb _||_ mlr_ | hlth, incm, net, tmpr ## elgb _||_ rsst ## elgb _||_ tmpr ## hlth _||_ hshl ## hlth _||_ rsst ## hlth _||_ tmpr ## hshl _||_ incm ## hshl _||_ mlr_ | hlth, incm, net, tmpr ## hshl _||_ rsst ## hshl _||_ tmpr ## incm _||_ rsst ## incm _||_ tmpr ## net _||_ rsst ## rsst _||_ tmpr The _||_ symbol in the output here is the \\(\\perp\\) symbol, which means \u0026ldquo;independent of\u0026rdquo;. The | in the output means \u0026ldquo;given\u0026rdquo;.\nIn the interest of space, we will not verify all these implied independencies, but we can test a few of them:\n  \\(\\text{Health} \\perp \\text{Household members}\\): (Read as \u0026ldquo;Health is independent of household member count\u0026rdquo;.) Health should be independent of the number of people in each household. In the data, the two variables should not be correlated. This is indeed the case:\ncor(mosquito_nets$health, mosquito_nets$household) ## [1] 9.8e-05   \\(\\text{Income} \\perp \\text{Insecticide resistance}\\): (Read as \u0026ldquo;Income is independent of insecticide resistance\u0026rdquo;.) Income should be independent of insecticide resistance. This is again true:\ncor(mosquito_nets$income, mosquito_nets$resistance) ## [1] 0.014   \\(\\text{Malaria risk} \\perp \\text{Household members}\\ |\\ \\text{Health, Income, Bed net use, Temperature}\\): (Read as \u0026ldquo;Malaria risk is independent of house member count given health, income, bed net use, and temperature\u0026rdquo;.) Malaria risk should be independent of the number of household members given similar levels of health, income, mosquito net use, and nighttime temperatures. We cannot use cor() to test this implication, since there are many variables involved, but we can use a regression model to check if the number of household members is significantly related to malaria risk. It is not significant ($t = -0.17$, \\(p = 0.863\\)), which means the two are independent, as expected.\nlm(malaria_risk ~ household + health + income + net + temperature, data = mosquito_nets) %\u0026gt;% broom::tidy() ## # A tibble: 6 x 5 ## term estimate std.error statistic p.value ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 (Intercept) 76.2 0.966 78.9 0.  ## 2 household -0.0155 0.0893 -0.173 8.63e- 1 ## 3 health 0.148 0.0107 13.9 9.75e- 42 ## 4 income -0.0751 0.00104 -72.6 0.  ## 5 netTRUE -10.4 0.266 -39.2 2.63e-241 ## 6 temperature 1.01 0.0310 32.5 1.88e-181   We can check all the other conditional independencies to see if the DAG captures the reality of the full system of factors that influence mosquito net use and malaria risk. If there are substantial and significant correlations between nodes that should be independent, there is likely an issue with the specification of the DAG. Return to the theory of how the phenomena are generated and refine the DAG more.\nMosquito net adjustment sets There is a direct path between mosquito net use and the risk of malaria, but the effect is not causally identified due to several other open paths. We can either list out all the paths and find which open paths have arrows pointing backwards into the mosquito net node (run paths(mosquito_dag) to see these results), or we can let R find the appropriate adjustment sets automatically:\nadjustmentSets(mosquito_dag) ## { health, income, temperature } Based on the relationships between all the nodes in the DAG, adjusting for health, income, and temperature is enough to close all backdoors and identify the relationship between net use and malaria risk. Importantly, we do not need to worry about any of the nodes related to the government program for free nets, since those nodes are not d-connected to malaria risk. We only need to worry about confounding relationships.\nWe can confirm this graphically with ggdag_adjustment_set():\nggdag_adjustment_set(mosquito_dag, shadow = TRUE, use_labels = \u0026#34;label\u0026#34;, text = FALSE) + theme_dag() ","date":1636329600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1636329600,"objectID":"0e89e793d43c95c5db4acc2530a93ca8","permalink":"/example/11-practico/","publishdate":"2021-11-08T00:00:00Z","relpermalink":"/example/11-practico/","section":"example","summary":"DAGs with dagitty.net The easiest way to quickly build DAGs and find adjustment sets and testable implications is to use dagitty.net.\nThis video shows how to use it:\n\r\rDAGs with R, ggdag, and dagitty You can use the ggdag and dagitty packages in R to build and work with DAGs too.","tags":null,"title":"Calidad de modelos y otras técnicas de estimación","type":"docs"},{"authors":null,"categories":null,"content":"Readings   Chapter 6 in Impact Evaluation in Practice1  Chapter 4 in Mastering ’Metrics2  Chapter 6, “Regression discontinuity” in Causal Inference: The Mixtape3  Regression discontinuity  The example page on regression discontinuity shows how to use R to analyze and estimate causal effects with regression discontinuity  Slides The slides for today’s lesson are available online as an HTML file. Use the buttons below to open the slides either as an interactive website or as a static PDF (for printing or storing for later). You can also click in the slides below and navigate through them with your left and right arrow keys.\n View all slides in new window  Download PDF of all slides\nIntroduction\r\rArbitrary cutoffs and causal inference\r\rDrawing lines and measuring gaps\r\rMain RDD concerns\r\r\r\r\r\r\r\r\r\r\r\r\r\r\r\rFun fact: If you type ? (or shift + /) while going through the slides, you can see a list of special slide-specific commands.\r\rVideos Videos for each section of the lecture are available at this YouTube playlist.\n Introduction Arbitrary cutoffs and causal inference Drawing lines and measuring gaps Main RDD concerns  You can also watch the playlist (and skip around to different sections) here:\n\r\r  Paul J. Gertler et al., Impact Evaluation in Practice, 2nd ed. (Inter-American Development Bank; World Bank, 2016), https://openknowledge.worldbank.org/handle/10986/25030. \u0026#x21a9;\u0026#xfe0e;\n Joshua D. Angrist and Jörn-Steffen Pischke, Mastering ’Metrics: The Path from Cause to Effect (Princeton, NJ: Princeton University Press, 2015). \u0026#x21a9;\u0026#xfe0e;\n Scott Cunningham, Causal Inference: The Mixtape (New Haven, CT: Yale University Press, 2021), https://mixtape.scunning.com/. \u0026#x21a9;\u0026#xfe0e;\n   ","date":1635120000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635120000,"objectID":"5d439166d6a0e49335e0267500e133e3","permalink":"/content/10-content/","publishdate":"2021-10-25T00:00:00Z","relpermalink":"/content/10-content/","section":"content","summary":"Readings   Chapter 6 in Impact Evaluation in Practice1  Chapter 4 in Mastering ’Metrics2  Chapter 6, “Regression discontinuity” in Causal Inference: The Mixtape3  Regression discontinuity  The example page on regression discontinuity shows how to use R to analyze and estimate causal effects with regression discontinuity  Slides The slides for today’s lesson are available online as an HTML file.","tags":null,"title":"Regresiones logísticas","type":"docs"},{"authors":null,"categories":null,"content":"DAGs with dagitty.net The easiest way to quickly build DAGs and find adjustment sets and testable implications is to use dagitty.net.\nThis video shows how to use it:\n\r\rDAGs with R, ggdag, and dagitty You can use the ggdag and dagitty packages in R to build and work with DAGs too. I typically draw an initial DAG in my browser with dagitty.net and then I rewrite it in code in R so that it\u0026rsquo;s more official and formal and reproducible.\nLive coding example \r\rBasic DAGs (This is a heavily cleaned up and annotated version of the code from the video.)\n# Load packages library(tidyverse) # For dplyr, ggplot, and friends library(ggdag) # For plotting DAGs library(dagitty) # For working with DAG logic The general process for making and working with DAGs in R is to create a DAG object with dagify() and then plot it with ggdag(). The documentation for ggdag is really good and helpful and full of examples. Check these pages for all sorts of additional details:\n \u0026ldquo;An introduction to ggdag\u0026rdquo; \u0026ldquo;An introduction to directed acyclic graphs\u0026rdquo; List of all ggdag-related functions  The syntax for dagify() is similar to the formula syntax you\u0026rsquo;ve been using for building regression models with lm(). The formulas you use in dagify() indicate the relationships between nodes.\nFor instance, in this DAG, y is caused by x, a, and b (y ~ x + a + b), and x is caused by a and b (x ~ a + b). This means that a and b are confounders. Use the exposure and outcome arguments to specify which nodes are the exposure (or treatment/intervention/program) and the outcome.\n# Create super basic DAG simple_dag \u0026lt;- dagify( y ~ x + a + b, x ~ a + b, exposure = \u0026#34;x\u0026#34;, outcome = \u0026#34;y\u0026#34; ) # Adding a theme_dag() layer to the plot makes it have a white background with no axis labels ggdag(simple_dag) + theme_dag() If you want to plot which nodes are the exposure and outcome, use ggdag_status() instead:\nggdag_status(simple_dag) + theme_dag() Layouts and manual coordinates Notice how the layout is different in both of those graphs. By default, ggdag() positions the nodes randomly every time using a network algorithm. You can change the algorithm by using the layout argument: ggdag(simple_dag, layout = \u0026quot;nicely\u0026quot;). You can see a full list of possible algorithms by running ?layout_tbl_graph_igraph in the console.\nAlternatively, you can specify your own coordinates so that the nodes are positioned in the same place every time. Do this with the coords argument in dagify().\nThe best way to figure out what these numbers should be is to draw the DAG on paper or on a whiteboard and add a grid to it and then figure out the coordinates. For instance, in this DAG there are three rows and three columns: x and y go in the middle row (row 2) while a and b go in the middle column (column 2). It can also be helpful to not include theme_dag() at first so you can see the numbers for the underlying grid. Once you have everything positioned correctly, add theme_dag() to clean it up.\nsimple_dag_with_coords \u0026lt;- dagify( y ~ x + a + b, x ~ a + b, exposure = \u0026#34;x\u0026#34;, outcome = \u0026#34;y\u0026#34;, coords = list(x = c(x = 1, a = 2, b = 2, y = 3), y = c(x = 2, a = 1, b = 3, y = 2)) ) ggdag_status(simple_dag_with_coords) + theme_dag() Node names and labels The variable names you use do not have to be limited to just x, y, and other lowercase letters. You can any names you want, as long as there are no spaces.\ndag_with_var_names \u0026lt;- dagify( outcome ~ treatment + confounder1 + confounder2, treatment ~ confounder1 + confounder2, exposure = \u0026#34;treatment\u0026#34;, outcome = \u0026#34;outcome\u0026#34; ) ggdag_status(dag_with_var_names) + theme_dag() However, unless you use very short names, it is likely that the text will not fit inside the nodes. To get around this, you can add labels to the nodes using the labels argument in dagify(). Plot the labels by setting use_labels = \u0026quot;label\u0026quot; in ggdag(). You can turn off the text in the nodes with text = FALSE in ggdag().\nsimple_dag_with_coords_and_labels \u0026lt;- dagify( y ~ x + a + b, x ~ a + b, exposure = \u0026#34;x\u0026#34;, outcome = \u0026#34;y\u0026#34;, labels = c(y = \u0026#34;Outcome\u0026#34;, x = \u0026#34;Treatment\u0026#34;, a = \u0026#34;Confounder 1\u0026#34;, b = \u0026#34;Confounder 2\u0026#34;), coords = list(x = c(x = 1, a = 2, b = 2, y = 3), y = c(x = 2, a = 1, b = 3, y = 2)) ) ggdag_status(simple_dag_with_coords_and_labels, use_labels = \u0026#34;label\u0026#34;, text = FALSE) + guides(fill = FALSE, color = FALSE) + # Disable the legend theme_dag() Paths and adjustment sets R can also perform analysis on DAG objects. For example, we can find all the testable implications from the DAG using the impliedConditionalIndependencies() function from the dagitty package. For this simple DAG, there is only one: a should be independent of b. If we had a dataset with columns for each of these variables, we could check if this is true by running cor(a, b) to see if the two are related.\nimpliedConditionalIndependencies(simple_dag) ## a _||_ b We can also find all the paths between x and y using the paths() function from the dagitty package. We can see that there are three open paths between x and y:\npaths(simple_dag) ## $paths ## [1] \u0026#34;x -\u0026gt; y\u0026#34; \u0026#34;x \u0026lt;- a -\u0026gt; y\u0026#34; \u0026#34;x \u0026lt;- b -\u0026gt; y\u0026#34; ##  ## $open ## [1] TRUE TRUE TRUE The first open path is fine—we want a single d-connected relationship between treatment and outcome—but the other two indicate that there is confounding from a and b. We can see what each of these paths are with the ggdag_paths() function from the ggdag package:\nggdag_paths(simple_dag_with_coords) + theme_dag() Instead of listing out all the possible paths and identifying backdoors by hand, you can use the adjustmentSets() function in the dagitty package to programmatically find all the nodes that need to be adjusted. Here we see that both a and b need to be controlled for to isolate the x -\u0026gt; y relationship.\nadjustmentSets(simple_dag) ## { a, b } You can also visualize the adjustment sets with ggdag_adjustment_set() in the ggdag package. Make sure you set shadow = TRUE to draw the arrows coming out of the adjusted nodes—by default, those are not included.\nggdag_adjustment_set(simple_dag_with_coords, shadow = TRUE) + theme_dag() R will find minimally sufficient adjustment sets, which includes the fewest number of adjustments needed to close all backdoors between x and y. In this example DAG there was only one set of variables (a and b), but in other situations there could be many possible sets, or none if the causal effect is not identifiable.\nPlot DAG from dagitty.net with ggdag() If you use dagitty.net to draw a DAG, you\u0026rsquo;ll notice that it generates some code for you in the model code section:\nYou can copy that dag{ ... } code and paste it into R to define a DAG object rather than use the dagify() function. To do this, use the dagitty() function from the dagitty library and include the whole generated model code in single quotes (''):\nmodel_from_dagitty \u0026lt;- dagitty(\u0026#39;dag { bb=\u0026#34;0,0,1,1\u0026#34; \u0026#34;A confounder\u0026#34; [pos=\u0026#34;0.809,0.306\u0026#34;] \u0026#34;Another confounder\u0026#34; [pos=\u0026#34;0.810,0.529\u0026#34;] \u0026#34;Some outcome\u0026#34; [outcome,pos=\u0026#34;0.918,0.432\u0026#34;] \u0026#34;Some treatment\u0026#34; [exposure,pos=\u0026#34;0.681,0.426\u0026#34;] \u0026#34;A confounder\u0026#34; -\u0026gt; \u0026#34;Some outcome\u0026#34; \u0026#34;A confounder\u0026#34; -\u0026gt; \u0026#34;Some treatment\u0026#34; \u0026#34;Another confounder\u0026#34; -\u0026gt; \u0026#34;Some outcome\u0026#34; \u0026#34;Another confounder\u0026#34; -\u0026gt; \u0026#34;Some treatment\u0026#34; \u0026#34;Some treatment\u0026#34; -\u0026gt; \u0026#34;Some outcome\u0026#34; } \u0026#39;) ggdag(model_from_dagitty) + theme_dag() By default it\u0026rsquo;s going to look ugly because (1) the node labels don\u0026rsquo;t fit, and (2) slight differences in the coordinates make it so the nodes don\u0026rsquo;t perfectly align with each other. To fix coordinate alignment, you can modify the numbers in the generated DAG code. Here I rounded the numbers so that they\u0026rsquo;re all 0.3, 0.8, etc.\nTo fix the label issue, you can add the use_labels argument like normally. Only here, you can\u0026rsquo;t specify use_labels = \u0026quot;label\u0026quot;. Instead, when you specify a DAG using dagitty\u0026rsquo;s code like this, the column in the underlying dataset that contains the labels is named name, so you need to use use_labels = \u0026quot;name\u0026quot;.\nOther ggdag() variations like ggdag_status() will still work fine.\nmodel_from_dagitty_rounded \u0026lt;- dagitty(\u0026#39;dag { bb=\u0026#34;0,0,1,1\u0026#34; \u0026#34;A confounder\u0026#34; [pos=\u0026#34;0.8,0.3\u0026#34;] \u0026#34;Another confounder\u0026#34; [pos=\u0026#34;0.8,0.5\u0026#34;] \u0026#34;Some outcome\u0026#34; [outcome,pos=\u0026#34;0.9,0.4\u0026#34;] \u0026#34;Some treatment\u0026#34; [exposure,pos=\u0026#34;0.7,0.4\u0026#34;] \u0026#34;A confounder\u0026#34; -\u0026gt; \u0026#34;Some outcome\u0026#34; \u0026#34;A confounder\u0026#34; -\u0026gt; \u0026#34;Some treatment\u0026#34; \u0026#34;Another confounder\u0026#34; -\u0026gt; \u0026#34;Some outcome\u0026#34; \u0026#34;Another confounder\u0026#34; -\u0026gt; \u0026#34;Some treatment\u0026#34; \u0026#34;Some treatment\u0026#34; -\u0026gt; \u0026#34;Some outcome\u0026#34; } \u0026#39;) ggdag_status(model_from_dagitty_rounded, text = FALSE, use_labels = \u0026#34;name\u0026#34;) + guides(color = FALSE) + # Turn off legend theme_dag() Mosquito net example Conditional independencies You can test if your stated relationships are correct by looking at the conditional independencies that are implied by the DAG. In dagitty.net, these appear in the sidebar in the \u0026ldquo;Testable implications\u0026rdquo; section. To show how this works, we\u0026rsquo;ll use a simulated dataset that I generated about a fictional mosquito net program. Download the data here if you want to follow along:\n  mosquito_nets.csv  Researchers are interested in whether using mosquito nets decreases an individual\u0026rsquo;s risk of contracting malaria. They have collected data from 1,752 households in an unnamed country and have variables related to environmental factors, individual health, and household characteristics. Additionally, this country has a special government program that provides free mosquito nets to households that meet specific requirements: to qualify for the program, there must be more than 4 members of the household, and the household\u0026rsquo;s monthly income must be lower than $700 a month. Households are not automatically enrolled in the program, and many do not use it. The data is not experimental—researchers have no control over who uses mosquito nets, and individual households make their own choices over whether to apply for free nets or buy their own nets, as well as whether they use the nets if they have them.\nmosquito_dag \u0026lt;- dagify( malaria_risk ~ net + income + health + temperature + resistance, net ~ income + health + temperature + eligible + household, eligible ~ income + household, health ~ income, exposure = \u0026#34;net\u0026#34;, outcome = \u0026#34;malaria_risk\u0026#34;, coords = list(x = c(malaria_risk = 7, net = 3, income = 4, health = 5, temperature = 6, resistance = 8.5, eligible = 2, household = 1), y = c(malaria_risk = 2, net = 2, income = 3, health = 1, temperature = 3, resistance = 2, eligible = 3, household = 2)), labels = c(malaria_risk = \u0026#34;Risk of malaria\u0026#34;, net = \u0026#34;Mosquito net\u0026#34;, income = \u0026#34;Income\u0026#34;, health = \u0026#34;Health\u0026#34;, temperature = \u0026#34;Nighttime temperatures\u0026#34;, resistance = \u0026#34;Insecticide resistance\u0026#34;, eligible = \u0026#34;Eligible for program\u0026#34;, household = \u0026#34;Number in household\u0026#34;) ) ggdag_status(mosquito_dag, use_labels = \u0026#34;label\u0026#34;, text = FALSE) + guides(fill = FALSE, color = FALSE) + # Disable the legend theme_dag() The causal graph above outlines the complete relationship between mosquito net use and risk of malaria. Each node in the DAG is a column in the dataset collected by the researchers, and includes the following:\n Malaria risk (malaria_risk): The likelihood that someone in the household will be infected with malaria. Measured on a scale of 0–100, with higher values indicating higher risk. Mosquito net (net and net_num): A binary variable indicating if the household used mosquito nets. Eligible for program (eligible): A binary variable indicating if the household is eligible for the free net program. Income (income): The household\u0026rsquo;s monthly income, in US dollars. Nighttime temperatures (temperature): The average temperature at night, in Celsius. Health (health): Self-reported healthiness in the household. Measured on a scale of 0–100, with higher values indicating better health. Number in household (household): Number of people living in the household. Insecticide resistance (resistance): Some strains of mosquitoes are more resistant to insecticide and thus pose a higher risk of infecting people with malaria. This is measured on a scale of 0–100, with higher values indicating higher resistance.  According to the DAG, malaria risk is caused by income, temperatures, health, insecticide resistance, and mosquito net use. People who live in hotter regions, have lower incomes, have worse health, are surrounded by mosquitoes with high resistance to insecticide, and who do not use mosquito nets are at higher risk of contracting malaria than those who do not. Mosquito net use is caused by income, nighttime temperatures, health, the number of people living in the house, and eligibility for the free net program. People who live in areas that are cooler at night, have higher incomes, have better health, have more people in the home, and are eligible for free government nets are more likely to regularly use nets than those who do not. The DAG also shows that eligibility for the free net program is caused by income and household size, since households must meet specific thresholds to qualify.\nFirst, let\u0026rsquo;s download the dataset, put in a folder named data, and load it:\n  mosquito_nets.csv  # Load the data. # It\u0026#39;d be a good idea to click on the \u0026#34;mosquito_nets.csv\u0026#34; object in the # Environment panel in RStudio to see what the data looks like after you load it mosquito_nets \u0026lt;- read_csv(\u0026#34;data/mosquito_nets.csv\u0026#34;) We can use this data to check if the relationships defined by our DAG reflect reality. Recall that d-separation implies that nodes are statistically independent of each other and do not transfer associational information. If you draw the mosquito net DAG with dagitty.net, or if you run impliedConditionalIndependencies() in R, you can see a list of all the implied conditional independencies.\nimpliedConditionalIndependencies(mosquito_dag) ## elgb _||_ hlth | incm ## elgb _||_ mlr_ | hlth, incm, net, tmpr ## elgb _||_ rsst ## elgb _||_ tmpr ## hlth _||_ hshl ## hlth _||_ rsst ## hlth _||_ tmpr ## hshl _||_ incm ## hshl _||_ mlr_ | hlth, incm, net, tmpr ## hshl _||_ rsst ## hshl _||_ tmpr ## incm _||_ rsst ## incm _||_ tmpr ## net _||_ rsst ## rsst _||_ tmpr The _||_ symbol in the output here is the \\(\\perp\\) symbol, which means \u0026ldquo;independent of\u0026rdquo;. The | in the output means \u0026ldquo;given\u0026rdquo;.\nIn the interest of space, we will not verify all these implied independencies, but we can test a few of them:\n  \\(\\text{Health} \\perp \\text{Household members}\\): (Read as \u0026ldquo;Health is independent of household member count\u0026rdquo;.) Health should be independent of the number of people in each household. In the data, the two variables should not be correlated. This is indeed the case:\ncor(mosquito_nets$health, mosquito_nets$household) ## [1] 9.8e-05   \\(\\text{Income} \\perp \\text{Insecticide resistance}\\): (Read as \u0026ldquo;Income is independent of insecticide resistance\u0026rdquo;.) Income should be independent of insecticide resistance. This is again true:\ncor(mosquito_nets$income, mosquito_nets$resistance) ## [1] 0.014   \\(\\text{Malaria risk} \\perp \\text{Household members}\\ |\\ \\text{Health, Income, Bed net use, Temperature}\\): (Read as \u0026ldquo;Malaria risk is independent of house member count given health, income, bed net use, and temperature\u0026rdquo;.) Malaria risk should be independent of the number of household members given similar levels of health, income, mosquito net use, and nighttime temperatures. We cannot use cor() to test this implication, since there are many variables involved, but we can use a regression model to check if the number of household members is significantly related to malaria risk. It is not significant ($t = -0.17$, \\(p = 0.863\\)), which means the two are independent, as expected.\nlm(malaria_risk ~ household + health + income + net + temperature, data = mosquito_nets) %\u0026gt;% broom::tidy() ## # A tibble: 6 x 5 ## term estimate std.error statistic p.value ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 (Intercept) 76.2 0.966 78.9 0.  ## 2 household -0.0155 0.0893 -0.173 8.63e- 1 ## 3 health 0.148 0.0107 13.9 9.75e- 42 ## 4 income -0.0751 0.00104 -72.6 0.  ## 5 netTRUE -10.4 0.266 -39.2 2.63e-241 ## 6 temperature 1.01 0.0310 32.5 1.88e-181   We can check all the other conditional independencies to see if the DAG captures the reality of the full system of factors that influence mosquito net use and malaria risk. If there are substantial and significant correlations between nodes that should be independent, there is likely an issue with the specification of the DAG. Return to the theory of how the phenomena are generated and refine the DAG more.\nMosquito net adjustment sets There is a direct path between mosquito net use and the risk of malaria, but the effect is not causally identified due to several other open paths. We can either list out all the paths and find which open paths have arrows pointing backwards into the mosquito net node (run paths(mosquito_dag) to see these results), or we can let R find the appropriate adjustment sets automatically:\nadjustmentSets(mosquito_dag) ## { health, income, temperature } Based on the relationships between all the nodes in the DAG, adjusting for health, income, and temperature is enough to close all backdoors and identify the relationship between net use and malaria risk. Importantly, we do not need to worry about any of the nodes related to the government program for free nets, since those nodes are not d-connected to malaria risk. We only need to worry about confounding relationships.\nWe can confirm this graphically with ggdag_adjustment_set():\nggdag_adjustment_set(mosquito_dag, shadow = TRUE, use_labels = \u0026#34;label\u0026#34;, text = FALSE) + theme_dag() ","date":1635120000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635120000,"objectID":"0c18db48940c34af079916db6632daff","permalink":"/example/10-practico/","publishdate":"2021-10-25T00:00:00Z","relpermalink":"/example/10-practico/","section":"example","summary":"DAGs with dagitty.net The easiest way to quickly build DAGs and find adjustment sets and testable implications is to use dagitty.net.\nThis video shows how to use it:\n\r\rDAGs with R, ggdag, and dagitty You can use the ggdag and dagitty packages in R to build and work with DAGs too.","tags":null,"title":"Regresiones logísticas, exponenciación y representación gráfica","type":"docs"},{"authors":null,"categories":null,"content":"Readings This session is a continuation of session 8.\n","date":1634515200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1634515200,"objectID":"2e7abf6f521ec7257264b0f3edde6da9","permalink":"/content/09-content/","publishdate":"2021-10-18T00:00:00Z","relpermalink":"/content/09-content/","section":"content","summary":"Readings This session is a continuation of session 8.","tags":null,"title":"Regresiones lineales","type":"docs"},{"authors":null,"categories":null,"content":"DAGs with dagitty.net The easiest way to quickly build DAGs and find adjustment sets and testable implications is to use dagitty.net.\nThis video shows how to use it:\n\r\rDAGs with R, ggdag, and dagitty You can use the ggdag and dagitty packages in R to build and work with DAGs too. I typically draw an initial DAG in my browser with dagitty.net and then I rewrite it in code in R so that it\u0026rsquo;s more official and formal and reproducible.\nLive coding example \r\rBasic DAGs (This is a heavily cleaned up and annotated version of the code from the video.)\n# Load packages library(tidyverse) # For dplyr, ggplot, and friends library(ggdag) # For plotting DAGs library(dagitty) # For working with DAG logic The general process for making and working with DAGs in R is to create a DAG object with dagify() and then plot it with ggdag(). The documentation for ggdag is really good and helpful and full of examples. Check these pages for all sorts of additional details:\n \u0026ldquo;An introduction to ggdag\u0026rdquo; \u0026ldquo;An introduction to directed acyclic graphs\u0026rdquo; List of all ggdag-related functions  The syntax for dagify() is similar to the formula syntax you\u0026rsquo;ve been using for building regression models with lm(). The formulas you use in dagify() indicate the relationships between nodes.\nFor instance, in this DAG, y is caused by x, a, and b (y ~ x + a + b), and x is caused by a and b (x ~ a + b). This means that a and b are confounders. Use the exposure and outcome arguments to specify which nodes are the exposure (or treatment/intervention/program) and the outcome.\n# Create super basic DAG simple_dag \u0026lt;- dagify( y ~ x + a + b, x ~ a + b, exposure = \u0026#34;x\u0026#34;, outcome = \u0026#34;y\u0026#34; ) # Adding a theme_dag() layer to the plot makes it have a white background with no axis labels ggdag(simple_dag) + theme_dag() If you want to plot which nodes are the exposure and outcome, use ggdag_status() instead:\nggdag_status(simple_dag) + theme_dag() Layouts and manual coordinates Notice how the layout is different in both of those graphs. By default, ggdag() positions the nodes randomly every time using a network algorithm. You can change the algorithm by using the layout argument: ggdag(simple_dag, layout = \u0026quot;nicely\u0026quot;). You can see a full list of possible algorithms by running ?layout_tbl_graph_igraph in the console.\nAlternatively, you can specify your own coordinates so that the nodes are positioned in the same place every time. Do this with the coords argument in dagify().\nThe best way to figure out what these numbers should be is to draw the DAG on paper or on a whiteboard and add a grid to it and then figure out the coordinates. For instance, in this DAG there are three rows and three columns: x and y go in the middle row (row 2) while a and b go in the middle column (column 2). It can also be helpful to not include theme_dag() at first so you can see the numbers for the underlying grid. Once you have everything positioned correctly, add theme_dag() to clean it up.\nsimple_dag_with_coords \u0026lt;- dagify( y ~ x + a + b, x ~ a + b, exposure = \u0026#34;x\u0026#34;, outcome = \u0026#34;y\u0026#34;, coords = list(x = c(x = 1, a = 2, b = 2, y = 3), y = c(x = 2, a = 1, b = 3, y = 2)) ) ggdag_status(simple_dag_with_coords) + theme_dag() Node names and labels The variable names you use do not have to be limited to just x, y, and other lowercase letters. You can any names you want, as long as there are no spaces.\ndag_with_var_names \u0026lt;- dagify( outcome ~ treatment + confounder1 + confounder2, treatment ~ confounder1 + confounder2, exposure = \u0026#34;treatment\u0026#34;, outcome = \u0026#34;outcome\u0026#34; ) ggdag_status(dag_with_var_names) + theme_dag() However, unless you use very short names, it is likely that the text will not fit inside the nodes. To get around this, you can add labels to the nodes using the labels argument in dagify(). Plot the labels by setting use_labels = \u0026quot;label\u0026quot; in ggdag(). You can turn off the text in the nodes with text = FALSE in ggdag().\nsimple_dag_with_coords_and_labels \u0026lt;- dagify( y ~ x + a + b, x ~ a + b, exposure = \u0026#34;x\u0026#34;, outcome = \u0026#34;y\u0026#34;, labels = c(y = \u0026#34;Outcome\u0026#34;, x = \u0026#34;Treatment\u0026#34;, a = \u0026#34;Confounder 1\u0026#34;, b = \u0026#34;Confounder 2\u0026#34;), coords = list(x = c(x = 1, a = 2, b = 2, y = 3), y = c(x = 2, a = 1, b = 3, y = 2)) ) ggdag_status(simple_dag_with_coords_and_labels, use_labels = \u0026#34;label\u0026#34;, text = FALSE) + guides(fill = FALSE, color = FALSE) + # Disable the legend theme_dag() Paths and adjustment sets R can also perform analysis on DAG objects. For example, we can find all the testable implications from the DAG using the impliedConditionalIndependencies() function from the dagitty package. For this simple DAG, there is only one: a should be independent of b. If we had a dataset with columns for each of these variables, we could check if this is true by running cor(a, b) to see if the two are related.\nimpliedConditionalIndependencies(simple_dag) ## a _||_ b We can also find all the paths between x and y using the paths() function from the dagitty package. We can see that there are three open paths between x and y:\npaths(simple_dag) ## $paths ## [1] \u0026#34;x -\u0026gt; y\u0026#34; \u0026#34;x \u0026lt;- a -\u0026gt; y\u0026#34; \u0026#34;x \u0026lt;- b -\u0026gt; y\u0026#34; ##  ## $open ## [1] TRUE TRUE TRUE The first open path is fine—we want a single d-connected relationship between treatment and outcome—but the other two indicate that there is confounding from a and b. We can see what each of these paths are with the ggdag_paths() function from the ggdag package:\nggdag_paths(simple_dag_with_coords) + theme_dag() Instead of listing out all the possible paths and identifying backdoors by hand, you can use the adjustmentSets() function in the dagitty package to programmatically find all the nodes that need to be adjusted. Here we see that both a and b need to be controlled for to isolate the x -\u0026gt; y relationship.\nadjustmentSets(simple_dag) ## { a, b } You can also visualize the adjustment sets with ggdag_adjustment_set() in the ggdag package. Make sure you set shadow = TRUE to draw the arrows coming out of the adjusted nodes—by default, those are not included.\nggdag_adjustment_set(simple_dag_with_coords, shadow = TRUE) + theme_dag() R will find minimally sufficient adjustment sets, which includes the fewest number of adjustments needed to close all backdoors between x and y. In this example DAG there was only one set of variables (a and b), but in other situations there could be many possible sets, or none if the causal effect is not identifiable.\nPlot DAG from dagitty.net with ggdag() If you use dagitty.net to draw a DAG, you\u0026rsquo;ll notice that it generates some code for you in the model code section:\nYou can copy that dag{ ... } code and paste it into R to define a DAG object rather than use the dagify() function. To do this, use the dagitty() function from the dagitty library and include the whole generated model code in single quotes (''):\nmodel_from_dagitty \u0026lt;- dagitty(\u0026#39;dag { bb=\u0026#34;0,0,1,1\u0026#34; \u0026#34;A confounder\u0026#34; [pos=\u0026#34;0.809,0.306\u0026#34;] \u0026#34;Another confounder\u0026#34; [pos=\u0026#34;0.810,0.529\u0026#34;] \u0026#34;Some outcome\u0026#34; [outcome,pos=\u0026#34;0.918,0.432\u0026#34;] \u0026#34;Some treatment\u0026#34; [exposure,pos=\u0026#34;0.681,0.426\u0026#34;] \u0026#34;A confounder\u0026#34; -\u0026gt; \u0026#34;Some outcome\u0026#34; \u0026#34;A confounder\u0026#34; -\u0026gt; \u0026#34;Some treatment\u0026#34; \u0026#34;Another confounder\u0026#34; -\u0026gt; \u0026#34;Some outcome\u0026#34; \u0026#34;Another confounder\u0026#34; -\u0026gt; \u0026#34;Some treatment\u0026#34; \u0026#34;Some treatment\u0026#34; -\u0026gt; \u0026#34;Some outcome\u0026#34; } \u0026#39;) ggdag(model_from_dagitty) + theme_dag() By default it\u0026rsquo;s going to look ugly because (1) the node labels don\u0026rsquo;t fit, and (2) slight differences in the coordinates make it so the nodes don\u0026rsquo;t perfectly align with each other. To fix coordinate alignment, you can modify the numbers in the generated DAG code. Here I rounded the numbers so that they\u0026rsquo;re all 0.3, 0.8, etc.\nTo fix the label issue, you can add the use_labels argument like normally. Only here, you can\u0026rsquo;t specify use_labels = \u0026quot;label\u0026quot;. Instead, when you specify a DAG using dagitty\u0026rsquo;s code like this, the column in the underlying dataset that contains the labels is named name, so you need to use use_labels = \u0026quot;name\u0026quot;.\nOther ggdag() variations like ggdag_status() will still work fine.\nmodel_from_dagitty_rounded \u0026lt;- dagitty(\u0026#39;dag { bb=\u0026#34;0,0,1,1\u0026#34; \u0026#34;A confounder\u0026#34; [pos=\u0026#34;0.8,0.3\u0026#34;] \u0026#34;Another confounder\u0026#34; [pos=\u0026#34;0.8,0.5\u0026#34;] \u0026#34;Some outcome\u0026#34; [outcome,pos=\u0026#34;0.9,0.4\u0026#34;] \u0026#34;Some treatment\u0026#34; [exposure,pos=\u0026#34;0.7,0.4\u0026#34;] \u0026#34;A confounder\u0026#34; -\u0026gt; \u0026#34;Some outcome\u0026#34; \u0026#34;A confounder\u0026#34; -\u0026gt; \u0026#34;Some treatment\u0026#34; \u0026#34;Another confounder\u0026#34; -\u0026gt; \u0026#34;Some outcome\u0026#34; \u0026#34;Another confounder\u0026#34; -\u0026gt; \u0026#34;Some treatment\u0026#34; \u0026#34;Some treatment\u0026#34; -\u0026gt; \u0026#34;Some outcome\u0026#34; } \u0026#39;) ggdag_status(model_from_dagitty_rounded, text = FALSE, use_labels = \u0026#34;name\u0026#34;) + guides(color = FALSE) + # Turn off legend theme_dag() Mosquito net example Conditional independencies You can test if your stated relationships are correct by looking at the conditional independencies that are implied by the DAG. In dagitty.net, these appear in the sidebar in the \u0026ldquo;Testable implications\u0026rdquo; section. To show how this works, we\u0026rsquo;ll use a simulated dataset that I generated about a fictional mosquito net program. Download the data here if you want to follow along:\n  mosquito_nets.csv  Researchers are interested in whether using mosquito nets decreases an individual\u0026rsquo;s risk of contracting malaria. They have collected data from 1,752 households in an unnamed country and have variables related to environmental factors, individual health, and household characteristics. Additionally, this country has a special government program that provides free mosquito nets to households that meet specific requirements: to qualify for the program, there must be more than 4 members of the household, and the household\u0026rsquo;s monthly income must be lower than $700 a month. Households are not automatically enrolled in the program, and many do not use it. The data is not experimental—researchers have no control over who uses mosquito nets, and individual households make their own choices over whether to apply for free nets or buy their own nets, as well as whether they use the nets if they have them.\nmosquito_dag \u0026lt;- dagify( malaria_risk ~ net + income + health + temperature + resistance, net ~ income + health + temperature + eligible + household, eligible ~ income + household, health ~ income, exposure = \u0026#34;net\u0026#34;, outcome = \u0026#34;malaria_risk\u0026#34;, coords = list(x = c(malaria_risk = 7, net = 3, income = 4, health = 5, temperature = 6, resistance = 8.5, eligible = 2, household = 1), y = c(malaria_risk = 2, net = 2, income = 3, health = 1, temperature = 3, resistance = 2, eligible = 3, household = 2)), labels = c(malaria_risk = \u0026#34;Risk of malaria\u0026#34;, net = \u0026#34;Mosquito net\u0026#34;, income = \u0026#34;Income\u0026#34;, health = \u0026#34;Health\u0026#34;, temperature = \u0026#34;Nighttime temperatures\u0026#34;, resistance = \u0026#34;Insecticide resistance\u0026#34;, eligible = \u0026#34;Eligible for program\u0026#34;, household = \u0026#34;Number in household\u0026#34;) ) ggdag_status(mosquito_dag, use_labels = \u0026#34;label\u0026#34;, text = FALSE) + guides(fill = FALSE, color = FALSE) + # Disable the legend theme_dag() The causal graph above outlines the complete relationship between mosquito net use and risk of malaria. Each node in the DAG is a column in the dataset collected by the researchers, and includes the following:\n Malaria risk (malaria_risk): The likelihood that someone in the household will be infected with malaria. Measured on a scale of 0–100, with higher values indicating higher risk. Mosquito net (net and net_num): A binary variable indicating if the household used mosquito nets. Eligible for program (eligible): A binary variable indicating if the household is eligible for the free net program. Income (income): The household\u0026rsquo;s monthly income, in US dollars. Nighttime temperatures (temperature): The average temperature at night, in Celsius. Health (health): Self-reported healthiness in the household. Measured on a scale of 0–100, with higher values indicating better health. Number in household (household): Number of people living in the household. Insecticide resistance (resistance): Some strains of mosquitoes are more resistant to insecticide and thus pose a higher risk of infecting people with malaria. This is measured on a scale of 0–100, with higher values indicating higher resistance.  According to the DAG, malaria risk is caused by income, temperatures, health, insecticide resistance, and mosquito net use. People who live in hotter regions, have lower incomes, have worse health, are surrounded by mosquitoes with high resistance to insecticide, and who do not use mosquito nets are at higher risk of contracting malaria than those who do not. Mosquito net use is caused by income, nighttime temperatures, health, the number of people living in the house, and eligibility for the free net program. People who live in areas that are cooler at night, have higher incomes, have better health, have more people in the home, and are eligible for free government nets are more likely to regularly use nets than those who do not. The DAG also shows that eligibility for the free net program is caused by income and household size, since households must meet specific thresholds to qualify.\nFirst, let\u0026rsquo;s download the dataset, put in a folder named data, and load it:\n  mosquito_nets.csv  # Load the data. # It\u0026#39;d be a good idea to click on the \u0026#34;mosquito_nets.csv\u0026#34; object in the # Environment panel in RStudio to see what the data looks like after you load it mosquito_nets \u0026lt;- read_csv(\u0026#34;data/mosquito_nets.csv\u0026#34;) We can use this data to check if the relationships defined by our DAG reflect reality. Recall that d-separation implies that nodes are statistically independent of each other and do not transfer associational information. If you draw the mosquito net DAG with dagitty.net, or if you run impliedConditionalIndependencies() in R, you can see a list of all the implied conditional independencies.\nimpliedConditionalIndependencies(mosquito_dag) ## elgb _||_ hlth | incm ## elgb _||_ mlr_ | hlth, incm, net, tmpr ## elgb _||_ rsst ## elgb _||_ tmpr ## hlth _||_ hshl ## hlth _||_ rsst ## hlth _||_ tmpr ## hshl _||_ incm ## hshl _||_ mlr_ | hlth, incm, net, tmpr ## hshl _||_ rsst ## hshl _||_ tmpr ## incm _||_ rsst ## incm _||_ tmpr ## net _||_ rsst ## rsst _||_ tmpr The _||_ symbol in the output here is the \\(\\perp\\) symbol, which means \u0026ldquo;independent of\u0026rdquo;. The | in the output means \u0026ldquo;given\u0026rdquo;.\nIn the interest of space, we will not verify all these implied independencies, but we can test a few of them:\n  \\(\\text{Health} \\perp \\text{Household members}\\): (Read as \u0026ldquo;Health is independent of household member count\u0026rdquo;.) Health should be independent of the number of people in each household. In the data, the two variables should not be correlated. This is indeed the case:\ncor(mosquito_nets$health, mosquito_nets$household) ## [1] 9.8e-05   \\(\\text{Income} \\perp \\text{Insecticide resistance}\\): (Read as \u0026ldquo;Income is independent of insecticide resistance\u0026rdquo;.) Income should be independent of insecticide resistance. This is again true:\ncor(mosquito_nets$income, mosquito_nets$resistance) ## [1] 0.014   \\(\\text{Malaria risk} \\perp \\text{Household members}\\ |\\ \\text{Health, Income, Bed net use, Temperature}\\): (Read as \u0026ldquo;Malaria risk is independent of house member count given health, income, bed net use, and temperature\u0026rdquo;.) Malaria risk should be independent of the number of household members given similar levels of health, income, mosquito net use, and nighttime temperatures. We cannot use cor() to test this implication, since there are many variables involved, but we can use a regression model to check if the number of household members is significantly related to malaria risk. It is not significant ($t = -0.17$, \\(p = 0.863\\)), which means the two are independent, as expected.\nlm(malaria_risk ~ household + health + income + net + temperature, data = mosquito_nets) %\u0026gt;% broom::tidy() ## # A tibble: 6 x 5 ## term estimate std.error statistic p.value ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 (Intercept) 76.2 0.966 78.9 0.  ## 2 household -0.0155 0.0893 -0.173 8.63e- 1 ## 3 health 0.148 0.0107 13.9 9.75e- 42 ## 4 income -0.0751 0.00104 -72.6 0.  ## 5 netTRUE -10.4 0.266 -39.2 2.63e-241 ## 6 temperature 1.01 0.0310 32.5 1.88e-181   We can check all the other conditional independencies to see if the DAG captures the reality of the full system of factors that influence mosquito net use and malaria risk. If there are substantial and significant correlations between nodes that should be independent, there is likely an issue with the specification of the DAG. Return to the theory of how the phenomena are generated and refine the DAG more.\nMosquito net adjustment sets There is a direct path between mosquito net use and the risk of malaria, but the effect is not causally identified due to several other open paths. We can either list out all the paths and find which open paths have arrows pointing backwards into the mosquito net node (run paths(mosquito_dag) to see these results), or we can let R find the appropriate adjustment sets automatically:\nadjustmentSets(mosquito_dag) ## { health, income, temperature } Based on the relationships between all the nodes in the DAG, adjusting for health, income, and temperature is enough to close all backdoors and identify the relationship between net use and malaria risk. Importantly, we do not need to worry about any of the nodes related to the government program for free nets, since those nodes are not d-connected to malaria risk. We only need to worry about confounding relationships.\nWe can confirm this graphically with ggdag_adjustment_set():\nggdag_adjustment_set(mosquito_dag, shadow = TRUE, use_labels = \u0026#34;label\u0026#34;, text = FALSE) + theme_dag() ","date":1634515200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1634515200,"objectID":"dd0313403c2b2a54c8fa0797453a566b","permalink":"/example/09-practico/","publishdate":"2021-10-18T00:00:00Z","relpermalink":"/example/09-practico/","section":"example","summary":"DAGs with dagitty.net The easiest way to quickly build DAGs and find adjustment sets and testable implications is to use dagitty.net.\nThis video shows how to use it:\n\r\rDAGs with R, ggdag, and dagitty You can use the ggdag and dagitty packages in R to build and work with DAGs too.","tags":null,"title":"Regresiones lineales, predictores categóricos y representación gráfica","type":"docs"},{"authors":null,"categories":null,"content":"Readings   Chapter 7 in Impact Evaluation in Practice1  Chapter 5 in Mastering ’Metrics2  Chapter 9, “Differences-in-differences” in Causal Inference: The Mixtape3  Slides The slides for today’s lesson are available online as an HTML file. Use the buttons below to open the slides either as an interactive website or as a static PDF (for printing or storing for later). You can also click in the slides below and navigate through them with your left and right arrow keys.\n View all slides in new window  Download PDF of all slides\nIntroduction\r\rQuasi-experiments\r\rInteractions \u0026 regression\r\rTwo wrongs make a right\r\rDiff-in-diff assumptions\r\r\r\r\r\r\r\r\r\r\r\r\r\r\r\r\r\r\rFun fact: If you type ? (or shift + /) while going through the slides, you can see a list of special slide-specific commands.\r\rVideos Videos for each section of the lecture are available at this YouTube playlist.\n Introduction Quasi-experiments Interactions \u0026amp; regression Two wrongs make a right Diff-in-diff assumptions  You can also watch the playlist (and skip around to different sections) here:\n\r\r  Paul J. Gertler et al., Impact Evaluation in Practice, 2nd ed. (Inter-American Development Bank; World Bank, 2016), https://openknowledge.worldbank.org/handle/10986/25030. \u0026#x21a9;\u0026#xfe0e;\n Joshua D. Angrist and Jörn-Steffen Pischke, Mastering ’Metrics: The Path from Cause to Effect (Princeton, NJ: Princeton University Press, 2015). \u0026#x21a9;\u0026#xfe0e;\n Scott Cunningham, Causal Inference: The Mixtape (New Haven, CT: Yale University Press, 2021), https://mixtape.scunning.com/. \u0026#x21a9;\u0026#xfe0e;\n   ","date":1633305600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1633305600,"objectID":"ac6f46f70a7f6537961cc77658f6cfcc","permalink":"/content/08-content/","publishdate":"2021-10-04T00:00:00Z","relpermalink":"/content/08-content/","section":"content","summary":"Readings   Chapter 7 in Impact Evaluation in Practice1  Chapter 5 in Mastering ’Metrics2  Chapter 9, “Differences-in-differences” in Causal Inference: The Mixtape3  Slides The slides for today’s lesson are available online as an HTML file.","tags":null,"title":"Pruebas de hipótesis","type":"docs"},{"authors":null,"categories":null,"content":"Readings   Andrew Heiss, “Causal Inference,” Chapter 10 in R for Political Data Science: A Practical Guide (forthcoming) (Ignore the exercises!). Get the PDF here.  Chapter 4 in Impact Evaluation in Practice1  Chapter 1 in Mastering ’Metrics2  Planet Money, “Moving To Opportunity?,” episode 937  Aaron Carroll, “Workplace Wellness Programs Don’t Work Well. Why Some Studies Show Otherwise,” The Upshot, August 6, 2018  RCTs, matching, and inverse probability weighting  The example page on RCTs shows how to use R to analyze and estimate causal effects from RCTs The example page on matching and inverse probability weighting shows how to use R to close backdoors, make adjustments, and find causal effects from observational data using matching and inverse probability weighting  Slides The slides for today’s lesson are available online as an HTML file. Use the buttons below to open the slides either as an interactive website or as a static PDF (for printing or storing for later). You can also click in the slides below and navigate through them with your left and right arrow keys.\n View all slides in new window  Download PDF of all slides\nIntroduction\r\rThe magic of randomization\r\rHow to analyze RCTs\r\rThe “gold” standard\r\rAdjustment with matching\r\r\r\r\r\r\r\r\r\r\r\r\r\r\r\r\r\r\rFun fact: If you type ? (or shift + /) while going through the slides, you can see a list of special slide-specific commands.\r\rVideos Videos for each section of the lecture are available at this YouTube playlist.\n Introduction The magic of randomization How to analyze RCTs The “gold” standard Adjustment with matching  You can also watch the playlist (and skip around to different sections) here:\n\r\r  Paul J. Gertler et al., Impact Evaluation in Practice, 2nd ed. (Inter-American Development Bank; World Bank, 2016), https://openknowledge.worldbank.org/handle/10986/25030. \u0026#x21a9;\u0026#xfe0e;\n Joshua D. Angrist and Jörn-Steffen Pischke, Mastering ’Metrics: The Path from Cause to Effect (Princeton, NJ: Princeton University Press, 2015). \u0026#x21a9;\u0026#xfe0e;\n   ","date":1632700800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1632700800,"objectID":"708d5afb1e280ebc34cec655dde7f978","permalink":"/content/07-content/","publishdate":"2021-09-27T00:00:00Z","relpermalink":"/content/07-content/","section":"content","summary":"Readings   Andrew Heiss, “Causal Inference,” Chapter 10 in R for Political Data Science: A Practical Guide (forthcoming) (Ignore the exercises!). Get the PDF here.  Chapter 4 in Impact Evaluation in Practice1  Chapter 1 in Mastering ’Metrics2  Planet Money, “Moving To Opportunity?","tags":null,"title":"Muestras complejas","type":"docs"},{"authors":null,"categories":null,"content":"DAGs with dagitty.net The easiest way to quickly build DAGs and find adjustment sets and testable implications is to use dagitty.net.\nThis video shows how to use it:\n\r\rDAGs with R, ggdag, and dagitty You can use the ggdag and dagitty packages in R to build and work with DAGs too. I typically draw an initial DAG in my browser with dagitty.net and then I rewrite it in code in R so that it\u0026rsquo;s more official and formal and reproducible.\nLive coding example \r\rBasic DAGs (This is a heavily cleaned up and annotated version of the code from the video.)\n# Load packages library(tidyverse) # For dplyr, ggplot, and friends library(ggdag) # For plotting DAGs library(dagitty) # For working with DAG logic The general process for making and working with DAGs in R is to create a DAG object with dagify() and then plot it with ggdag(). The documentation for ggdag is really good and helpful and full of examples. Check these pages for all sorts of additional details:\n \u0026ldquo;An introduction to ggdag\u0026rdquo; \u0026ldquo;An introduction to directed acyclic graphs\u0026rdquo; List of all ggdag-related functions  The syntax for dagify() is similar to the formula syntax you\u0026rsquo;ve been using for building regression models with lm(). The formulas you use in dagify() indicate the relationships between nodes.\nFor instance, in this DAG, y is caused by x, a, and b (y ~ x + a + b), and x is caused by a and b (x ~ a + b). This means that a and b are confounders. Use the exposure and outcome arguments to specify which nodes are the exposure (or treatment/intervention/program) and the outcome.\n# Create super basic DAG simple_dag \u0026lt;- dagify( y ~ x + a + b, x ~ a + b, exposure = \u0026#34;x\u0026#34;, outcome = \u0026#34;y\u0026#34; ) # Adding a theme_dag() layer to the plot makes it have a white background with no axis labels ggdag(simple_dag) + theme_dag() If you want to plot which nodes are the exposure and outcome, use ggdag_status() instead:\nggdag_status(simple_dag) + theme_dag() Layouts and manual coordinates Notice how the layout is different in both of those graphs. By default, ggdag() positions the nodes randomly every time using a network algorithm. You can change the algorithm by using the layout argument: ggdag(simple_dag, layout = \u0026quot;nicely\u0026quot;). You can see a full list of possible algorithms by running ?layout_tbl_graph_igraph in the console.\nAlternatively, you can specify your own coordinates so that the nodes are positioned in the same place every time. Do this with the coords argument in dagify().\nThe best way to figure out what these numbers should be is to draw the DAG on paper or on a whiteboard and add a grid to it and then figure out the coordinates. For instance, in this DAG there are three rows and three columns: x and y go in the middle row (row 2) while a and b go in the middle column (column 2). It can also be helpful to not include theme_dag() at first so you can see the numbers for the underlying grid. Once you have everything positioned correctly, add theme_dag() to clean it up.\nsimple_dag_with_coords \u0026lt;- dagify( y ~ x + a + b, x ~ a + b, exposure = \u0026#34;x\u0026#34;, outcome = \u0026#34;y\u0026#34;, coords = list(x = c(x = 1, a = 2, b = 2, y = 3), y = c(x = 2, a = 1, b = 3, y = 2)) ) ggdag_status(simple_dag_with_coords) + theme_dag() Node names and labels The variable names you use do not have to be limited to just x, y, and other lowercase letters. You can any names you want, as long as there are no spaces.\ndag_with_var_names \u0026lt;- dagify( outcome ~ treatment + confounder1 + confounder2, treatment ~ confounder1 + confounder2, exposure = \u0026#34;treatment\u0026#34;, outcome = \u0026#34;outcome\u0026#34; ) ggdag_status(dag_with_var_names) + theme_dag() However, unless you use very short names, it is likely that the text will not fit inside the nodes. To get around this, you can add labels to the nodes using the labels argument in dagify(). Plot the labels by setting use_labels = \u0026quot;label\u0026quot; in ggdag(). You can turn off the text in the nodes with text = FALSE in ggdag().\nsimple_dag_with_coords_and_labels \u0026lt;- dagify( y ~ x + a + b, x ~ a + b, exposure = \u0026#34;x\u0026#34;, outcome = \u0026#34;y\u0026#34;, labels = c(y = \u0026#34;Outcome\u0026#34;, x = \u0026#34;Treatment\u0026#34;, a = \u0026#34;Confounder 1\u0026#34;, b = \u0026#34;Confounder 2\u0026#34;), coords = list(x = c(x = 1, a = 2, b = 2, y = 3), y = c(x = 2, a = 1, b = 3, y = 2)) ) ggdag_status(simple_dag_with_coords_and_labels, use_labels = \u0026#34;label\u0026#34;, text = FALSE) + guides(fill = FALSE, color = FALSE) + # Disable the legend theme_dag() Paths and adjustment sets R can also perform analysis on DAG objects. For example, we can find all the testable implications from the DAG using the impliedConditionalIndependencies() function from the dagitty package. For this simple DAG, there is only one: a should be independent of b. If we had a dataset with columns for each of these variables, we could check if this is true by running cor(a, b) to see if the two are related.\nimpliedConditionalIndependencies(simple_dag) ## a _||_ b We can also find all the paths between x and y using the paths() function from the dagitty package. We can see that there are three open paths between x and y:\npaths(simple_dag) ## $paths ## [1] \u0026#34;x -\u0026gt; y\u0026#34; \u0026#34;x \u0026lt;- a -\u0026gt; y\u0026#34; \u0026#34;x \u0026lt;- b -\u0026gt; y\u0026#34; ##  ## $open ## [1] TRUE TRUE TRUE The first open path is fine—we want a single d-connected relationship between treatment and outcome—but the other two indicate that there is confounding from a and b. We can see what each of these paths are with the ggdag_paths() function from the ggdag package:\nggdag_paths(simple_dag_with_coords) + theme_dag() Instead of listing out all the possible paths and identifying backdoors by hand, you can use the adjustmentSets() function in the dagitty package to programmatically find all the nodes that need to be adjusted. Here we see that both a and b need to be controlled for to isolate the x -\u0026gt; y relationship.\nadjustmentSets(simple_dag) ## { a, b } You can also visualize the adjustment sets with ggdag_adjustment_set() in the ggdag package. Make sure you set shadow = TRUE to draw the arrows coming out of the adjusted nodes—by default, those are not included.\nggdag_adjustment_set(simple_dag_with_coords, shadow = TRUE) + theme_dag() R will find minimally sufficient adjustment sets, which includes the fewest number of adjustments needed to close all backdoors between x and y. In this example DAG there was only one set of variables (a and b), but in other situations there could be many possible sets, or none if the causal effect is not identifiable.\nPlot DAG from dagitty.net with ggdag() If you use dagitty.net to draw a DAG, you\u0026rsquo;ll notice that it generates some code for you in the model code section:\nYou can copy that dag{ ... } code and paste it into R to define a DAG object rather than use the dagify() function. To do this, use the dagitty() function from the dagitty library and include the whole generated model code in single quotes (''):\nmodel_from_dagitty \u0026lt;- dagitty(\u0026#39;dag { bb=\u0026#34;0,0,1,1\u0026#34; \u0026#34;A confounder\u0026#34; [pos=\u0026#34;0.809,0.306\u0026#34;] \u0026#34;Another confounder\u0026#34; [pos=\u0026#34;0.810,0.529\u0026#34;] \u0026#34;Some outcome\u0026#34; [outcome,pos=\u0026#34;0.918,0.432\u0026#34;] \u0026#34;Some treatment\u0026#34; [exposure,pos=\u0026#34;0.681,0.426\u0026#34;] \u0026#34;A confounder\u0026#34; -\u0026gt; \u0026#34;Some outcome\u0026#34; \u0026#34;A confounder\u0026#34; -\u0026gt; \u0026#34;Some treatment\u0026#34; \u0026#34;Another confounder\u0026#34; -\u0026gt; \u0026#34;Some outcome\u0026#34; \u0026#34;Another confounder\u0026#34; -\u0026gt; \u0026#34;Some treatment\u0026#34; \u0026#34;Some treatment\u0026#34; -\u0026gt; \u0026#34;Some outcome\u0026#34; } \u0026#39;) ggdag(model_from_dagitty) + theme_dag() By default it\u0026rsquo;s going to look ugly because (1) the node labels don\u0026rsquo;t fit, and (2) slight differences in the coordinates make it so the nodes don\u0026rsquo;t perfectly align with each other. To fix coordinate alignment, you can modify the numbers in the generated DAG code. Here I rounded the numbers so that they\u0026rsquo;re all 0.3, 0.8, etc.\nTo fix the label issue, you can add the use_labels argument like normally. Only here, you can\u0026rsquo;t specify use_labels = \u0026quot;label\u0026quot;. Instead, when you specify a DAG using dagitty\u0026rsquo;s code like this, the column in the underlying dataset that contains the labels is named name, so you need to use use_labels = \u0026quot;name\u0026quot;.\nOther ggdag() variations like ggdag_status() will still work fine.\nmodel_from_dagitty_rounded \u0026lt;- dagitty(\u0026#39;dag { bb=\u0026#34;0,0,1,1\u0026#34; \u0026#34;A confounder\u0026#34; [pos=\u0026#34;0.8,0.3\u0026#34;] \u0026#34;Another confounder\u0026#34; [pos=\u0026#34;0.8,0.5\u0026#34;] \u0026#34;Some outcome\u0026#34; [outcome,pos=\u0026#34;0.9,0.4\u0026#34;] \u0026#34;Some treatment\u0026#34; [exposure,pos=\u0026#34;0.7,0.4\u0026#34;] \u0026#34;A confounder\u0026#34; -\u0026gt; \u0026#34;Some outcome\u0026#34; \u0026#34;A confounder\u0026#34; -\u0026gt; \u0026#34;Some treatment\u0026#34; \u0026#34;Another confounder\u0026#34; -\u0026gt; \u0026#34;Some outcome\u0026#34; \u0026#34;Another confounder\u0026#34; -\u0026gt; \u0026#34;Some treatment\u0026#34; \u0026#34;Some treatment\u0026#34; -\u0026gt; \u0026#34;Some outcome\u0026#34; } \u0026#39;) ggdag_status(model_from_dagitty_rounded, text = FALSE, use_labels = \u0026#34;name\u0026#34;) + guides(color = FALSE) + # Turn off legend theme_dag() Mosquito net example Conditional independencies You can test if your stated relationships are correct by looking at the conditional independencies that are implied by the DAG. In dagitty.net, these appear in the sidebar in the \u0026ldquo;Testable implications\u0026rdquo; section. To show how this works, we\u0026rsquo;ll use a simulated dataset that I generated about a fictional mosquito net program. Download the data here if you want to follow along:\n  mosquito_nets.csv  Researchers are interested in whether using mosquito nets decreases an individual\u0026rsquo;s risk of contracting malaria. They have collected data from 1,752 households in an unnamed country and have variables related to environmental factors, individual health, and household characteristics. Additionally, this country has a special government program that provides free mosquito nets to households that meet specific requirements: to qualify for the program, there must be more than 4 members of the household, and the household\u0026rsquo;s monthly income must be lower than $700 a month. Households are not automatically enrolled in the program, and many do not use it. The data is not experimental—researchers have no control over who uses mosquito nets, and individual households make their own choices over whether to apply for free nets or buy their own nets, as well as whether they use the nets if they have them.\nmosquito_dag \u0026lt;- dagify( malaria_risk ~ net + income + health + temperature + resistance, net ~ income + health + temperature + eligible + household, eligible ~ income + household, health ~ income, exposure = \u0026#34;net\u0026#34;, outcome = \u0026#34;malaria_risk\u0026#34;, coords = list(x = c(malaria_risk = 7, net = 3, income = 4, health = 5, temperature = 6, resistance = 8.5, eligible = 2, household = 1), y = c(malaria_risk = 2, net = 2, income = 3, health = 1, temperature = 3, resistance = 2, eligible = 3, household = 2)), labels = c(malaria_risk = \u0026#34;Risk of malaria\u0026#34;, net = \u0026#34;Mosquito net\u0026#34;, income = \u0026#34;Income\u0026#34;, health = \u0026#34;Health\u0026#34;, temperature = \u0026#34;Nighttime temperatures\u0026#34;, resistance = \u0026#34;Insecticide resistance\u0026#34;, eligible = \u0026#34;Eligible for program\u0026#34;, household = \u0026#34;Number in household\u0026#34;) ) ggdag_status(mosquito_dag, use_labels = \u0026#34;label\u0026#34;, text = FALSE) + guides(fill = FALSE, color = FALSE) + # Disable the legend theme_dag() The causal graph above outlines the complete relationship between mosquito net use and risk of malaria. Each node in the DAG is a column in the dataset collected by the researchers, and includes the following:\n Malaria risk (malaria_risk): The likelihood that someone in the household will be infected with malaria. Measured on a scale of 0–100, with higher values indicating higher risk. Mosquito net (net and net_num): A binary variable indicating if the household used mosquito nets. Eligible for program (eligible): A binary variable indicating if the household is eligible for the free net program. Income (income): The household\u0026rsquo;s monthly income, in US dollars. Nighttime temperatures (temperature): The average temperature at night, in Celsius. Health (health): Self-reported healthiness in the household. Measured on a scale of 0–100, with higher values indicating better health. Number in household (household): Number of people living in the household. Insecticide resistance (resistance): Some strains of mosquitoes are more resistant to insecticide and thus pose a higher risk of infecting people with malaria. This is measured on a scale of 0–100, with higher values indicating higher resistance.  According to the DAG, malaria risk is caused by income, temperatures, health, insecticide resistance, and mosquito net use. People who live in hotter regions, have lower incomes, have worse health, are surrounded by mosquitoes with high resistance to insecticide, and who do not use mosquito nets are at higher risk of contracting malaria than those who do not. Mosquito net use is caused by income, nighttime temperatures, health, the number of people living in the house, and eligibility for the free net program. People who live in areas that are cooler at night, have higher incomes, have better health, have more people in the home, and are eligible for free government nets are more likely to regularly use nets than those who do not. The DAG also shows that eligibility for the free net program is caused by income and household size, since households must meet specific thresholds to qualify.\nFirst, let\u0026rsquo;s download the dataset, put in a folder named data, and load it:\n  mosquito_nets.csv  # Load the data. # It\u0026#39;d be a good idea to click on the \u0026#34;mosquito_nets.csv\u0026#34; object in the # Environment panel in RStudio to see what the data looks like after you load it mosquito_nets \u0026lt;- read_csv(\u0026#34;data/mosquito_nets.csv\u0026#34;) We can use this data to check if the relationships defined by our DAG reflect reality. Recall that d-separation implies that nodes are statistically independent of each other and do not transfer associational information. If you draw the mosquito net DAG with dagitty.net, or if you run impliedConditionalIndependencies() in R, you can see a list of all the implied conditional independencies.\nimpliedConditionalIndependencies(mosquito_dag) ## elgb _||_ hlth | incm ## elgb _||_ mlr_ | hlth, incm, net, tmpr ## elgb _||_ rsst ## elgb _||_ tmpr ## hlth _||_ hshl ## hlth _||_ rsst ## hlth _||_ tmpr ## hshl _||_ incm ## hshl _||_ mlr_ | hlth, incm, net, tmpr ## hshl _||_ rsst ## hshl _||_ tmpr ## incm _||_ rsst ## incm _||_ tmpr ## net _||_ rsst ## rsst _||_ tmpr The _||_ symbol in the output here is the \\(\\perp\\) symbol, which means \u0026ldquo;independent of\u0026rdquo;. The | in the output means \u0026ldquo;given\u0026rdquo;.\nIn the interest of space, we will not verify all these implied independencies, but we can test a few of them:\n  \\(\\text{Health} \\perp \\text{Household members}\\): (Read as \u0026ldquo;Health is independent of household member count\u0026rdquo;.) Health should be independent of the number of people in each household. In the data, the two variables should not be correlated. This is indeed the case:\ncor(mosquito_nets$health, mosquito_nets$household) ## [1] 9.8e-05   \\(\\text{Income} \\perp \\text{Insecticide resistance}\\): (Read as \u0026ldquo;Income is independent of insecticide resistance\u0026rdquo;.) Income should be independent of insecticide resistance. This is again true:\ncor(mosquito_nets$income, mosquito_nets$resistance) ## [1] 0.014   \\(\\text{Malaria risk} \\perp \\text{Household members}\\ |\\ \\text{Health, Income, Bed net use, Temperature}\\): (Read as \u0026ldquo;Malaria risk is independent of house member count given health, income, bed net use, and temperature\u0026rdquo;.) Malaria risk should be independent of the number of household members given similar levels of health, income, mosquito net use, and nighttime temperatures. We cannot use cor() to test this implication, since there are many variables involved, but we can use a regression model to check if the number of household members is significantly related to malaria risk. It is not significant ($t = -0.17$, \\(p = 0.863\\)), which means the two are independent, as expected.\nlm(malaria_risk ~ household + health + income + net + temperature, data = mosquito_nets) %\u0026gt;% broom::tidy() ## # A tibble: 6 x 5 ## term estimate std.error statistic p.value ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 (Intercept) 76.2 0.966 78.9 0.  ## 2 household -0.0155 0.0893 -0.173 8.63e- 1 ## 3 health 0.148 0.0107 13.9 9.75e- 42 ## 4 income -0.0751 0.00104 -72.6 0.  ## 5 netTRUE -10.4 0.266 -39.2 2.63e-241 ## 6 temperature 1.01 0.0310 32.5 1.88e-181   We can check all the other conditional independencies to see if the DAG captures the reality of the full system of factors that influence mosquito net use and malaria risk. If there are substantial and significant correlations between nodes that should be independent, there is likely an issue with the specification of the DAG. Return to the theory of how the phenomena are generated and refine the DAG more.\nMosquito net adjustment sets There is a direct path between mosquito net use and the risk of malaria, but the effect is not causally identified due to several other open paths. We can either list out all the paths and find which open paths have arrows pointing backwards into the mosquito net node (run paths(mosquito_dag) to see these results), or we can let R find the appropriate adjustment sets automatically:\nadjustmentSets(mosquito_dag) ## { health, income, temperature } Based on the relationships between all the nodes in the DAG, adjusting for health, income, and temperature is enough to close all backdoors and identify the relationship between net use and malaria risk. Importantly, we do not need to worry about any of the nodes related to the government program for free nets, since those nodes are not d-connected to malaria risk. We only need to worry about confounding relationships.\nWe can confirm this graphically with ggdag_adjustment_set():\nggdag_adjustment_set(mosquito_dag, shadow = TRUE, use_labels = \u0026#34;label\u0026#34;, text = FALSE) + theme_dag() ","date":1632700800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1632700800,"objectID":"ac44be22ccfcf2ee3a427ed6eaf14c79","permalink":"/example/07-practico/","publishdate":"2021-09-27T00:00:00Z","relpermalink":"/example/07-practico/","section":"example","summary":"DAGs with dagitty.net The easiest way to quickly build DAGs and find adjustment sets and testable implications is to use dagitty.net.\nThis video shows how to use it:\n\r\rDAGs with R, ggdag, and dagitty You can use the ggdag and dagitty packages in R to build and work with DAGs too.","tags":null,"title":"Muestras complejas","type":"docs"},{"authors":null,"categories":null,"content":"Readings   Randall Munroe, “Significant”  Alexander Coppock, “10 Things to Know About Statistical Power”  Play around with FiveThirtyEight, “Hack Your Way To Scientific Glory”  Chapter 9 in Impact Evaluation in Practice1  Slides The slides for today’s lesson are available online as an HTML file. Use the buttons below to open the slides either as an interactive website or as a static PDF (for printing or storing for later). You can also click in the slides below and navigate through them with your left and right arrow keys.\n View all slides in new window  Download PDF of all slides\nIntroduction\r\rConstruct validity\r\rStatistical conclusion validity\r\rInternal validity\r\rExternal validity\r\r\r\r\r\r\r\r\r\r\r\r\r\r\r\r\r\r\rFun fact: If you type ? (or shift + /) while going through the slides, you can see a list of special slide-specific commands.\r\rVideos Videos for each section of the lecture are available at this YouTube playlist.\n Introduction Construct validity Statistical conclusion validity Internal validity External validity  You can also watch the playlist (and skip around to different sections) here:\n\r\r  Paul J. Gertler et al., Impact Evaluation in Practice, 2nd ed. (Inter-American Development Bank; World Bank, 2016), https://openknowledge.worldbank.org/handle/10986/25030. \u0026#x21a9;\u0026#xfe0e;\n   ","date":1631491200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1631491200,"objectID":"ab54958bbaea5fe381467c40300e6fb1","permalink":"/content/06-content/","publishdate":"2021-09-13T00:00:00Z","relpermalink":"/content/06-content/","section":"content","summary":"Readings   Randall Munroe, “Significant”  Alexander Coppock, “10 Things to Know About Statistical Power”  Play around with FiveThirtyEight, “Hack Your Way To Scientific Glory”  Chapter 9 in Impact Evaluation in Practice1  Slides The slides for today’s lesson are available online as an HTML file.","tags":null,"title":"Análisis descriptivo","type":"docs"},{"authors":null,"categories":null,"content":"DAGs with dagitty.net The easiest way to quickly build DAGs and find adjustment sets and testable implications is to use dagitty.net.\nThis video shows how to use it:\n\r\rDAGs with R, ggdag, and dagitty You can use the ggdag and dagitty packages in R to build and work with DAGs too. I typically draw an initial DAG in my browser with dagitty.net and then I rewrite it in code in R so that it\u0026rsquo;s more official and formal and reproducible.\nLive coding example \r\rBasic DAGs (This is a heavily cleaned up and annotated version of the code from the video.)\n# Load packages library(tidyverse) # For dplyr, ggplot, and friends library(ggdag) # For plotting DAGs library(dagitty) # For working with DAG logic The general process for making and working with DAGs in R is to create a DAG object with dagify() and then plot it with ggdag(). The documentation for ggdag is really good and helpful and full of examples. Check these pages for all sorts of additional details:\n \u0026ldquo;An introduction to ggdag\u0026rdquo; \u0026ldquo;An introduction to directed acyclic graphs\u0026rdquo; List of all ggdag-related functions  The syntax for dagify() is similar to the formula syntax you\u0026rsquo;ve been using for building regression models with lm(). The formulas you use in dagify() indicate the relationships between nodes.\nFor instance, in this DAG, y is caused by x, a, and b (y ~ x + a + b), and x is caused by a and b (x ~ a + b). This means that a and b are confounders. Use the exposure and outcome arguments to specify which nodes are the exposure (or treatment/intervention/program) and the outcome.\n# Create super basic DAG simple_dag \u0026lt;- dagify( y ~ x + a + b, x ~ a + b, exposure = \u0026#34;x\u0026#34;, outcome = \u0026#34;y\u0026#34; ) # Adding a theme_dag() layer to the plot makes it have a white background with no axis labels ggdag(simple_dag) + theme_dag() If you want to plot which nodes are the exposure and outcome, use ggdag_status() instead:\nggdag_status(simple_dag) + theme_dag() Layouts and manual coordinates Notice how the layout is different in both of those graphs. By default, ggdag() positions the nodes randomly every time using a network algorithm. You can change the algorithm by using the layout argument: ggdag(simple_dag, layout = \u0026quot;nicely\u0026quot;). You can see a full list of possible algorithms by running ?layout_tbl_graph_igraph in the console.\nAlternatively, you can specify your own coordinates so that the nodes are positioned in the same place every time. Do this with the coords argument in dagify().\nThe best way to figure out what these numbers should be is to draw the DAG on paper or on a whiteboard and add a grid to it and then figure out the coordinates. For instance, in this DAG there are three rows and three columns: x and y go in the middle row (row 2) while a and b go in the middle column (column 2). It can also be helpful to not include theme_dag() at first so you can see the numbers for the underlying grid. Once you have everything positioned correctly, add theme_dag() to clean it up.\nsimple_dag_with_coords \u0026lt;- dagify( y ~ x + a + b, x ~ a + b, exposure = \u0026#34;x\u0026#34;, outcome = \u0026#34;y\u0026#34;, coords = list(x = c(x = 1, a = 2, b = 2, y = 3), y = c(x = 2, a = 1, b = 3, y = 2)) ) ggdag_status(simple_dag_with_coords) + theme_dag() Node names and labels The variable names you use do not have to be limited to just x, y, and other lowercase letters. You can any names you want, as long as there are no spaces.\ndag_with_var_names \u0026lt;- dagify( outcome ~ treatment + confounder1 + confounder2, treatment ~ confounder1 + confounder2, exposure = \u0026#34;treatment\u0026#34;, outcome = \u0026#34;outcome\u0026#34; ) ggdag_status(dag_with_var_names) + theme_dag() However, unless you use very short names, it is likely that the text will not fit inside the nodes. To get around this, you can add labels to the nodes using the labels argument in dagify(). Plot the labels by setting use_labels = \u0026quot;label\u0026quot; in ggdag(). You can turn off the text in the nodes with text = FALSE in ggdag().\nsimple_dag_with_coords_and_labels \u0026lt;- dagify( y ~ x + a + b, x ~ a + b, exposure = \u0026#34;x\u0026#34;, outcome = \u0026#34;y\u0026#34;, labels = c(y = \u0026#34;Outcome\u0026#34;, x = \u0026#34;Treatment\u0026#34;, a = \u0026#34;Confounder 1\u0026#34;, b = \u0026#34;Confounder 2\u0026#34;), coords = list(x = c(x = 1, a = 2, b = 2, y = 3), y = c(x = 2, a = 1, b = 3, y = 2)) ) ggdag_status(simple_dag_with_coords_and_labels, use_labels = \u0026#34;label\u0026#34;, text = FALSE) + guides(fill = FALSE, color = FALSE) + # Disable the legend theme_dag() Paths and adjustment sets R can also perform analysis on DAG objects. For example, we can find all the testable implications from the DAG using the impliedConditionalIndependencies() function from the dagitty package. For this simple DAG, there is only one: a should be independent of b. If we had a dataset with columns for each of these variables, we could check if this is true by running cor(a, b) to see if the two are related.\nimpliedConditionalIndependencies(simple_dag) ## a _||_ b We can also find all the paths between x and y using the paths() function from the dagitty package. We can see that there are three open paths between x and y:\npaths(simple_dag) ## $paths ## [1] \u0026#34;x -\u0026gt; y\u0026#34; \u0026#34;x \u0026lt;- a -\u0026gt; y\u0026#34; \u0026#34;x \u0026lt;- b -\u0026gt; y\u0026#34; ##  ## $open ## [1] TRUE TRUE TRUE The first open path is fine—we want a single d-connected relationship between treatment and outcome—but the other two indicate that there is confounding from a and b. We can see what each of these paths are with the ggdag_paths() function from the ggdag package:\nggdag_paths(simple_dag_with_coords) + theme_dag() Instead of listing out all the possible paths and identifying backdoors by hand, you can use the adjustmentSets() function in the dagitty package to programmatically find all the nodes that need to be adjusted. Here we see that both a and b need to be controlled for to isolate the x -\u0026gt; y relationship.\nadjustmentSets(simple_dag) ## { a, b } You can also visualize the adjustment sets with ggdag_adjustment_set() in the ggdag package. Make sure you set shadow = TRUE to draw the arrows coming out of the adjusted nodes—by default, those are not included.\nggdag_adjustment_set(simple_dag_with_coords, shadow = TRUE) + theme_dag() R will find minimally sufficient adjustment sets, which includes the fewest number of adjustments needed to close all backdoors between x and y. In this example DAG there was only one set of variables (a and b), but in other situations there could be many possible sets, or none if the causal effect is not identifiable.\nPlot DAG from dagitty.net with ggdag() If you use dagitty.net to draw a DAG, you\u0026rsquo;ll notice that it generates some code for you in the model code section:\nYou can copy that dag{ ... } code and paste it into R to define a DAG object rather than use the dagify() function. To do this, use the dagitty() function from the dagitty library and include the whole generated model code in single quotes (''):\nmodel_from_dagitty \u0026lt;- dagitty(\u0026#39;dag { bb=\u0026#34;0,0,1,1\u0026#34; \u0026#34;A confounder\u0026#34; [pos=\u0026#34;0.809,0.306\u0026#34;] \u0026#34;Another confounder\u0026#34; [pos=\u0026#34;0.810,0.529\u0026#34;] \u0026#34;Some outcome\u0026#34; [outcome,pos=\u0026#34;0.918,0.432\u0026#34;] \u0026#34;Some treatment\u0026#34; [exposure,pos=\u0026#34;0.681,0.426\u0026#34;] \u0026#34;A confounder\u0026#34; -\u0026gt; \u0026#34;Some outcome\u0026#34; \u0026#34;A confounder\u0026#34; -\u0026gt; \u0026#34;Some treatment\u0026#34; \u0026#34;Another confounder\u0026#34; -\u0026gt; \u0026#34;Some outcome\u0026#34; \u0026#34;Another confounder\u0026#34; -\u0026gt; \u0026#34;Some treatment\u0026#34; \u0026#34;Some treatment\u0026#34; -\u0026gt; \u0026#34;Some outcome\u0026#34; } \u0026#39;) ggdag(model_from_dagitty) + theme_dag() By default it\u0026rsquo;s going to look ugly because (1) the node labels don\u0026rsquo;t fit, and (2) slight differences in the coordinates make it so the nodes don\u0026rsquo;t perfectly align with each other. To fix coordinate alignment, you can modify the numbers in the generated DAG code. Here I rounded the numbers so that they\u0026rsquo;re all 0.3, 0.8, etc.\nTo fix the label issue, you can add the use_labels argument like normally. Only here, you can\u0026rsquo;t specify use_labels = \u0026quot;label\u0026quot;. Instead, when you specify a DAG using dagitty\u0026rsquo;s code like this, the column in the underlying dataset that contains the labels is named name, so you need to use use_labels = \u0026quot;name\u0026quot;.\nOther ggdag() variations like ggdag_status() will still work fine.\nmodel_from_dagitty_rounded \u0026lt;- dagitty(\u0026#39;dag { bb=\u0026#34;0,0,1,1\u0026#34; \u0026#34;A confounder\u0026#34; [pos=\u0026#34;0.8,0.3\u0026#34;] \u0026#34;Another confounder\u0026#34; [pos=\u0026#34;0.8,0.5\u0026#34;] \u0026#34;Some outcome\u0026#34; [outcome,pos=\u0026#34;0.9,0.4\u0026#34;] \u0026#34;Some treatment\u0026#34; [exposure,pos=\u0026#34;0.7,0.4\u0026#34;] \u0026#34;A confounder\u0026#34; -\u0026gt; \u0026#34;Some outcome\u0026#34; \u0026#34;A confounder\u0026#34; -\u0026gt; \u0026#34;Some treatment\u0026#34; \u0026#34;Another confounder\u0026#34; -\u0026gt; \u0026#34;Some outcome\u0026#34; \u0026#34;Another confounder\u0026#34; -\u0026gt; \u0026#34;Some treatment\u0026#34; \u0026#34;Some treatment\u0026#34; -\u0026gt; \u0026#34;Some outcome\u0026#34; } \u0026#39;) ggdag_status(model_from_dagitty_rounded, text = FALSE, use_labels = \u0026#34;name\u0026#34;) + guides(color = FALSE) + # Turn off legend theme_dag() Mosquito net example Conditional independencies You can test if your stated relationships are correct by looking at the conditional independencies that are implied by the DAG. In dagitty.net, these appear in the sidebar in the \u0026ldquo;Testable implications\u0026rdquo; section. To show how this works, we\u0026rsquo;ll use a simulated dataset that I generated about a fictional mosquito net program. Download the data here if you want to follow along:\n  mosquito_nets.csv  Researchers are interested in whether using mosquito nets decreases an individual\u0026rsquo;s risk of contracting malaria. They have collected data from 1,752 households in an unnamed country and have variables related to environmental factors, individual health, and household characteristics. Additionally, this country has a special government program that provides free mosquito nets to households that meet specific requirements: to qualify for the program, there must be more than 4 members of the household, and the household\u0026rsquo;s monthly income must be lower than $700 a month. Households are not automatically enrolled in the program, and many do not use it. The data is not experimental—researchers have no control over who uses mosquito nets, and individual households make their own choices over whether to apply for free nets or buy their own nets, as well as whether they use the nets if they have them.\nmosquito_dag \u0026lt;- dagify( malaria_risk ~ net + income + health + temperature + resistance, net ~ income + health + temperature + eligible + household, eligible ~ income + household, health ~ income, exposure = \u0026#34;net\u0026#34;, outcome = \u0026#34;malaria_risk\u0026#34;, coords = list(x = c(malaria_risk = 7, net = 3, income = 4, health = 5, temperature = 6, resistance = 8.5, eligible = 2, household = 1), y = c(malaria_risk = 2, net = 2, income = 3, health = 1, temperature = 3, resistance = 2, eligible = 3, household = 2)), labels = c(malaria_risk = \u0026#34;Risk of malaria\u0026#34;, net = \u0026#34;Mosquito net\u0026#34;, income = \u0026#34;Income\u0026#34;, health = \u0026#34;Health\u0026#34;, temperature = \u0026#34;Nighttime temperatures\u0026#34;, resistance = \u0026#34;Insecticide resistance\u0026#34;, eligible = \u0026#34;Eligible for program\u0026#34;, household = \u0026#34;Number in household\u0026#34;) ) ggdag_status(mosquito_dag, use_labels = \u0026#34;label\u0026#34;, text = FALSE) + guides(fill = FALSE, color = FALSE) + # Disable the legend theme_dag() The causal graph above outlines the complete relationship between mosquito net use and risk of malaria. Each node in the DAG is a column in the dataset collected by the researchers, and includes the following:\n Malaria risk (malaria_risk): The likelihood that someone in the household will be infected with malaria. Measured on a scale of 0–100, with higher values indicating higher risk. Mosquito net (net and net_num): A binary variable indicating if the household used mosquito nets. Eligible for program (eligible): A binary variable indicating if the household is eligible for the free net program. Income (income): The household\u0026rsquo;s monthly income, in US dollars. Nighttime temperatures (temperature): The average temperature at night, in Celsius. Health (health): Self-reported healthiness in the household. Measured on a scale of 0–100, with higher values indicating better health. Number in household (household): Number of people living in the household. Insecticide resistance (resistance): Some strains of mosquitoes are more resistant to insecticide and thus pose a higher risk of infecting people with malaria. This is measured on a scale of 0–100, with higher values indicating higher resistance.  According to the DAG, malaria risk is caused by income, temperatures, health, insecticide resistance, and mosquito net use. People who live in hotter regions, have lower incomes, have worse health, are surrounded by mosquitoes with high resistance to insecticide, and who do not use mosquito nets are at higher risk of contracting malaria than those who do not. Mosquito net use is caused by income, nighttime temperatures, health, the number of people living in the house, and eligibility for the free net program. People who live in areas that are cooler at night, have higher incomes, have better health, have more people in the home, and are eligible for free government nets are more likely to regularly use nets than those who do not. The DAG also shows that eligibility for the free net program is caused by income and household size, since households must meet specific thresholds to qualify.\nFirst, let\u0026rsquo;s download the dataset, put in a folder named data, and load it:\n  mosquito_nets.csv  # Load the data. # It\u0026#39;d be a good idea to click on the \u0026#34;mosquito_nets.csv\u0026#34; object in the # Environment panel in RStudio to see what the data looks like after you load it mosquito_nets \u0026lt;- read_csv(\u0026#34;data/mosquito_nets.csv\u0026#34;) We can use this data to check if the relationships defined by our DAG reflect reality. Recall that d-separation implies that nodes are statistically independent of each other and do not transfer associational information. If you draw the mosquito net DAG with dagitty.net, or if you run impliedConditionalIndependencies() in R, you can see a list of all the implied conditional independencies.\nimpliedConditionalIndependencies(mosquito_dag) ## elgb _||_ hlth | incm ## elgb _||_ mlr_ | hlth, incm, net, tmpr ## elgb _||_ rsst ## elgb _||_ tmpr ## hlth _||_ hshl ## hlth _||_ rsst ## hlth _||_ tmpr ## hshl _||_ incm ## hshl _||_ mlr_ | hlth, incm, net, tmpr ## hshl _||_ rsst ## hshl _||_ tmpr ## incm _||_ rsst ## incm _||_ tmpr ## net _||_ rsst ## rsst _||_ tmpr The _||_ symbol in the output here is the \\(\\perp\\) symbol, which means \u0026ldquo;independent of\u0026rdquo;. The | in the output means \u0026ldquo;given\u0026rdquo;.\nIn the interest of space, we will not verify all these implied independencies, but we can test a few of them:\n  \\(\\text{Health} \\perp \\text{Household members}\\): (Read as \u0026ldquo;Health is independent of household member count\u0026rdquo;.) Health should be independent of the number of people in each household. In the data, the two variables should not be correlated. This is indeed the case:\ncor(mosquito_nets$health, mosquito_nets$household) ## [1] 9.8e-05   \\(\\text{Income} \\perp \\text{Insecticide resistance}\\): (Read as \u0026ldquo;Income is independent of insecticide resistance\u0026rdquo;.) Income should be independent of insecticide resistance. This is again true:\ncor(mosquito_nets$income, mosquito_nets$resistance) ## [1] 0.014   \\(\\text{Malaria risk} \\perp \\text{Household members}\\ |\\ \\text{Health, Income, Bed net use, Temperature}\\): (Read as \u0026ldquo;Malaria risk is independent of house member count given health, income, bed net use, and temperature\u0026rdquo;.) Malaria risk should be independent of the number of household members given similar levels of health, income, mosquito net use, and nighttime temperatures. We cannot use cor() to test this implication, since there are many variables involved, but we can use a regression model to check if the number of household members is significantly related to malaria risk. It is not significant ($t = -0.17$, \\(p = 0.863\\)), which means the two are independent, as expected.\nlm(malaria_risk ~ household + health + income + net + temperature, data = mosquito_nets) %\u0026gt;% broom::tidy() ## # A tibble: 6 x 5 ## term estimate std.error statistic p.value ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 (Intercept) 76.2 0.966 78.9 0.  ## 2 household -0.0155 0.0893 -0.173 8.63e- 1 ## 3 health 0.148 0.0107 13.9 9.75e- 42 ## 4 income -0.0751 0.00104 -72.6 0.  ## 5 netTRUE -10.4 0.266 -39.2 2.63e-241 ## 6 temperature 1.01 0.0310 32.5 1.88e-181   We can check all the other conditional independencies to see if the DAG captures the reality of the full system of factors that influence mosquito net use and malaria risk. If there are substantial and significant correlations between nodes that should be independent, there is likely an issue with the specification of the DAG. Return to the theory of how the phenomena are generated and refine the DAG more.\nMosquito net adjustment sets There is a direct path between mosquito net use and the risk of malaria, but the effect is not causally identified due to several other open paths. We can either list out all the paths and find which open paths have arrows pointing backwards into the mosquito net node (run paths(mosquito_dag) to see these results), or we can let R find the appropriate adjustment sets automatically:\nadjustmentSets(mosquito_dag) ## { health, income, temperature } Based on the relationships between all the nodes in the DAG, adjusting for health, income, and temperature is enough to close all backdoors and identify the relationship between net use and malaria risk. Importantly, we do not need to worry about any of the nodes related to the government program for free nets, since those nodes are not d-connected to malaria risk. We only need to worry about confounding relationships.\nWe can confirm this graphically with ggdag_adjustment_set():\nggdag_adjustment_set(mosquito_dag, shadow = TRUE, use_labels = \u0026#34;label\u0026#34;, text = FALSE) + theme_dag() ","date":1631491200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1631491200,"objectID":"757ea7660f0f62bd23967c18561cd81b","permalink":"/example/06-practico/","publishdate":"2021-09-13T00:00:00Z","relpermalink":"/example/06-practico/","section":"example","summary":"DAGs with dagitty.net The easiest way to quickly build DAGs and find adjustment sets and testable implications is to use dagitty.net.\nThis video shows how to use it:\n\r\rDAGs with R, ggdag, and dagitty You can use the ggdag and dagitty packages in R to build and work with DAGs too.","tags":null,"title":"Análisis descriptivo","type":"docs"},{"authors":null,"categories":null,"content":"DAGs with dagitty.net The easiest way to quickly build DAGs and find adjustment sets and testable implications is to use dagitty.net.\nThis video shows how to use it:\n\r\rDAGs with R, ggdag, and dagitty You can use the ggdag and dagitty packages in R to build and work with DAGs too. I typically draw an initial DAG in my browser with dagitty.net and then I rewrite it in code in R so that it\u0026rsquo;s more official and formal and reproducible.\nLive coding example \r\rBasic DAGs (This is a heavily cleaned up and annotated version of the code from the video.)\n# Load packages library(tidyverse) # For dplyr, ggplot, and friends library(ggdag) # For plotting DAGs library(dagitty) # For working with DAG logic The general process for making and working with DAGs in R is to create a DAG object with dagify() and then plot it with ggdag(). The documentation for ggdag is really good and helpful and full of examples. Check these pages for all sorts of additional details:\n \u0026ldquo;An introduction to ggdag\u0026rdquo; \u0026ldquo;An introduction to directed acyclic graphs\u0026rdquo; List of all ggdag-related functions  The syntax for dagify() is similar to the formula syntax you\u0026rsquo;ve been using for building regression models with lm(). The formulas you use in dagify() indicate the relationships between nodes.\nFor instance, in this DAG, y is caused by x, a, and b (y ~ x + a + b), and x is caused by a and b (x ~ a + b). This means that a and b are confounders. Use the exposure and outcome arguments to specify which nodes are the exposure (or treatment/intervention/program) and the outcome.\n# Create super basic DAG simple_dag \u0026lt;- dagify( y ~ x + a + b, x ~ a + b, exposure = \u0026#34;x\u0026#34;, outcome = \u0026#34;y\u0026#34; ) # Adding a theme_dag() layer to the plot makes it have a white background with no axis labels ggdag(simple_dag) + theme_dag() If you want to plot which nodes are the exposure and outcome, use ggdag_status() instead:\nggdag_status(simple_dag) + theme_dag() Layouts and manual coordinates Notice how the layout is different in both of those graphs. By default, ggdag() positions the nodes randomly every time using a network algorithm. You can change the algorithm by using the layout argument: ggdag(simple_dag, layout = \u0026quot;nicely\u0026quot;). You can see a full list of possible algorithms by running ?layout_tbl_graph_igraph in the console.\nAlternatively, you can specify your own coordinates so that the nodes are positioned in the same place every time. Do this with the coords argument in dagify().\nThe best way to figure out what these numbers should be is to draw the DAG on paper or on a whiteboard and add a grid to it and then figure out the coordinates. For instance, in this DAG there are three rows and three columns: x and y go in the middle row (row 2) while a and b go in the middle column (column 2). It can also be helpful to not include theme_dag() at first so you can see the numbers for the underlying grid. Once you have everything positioned correctly, add theme_dag() to clean it up.\nsimple_dag_with_coords \u0026lt;- dagify( y ~ x + a + b, x ~ a + b, exposure = \u0026#34;x\u0026#34;, outcome = \u0026#34;y\u0026#34;, coords = list(x = c(x = 1, a = 2, b = 2, y = 3), y = c(x = 2, a = 1, b = 3, y = 2)) ) ggdag_status(simple_dag_with_coords) + theme_dag() Node names and labels The variable names you use do not have to be limited to just x, y, and other lowercase letters. You can any names you want, as long as there are no spaces.\ndag_with_var_names \u0026lt;- dagify( outcome ~ treatment + confounder1 + confounder2, treatment ~ confounder1 + confounder2, exposure = \u0026#34;treatment\u0026#34;, outcome = \u0026#34;outcome\u0026#34; ) ggdag_status(dag_with_var_names) + theme_dag() However, unless you use very short names, it is likely that the text will not fit inside the nodes. To get around this, you can add labels to the nodes using the labels argument in dagify(). Plot the labels by setting use_labels = \u0026quot;label\u0026quot; in ggdag(). You can turn off the text in the nodes with text = FALSE in ggdag().\nsimple_dag_with_coords_and_labels \u0026lt;- dagify( y ~ x + a + b, x ~ a + b, exposure = \u0026#34;x\u0026#34;, outcome = \u0026#34;y\u0026#34;, labels = c(y = \u0026#34;Outcome\u0026#34;, x = \u0026#34;Treatment\u0026#34;, a = \u0026#34;Confounder 1\u0026#34;, b = \u0026#34;Confounder 2\u0026#34;), coords = list(x = c(x = 1, a = 2, b = 2, y = 3), y = c(x = 2, a = 1, b = 3, y = 2)) ) ggdag_status(simple_dag_with_coords_and_labels, use_labels = \u0026#34;label\u0026#34;, text = FALSE) + guides(fill = FALSE, color = FALSE) + # Disable the legend theme_dag() Paths and adjustment sets R can also perform analysis on DAG objects. For example, we can find all the testable implications from the DAG using the impliedConditionalIndependencies() function from the dagitty package. For this simple DAG, there is only one: a should be independent of b. If we had a dataset with columns for each of these variables, we could check if this is true by running cor(a, b) to see if the two are related.\nimpliedConditionalIndependencies(simple_dag) ## a _||_ b We can also find all the paths between x and y using the paths() function from the dagitty package. We can see that there are three open paths between x and y:\npaths(simple_dag) ## $paths ## [1] \u0026#34;x -\u0026gt; y\u0026#34; \u0026#34;x \u0026lt;- a -\u0026gt; y\u0026#34; \u0026#34;x \u0026lt;- b -\u0026gt; y\u0026#34; ##  ## $open ## [1] TRUE TRUE TRUE The first open path is fine—we want a single d-connected relationship between treatment and outcome—but the other two indicate that there is confounding from a and b. We can see what each of these paths are with the ggdag_paths() function from the ggdag package:\nggdag_paths(simple_dag_with_coords) + theme_dag() Instead of listing out all the possible paths and identifying backdoors by hand, you can use the adjustmentSets() function in the dagitty package to programmatically find all the nodes that need to be adjusted. Here we see that both a and b need to be controlled for to isolate the x -\u0026gt; y relationship.\nadjustmentSets(simple_dag) ## { a, b } You can also visualize the adjustment sets with ggdag_adjustment_set() in the ggdag package. Make sure you set shadow = TRUE to draw the arrows coming out of the adjusted nodes—by default, those are not included.\nggdag_adjustment_set(simple_dag_with_coords, shadow = TRUE) + theme_dag() R will find minimally sufficient adjustment sets, which includes the fewest number of adjustments needed to close all backdoors between x and y. In this example DAG there was only one set of variables (a and b), but in other situations there could be many possible sets, or none if the causal effect is not identifiable.\nPlot DAG from dagitty.net with ggdag() If you use dagitty.net to draw a DAG, you\u0026rsquo;ll notice that it generates some code for you in the model code section:\nYou can copy that dag{ ... } code and paste it into R to define a DAG object rather than use the dagify() function. To do this, use the dagitty() function from the dagitty library and include the whole generated model code in single quotes (''):\nmodel_from_dagitty \u0026lt;- dagitty(\u0026#39;dag { bb=\u0026#34;0,0,1,1\u0026#34; \u0026#34;A confounder\u0026#34; [pos=\u0026#34;0.809,0.306\u0026#34;] \u0026#34;Another confounder\u0026#34; [pos=\u0026#34;0.810,0.529\u0026#34;] \u0026#34;Some outcome\u0026#34; [outcome,pos=\u0026#34;0.918,0.432\u0026#34;] \u0026#34;Some treatment\u0026#34; [exposure,pos=\u0026#34;0.681,0.426\u0026#34;] \u0026#34;A confounder\u0026#34; -\u0026gt; \u0026#34;Some outcome\u0026#34; \u0026#34;A confounder\u0026#34; -\u0026gt; \u0026#34;Some treatment\u0026#34; \u0026#34;Another confounder\u0026#34; -\u0026gt; \u0026#34;Some outcome\u0026#34; \u0026#34;Another confounder\u0026#34; -\u0026gt; \u0026#34;Some treatment\u0026#34; \u0026#34;Some treatment\u0026#34; -\u0026gt; \u0026#34;Some outcome\u0026#34; } \u0026#39;) ggdag(model_from_dagitty) + theme_dag() By default it\u0026rsquo;s going to look ugly because (1) the node labels don\u0026rsquo;t fit, and (2) slight differences in the coordinates make it so the nodes don\u0026rsquo;t perfectly align with each other. To fix coordinate alignment, you can modify the numbers in the generated DAG code. Here I rounded the numbers so that they\u0026rsquo;re all 0.3, 0.8, etc.\nTo fix the label issue, you can add the use_labels argument like normally. Only here, you can\u0026rsquo;t specify use_labels = \u0026quot;label\u0026quot;. Instead, when you specify a DAG using dagitty\u0026rsquo;s code like this, the column in the underlying dataset that contains the labels is named name, so you need to use use_labels = \u0026quot;name\u0026quot;.\nOther ggdag() variations like ggdag_status() will still work fine.\nmodel_from_dagitty_rounded \u0026lt;- dagitty(\u0026#39;dag { bb=\u0026#34;0,0,1,1\u0026#34; \u0026#34;A confounder\u0026#34; [pos=\u0026#34;0.8,0.3\u0026#34;] \u0026#34;Another confounder\u0026#34; [pos=\u0026#34;0.8,0.5\u0026#34;] \u0026#34;Some outcome\u0026#34; [outcome,pos=\u0026#34;0.9,0.4\u0026#34;] \u0026#34;Some treatment\u0026#34; [exposure,pos=\u0026#34;0.7,0.4\u0026#34;] \u0026#34;A confounder\u0026#34; -\u0026gt; \u0026#34;Some outcome\u0026#34; \u0026#34;A confounder\u0026#34; -\u0026gt; \u0026#34;Some treatment\u0026#34; \u0026#34;Another confounder\u0026#34; -\u0026gt; \u0026#34;Some outcome\u0026#34; \u0026#34;Another confounder\u0026#34; -\u0026gt; \u0026#34;Some treatment\u0026#34; \u0026#34;Some treatment\u0026#34; -\u0026gt; \u0026#34;Some outcome\u0026#34; } \u0026#39;) ggdag_status(model_from_dagitty_rounded, text = FALSE, use_labels = \u0026#34;name\u0026#34;) + guides(color = FALSE) + # Turn off legend theme_dag() Mosquito net example Conditional independencies You can test if your stated relationships are correct by looking at the conditional independencies that are implied by the DAG. In dagitty.net, these appear in the sidebar in the \u0026ldquo;Testable implications\u0026rdquo; section. To show how this works, we\u0026rsquo;ll use a simulated dataset that I generated about a fictional mosquito net program. Download the data here if you want to follow along:\n  mosquito_nets.csv  Researchers are interested in whether using mosquito nets decreases an individual\u0026rsquo;s risk of contracting malaria. They have collected data from 1,752 households in an unnamed country and have variables related to environmental factors, individual health, and household characteristics. Additionally, this country has a special government program that provides free mosquito nets to households that meet specific requirements: to qualify for the program, there must be more than 4 members of the household, and the household\u0026rsquo;s monthly income must be lower than $700 a month. Households are not automatically enrolled in the program, and many do not use it. The data is not experimental—researchers have no control over who uses mosquito nets, and individual households make their own choices over whether to apply for free nets or buy their own nets, as well as whether they use the nets if they have them.\nmosquito_dag \u0026lt;- dagify( malaria_risk ~ net + income + health + temperature + resistance, net ~ income + health + temperature + eligible + household, eligible ~ income + household, health ~ income, exposure = \u0026#34;net\u0026#34;, outcome = \u0026#34;malaria_risk\u0026#34;, coords = list(x = c(malaria_risk = 7, net = 3, income = 4, health = 5, temperature = 6, resistance = 8.5, eligible = 2, household = 1), y = c(malaria_risk = 2, net = 2, income = 3, health = 1, temperature = 3, resistance = 2, eligible = 3, household = 2)), labels = c(malaria_risk = \u0026#34;Risk of malaria\u0026#34;, net = \u0026#34;Mosquito net\u0026#34;, income = \u0026#34;Income\u0026#34;, health = \u0026#34;Health\u0026#34;, temperature = \u0026#34;Nighttime temperatures\u0026#34;, resistance = \u0026#34;Insecticide resistance\u0026#34;, eligible = \u0026#34;Eligible for program\u0026#34;, household = \u0026#34;Number in household\u0026#34;) ) ggdag_status(mosquito_dag, use_labels = \u0026#34;label\u0026#34;, text = FALSE) + guides(fill = FALSE, color = FALSE) + # Disable the legend theme_dag() The causal graph above outlines the complete relationship between mosquito net use and risk of malaria. Each node in the DAG is a column in the dataset collected by the researchers, and includes the following:\n Malaria risk (malaria_risk): The likelihood that someone in the household will be infected with malaria. Measured on a scale of 0–100, with higher values indicating higher risk. Mosquito net (net and net_num): A binary variable indicating if the household used mosquito nets. Eligible for program (eligible): A binary variable indicating if the household is eligible for the free net program. Income (income): The household\u0026rsquo;s monthly income, in US dollars. Nighttime temperatures (temperature): The average temperature at night, in Celsius. Health (health): Self-reported healthiness in the household. Measured on a scale of 0–100, with higher values indicating better health. Number in household (household): Number of people living in the household. Insecticide resistance (resistance): Some strains of mosquitoes are more resistant to insecticide and thus pose a higher risk of infecting people with malaria. This is measured on a scale of 0–100, with higher values indicating higher resistance.  According to the DAG, malaria risk is caused by income, temperatures, health, insecticide resistance, and mosquito net use. People who live in hotter regions, have lower incomes, have worse health, are surrounded by mosquitoes with high resistance to insecticide, and who do not use mosquito nets are at higher risk of contracting malaria than those who do not. Mosquito net use is caused by income, nighttime temperatures, health, the number of people living in the house, and eligibility for the free net program. People who live in areas that are cooler at night, have higher incomes, have better health, have more people in the home, and are eligible for free government nets are more likely to regularly use nets than those who do not. The DAG also shows that eligibility for the free net program is caused by income and household size, since households must meet specific thresholds to qualify.\nFirst, let\u0026rsquo;s download the dataset, put in a folder named data, and load it:\n  mosquito_nets.csv  # Load the data. # It\u0026#39;d be a good idea to click on the \u0026#34;mosquito_nets.csv\u0026#34; object in the # Environment panel in RStudio to see what the data looks like after you load it mosquito_nets \u0026lt;- read_csv(\u0026#34;data/mosquito_nets.csv\u0026#34;) We can use this data to check if the relationships defined by our DAG reflect reality. Recall that d-separation implies that nodes are statistically independent of each other and do not transfer associational information. If you draw the mosquito net DAG with dagitty.net, or if you run impliedConditionalIndependencies() in R, you can see a list of all the implied conditional independencies.\nimpliedConditionalIndependencies(mosquito_dag) ## elgb _||_ hlth | incm ## elgb _||_ mlr_ | hlth, incm, net, tmpr ## elgb _||_ rsst ## elgb _||_ tmpr ## hlth _||_ hshl ## hlth _||_ rsst ## hlth _||_ tmpr ## hshl _||_ incm ## hshl _||_ mlr_ | hlth, incm, net, tmpr ## hshl _||_ rsst ## hshl _||_ tmpr ## incm _||_ rsst ## incm _||_ tmpr ## net _||_ rsst ## rsst _||_ tmpr The _||_ symbol in the output here is the \\(\\perp\\) symbol, which means \u0026ldquo;independent of\u0026rdquo;. The | in the output means \u0026ldquo;given\u0026rdquo;.\nIn the interest of space, we will not verify all these implied independencies, but we can test a few of them:\n  \\(\\text{Health} \\perp \\text{Household members}\\): (Read as \u0026ldquo;Health is independent of household member count\u0026rdquo;.) Health should be independent of the number of people in each household. In the data, the two variables should not be correlated. This is indeed the case:\ncor(mosquito_nets$health, mosquito_nets$household) ## [1] 9.8e-05   \\(\\text{Income} \\perp \\text{Insecticide resistance}\\): (Read as \u0026ldquo;Income is independent of insecticide resistance\u0026rdquo;.) Income should be independent of insecticide resistance. This is again true:\ncor(mosquito_nets$income, mosquito_nets$resistance) ## [1] 0.014   \\(\\text{Malaria risk} \\perp \\text{Household members}\\ |\\ \\text{Health, Income, Bed net use, Temperature}\\): (Read as \u0026ldquo;Malaria risk is independent of house member count given health, income, bed net use, and temperature\u0026rdquo;.) Malaria risk should be independent of the number of household members given similar levels of health, income, mosquito net use, and nighttime temperatures. We cannot use cor() to test this implication, since there are many variables involved, but we can use a regression model to check if the number of household members is significantly related to malaria risk. It is not significant ($t = -0.17$, \\(p = 0.863\\)), which means the two are independent, as expected.\nlm(malaria_risk ~ household + health + income + net + temperature, data = mosquito_nets) %\u0026gt;% broom::tidy() ## # A tibble: 6 x 5 ## term estimate std.error statistic p.value ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 (Intercept) 76.2 0.966 78.9 0.  ## 2 household -0.0155 0.0893 -0.173 8.63e- 1 ## 3 health 0.148 0.0107 13.9 9.75e- 42 ## 4 income -0.0751 0.00104 -72.6 0.  ## 5 netTRUE -10.4 0.266 -39.2 2.63e-241 ## 6 temperature 1.01 0.0310 32.5 1.88e-181   We can check all the other conditional independencies to see if the DAG captures the reality of the full system of factors that influence mosquito net use and malaria risk. If there are substantial and significant correlations between nodes that should be independent, there is likely an issue with the specification of the DAG. Return to the theory of how the phenomena are generated and refine the DAG more.\nMosquito net adjustment sets There is a direct path between mosquito net use and the risk of malaria, but the effect is not causally identified due to several other open paths. We can either list out all the paths and find which open paths have arrows pointing backwards into the mosquito net node (run paths(mosquito_dag) to see these results), or we can let R find the appropriate adjustment sets automatically:\nadjustmentSets(mosquito_dag) ## { health, income, temperature } Based on the relationships between all the nodes in the DAG, adjusting for health, income, and temperature is enough to close all backdoors and identify the relationship between net use and malaria risk. Importantly, we do not need to worry about any of the nodes related to the government program for free nets, since those nodes are not d-connected to malaria risk. We only need to worry about confounding relationships.\nWe can confirm this graphically with ggdag_adjustment_set():\nggdag_adjustment_set(mosquito_dag, shadow = TRUE, use_labels = \u0026#34;label\u0026#34;, text = FALSE) + theme_dag() ","date":1630886400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1630886400,"objectID":"e2a631a0b19040e80b34f22067bbe0f3","permalink":"/example/05-practico/","publishdate":"2021-09-06T00:00:00Z","relpermalink":"/example/05-practico/","section":"example","summary":"DAGs with dagitty.net The easiest way to quickly build DAGs and find adjustment sets and testable implications is to use dagitty.net.\nThis video shows how to use it:\n\r\rDAGs with R, ggdag, and dagitty You can use the ggdag and dagitty packages in R to build and work with DAGs too.","tags":null,"title":"Transformación de datos en tidydata","type":"docs"},{"authors":null,"categories":null,"content":"Readings   Prologue and at least one of the four acts from This American Life, “Gardens of Branching Paths,” episode #691, January 10, 2020  Chapter 3 in Impact Evaluation in Practice1  Chapter 4, “Potential Outcomes Causal Model” in Causal Inference: The Mixtape2  Potential outcomes, ATEs, and CATEs example page  The example page on potential outcomes, ATEs, and CATEs shows how to use R to calculate ATEs and CATEs  Slides The slides for today’s lesson are available online as an HTML file. Use the buttons below to open the slides either as an interactive website or as a static PDF (for printing or storing for later). You can also click in the slides below and navigate through them with your left and right arrow keys.\n View all slides in new window  Download PDF of all slides\nIntroduction\r\rdo()ing observational causal inference\r\rPotential outcomes\r\r\r\r\r\r\r\r\r\r\r\r\rFun fact: If you type ? (or shift + /) while going through the slides, you can see a list of special slide-specific commands.\r\rVideos Videos for each section of the lecture are available at this YouTube playlist.\n Introduction do()ing observational causal inference Potential outcomes  You can also watch the playlist (and skip around to different sections) here:\n\r\r  Paul J. Gertler et al., Impact Evaluation in Practice, 2nd ed. (Inter-American Development Bank; World Bank, 2016), https://openknowledge.worldbank.org/handle/10986/25030. \u0026#x21a9;\u0026#xfe0e;\n Scott Cunningham, Causal Inference: The Mixtape (New Haven, CT: Yale University Press, 2021), https://mixtape.scunning.com/. \u0026#x21a9;\u0026#xfe0e;\n   ","date":1630886400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1630886400,"objectID":"b4d9dbb6ba244e4ebcb0ce95e47cebc7","permalink":"/content/05-content/","publishdate":"2021-09-06T00:00:00Z","relpermalink":"/content/05-content/","section":"content","summary":"Readings   Prologue and at least one of the four acts from This American Life, “Gardens of Branching Paths,” episode #691, January 10, 2020  Chapter 3 in Impact Evaluation in Practice1  Chapter 4, “Potential Outcomes Causal Model” in Causal Inference: The Mixtape2  Potential outcomes, ATEs, and CATEs example page  The example page on potential outcomes, ATEs, and CATEs shows how to use R to calculate ATEs and CATEs  Slides The slides for today’s lesson are available online as an HTML file.","tags":null,"title":"Transformación estructura datos","type":"docs"},{"authors":null,"categories":null,"content":"DAGs with dagitty.net The easiest way to quickly build DAGs and find adjustment sets and testable implications is to use dagitty.net.\nThis video shows how to use it:\n\r\rDAGs with R, ggdag, and dagitty You can use the ggdag and dagitty packages in R to build and work with DAGs too. I typically draw an initial DAG in my browser with dagitty.net and then I rewrite it in code in R so that it\u0026rsquo;s more official and formal and reproducible.\nLive coding example \r\rBasic DAGs (This is a heavily cleaned up and annotated version of the code from the video.)\n# Load packages library(tidyverse) # For dplyr, ggplot, and friends library(ggdag) # For plotting DAGs library(dagitty) # For working with DAG logic The general process for making and working with DAGs in R is to create a DAG object with dagify() and then plot it with ggdag(). The documentation for ggdag is really good and helpful and full of examples. Check these pages for all sorts of additional details:\n \u0026ldquo;An introduction to ggdag\u0026rdquo; \u0026ldquo;An introduction to directed acyclic graphs\u0026rdquo; List of all ggdag-related functions  The syntax for dagify() is similar to the formula syntax you\u0026rsquo;ve been using for building regression models with lm(). The formulas you use in dagify() indicate the relationships between nodes.\nFor instance, in this DAG, y is caused by x, a, and b (y ~ x + a + b), and x is caused by a and b (x ~ a + b). This means that a and b are confounders. Use the exposure and outcome arguments to specify which nodes are the exposure (or treatment/intervention/program) and the outcome.\n# Create super basic DAG simple_dag \u0026lt;- dagify( y ~ x + a + b, x ~ a + b, exposure = \u0026#34;x\u0026#34;, outcome = \u0026#34;y\u0026#34; ) # Adding a theme_dag() layer to the plot makes it have a white background with no axis labels ggdag(simple_dag) + theme_dag() If you want to plot which nodes are the exposure and outcome, use ggdag_status() instead:\nggdag_status(simple_dag) + theme_dag() Layouts and manual coordinates Notice how the layout is different in both of those graphs. By default, ggdag() positions the nodes randomly every time using a network algorithm. You can change the algorithm by using the layout argument: ggdag(simple_dag, layout = \u0026quot;nicely\u0026quot;). You can see a full list of possible algorithms by running ?layout_tbl_graph_igraph in the console.\nAlternatively, you can specify your own coordinates so that the nodes are positioned in the same place every time. Do this with the coords argument in dagify().\nThe best way to figure out what these numbers should be is to draw the DAG on paper or on a whiteboard and add a grid to it and then figure out the coordinates. For instance, in this DAG there are three rows and three columns: x and y go in the middle row (row 2) while a and b go in the middle column (column 2). It can also be helpful to not include theme_dag() at first so you can see the numbers for the underlying grid. Once you have everything positioned correctly, add theme_dag() to clean it up.\nsimple_dag_with_coords \u0026lt;- dagify( y ~ x + a + b, x ~ a + b, exposure = \u0026#34;x\u0026#34;, outcome = \u0026#34;y\u0026#34;, coords = list(x = c(x = 1, a = 2, b = 2, y = 3), y = c(x = 2, a = 1, b = 3, y = 2)) ) ggdag_status(simple_dag_with_coords) + theme_dag() Node names and labels The variable names you use do not have to be limited to just x, y, and other lowercase letters. You can any names you want, as long as there are no spaces.\ndag_with_var_names \u0026lt;- dagify( outcome ~ treatment + confounder1 + confounder2, treatment ~ confounder1 + confounder2, exposure = \u0026#34;treatment\u0026#34;, outcome = \u0026#34;outcome\u0026#34; ) ggdag_status(dag_with_var_names) + theme_dag() However, unless you use very short names, it is likely that the text will not fit inside the nodes. To get around this, you can add labels to the nodes using the labels argument in dagify(). Plot the labels by setting use_labels = \u0026quot;label\u0026quot; in ggdag(). You can turn off the text in the nodes with text = FALSE in ggdag().\nsimple_dag_with_coords_and_labels \u0026lt;- dagify( y ~ x + a + b, x ~ a + b, exposure = \u0026#34;x\u0026#34;, outcome = \u0026#34;y\u0026#34;, labels = c(y = \u0026#34;Outcome\u0026#34;, x = \u0026#34;Treatment\u0026#34;, a = \u0026#34;Confounder 1\u0026#34;, b = \u0026#34;Confounder 2\u0026#34;), coords = list(x = c(x = 1, a = 2, b = 2, y = 3), y = c(x = 2, a = 1, b = 3, y = 2)) ) ggdag_status(simple_dag_with_coords_and_labels, use_labels = \u0026#34;label\u0026#34;, text = FALSE) + guides(fill = FALSE, color = FALSE) + # Disable the legend theme_dag() Paths and adjustment sets R can also perform analysis on DAG objects. For example, we can find all the testable implications from the DAG using the impliedConditionalIndependencies() function from the dagitty package. For this simple DAG, there is only one: a should be independent of b. If we had a dataset with columns for each of these variables, we could check if this is true by running cor(a, b) to see if the two are related.\nimpliedConditionalIndependencies(simple_dag) ## a _||_ b We can also find all the paths between x and y using the paths() function from the dagitty package. We can see that there are three open paths between x and y:\npaths(simple_dag) ## $paths ## [1] \u0026#34;x -\u0026gt; y\u0026#34; \u0026#34;x \u0026lt;- a -\u0026gt; y\u0026#34; \u0026#34;x \u0026lt;- b -\u0026gt; y\u0026#34; ##  ## $open ## [1] TRUE TRUE TRUE The first open path is fine—we want a single d-connected relationship between treatment and outcome—but the other two indicate that there is confounding from a and b. We can see what each of these paths are with the ggdag_paths() function from the ggdag package:\nggdag_paths(simple_dag_with_coords) + theme_dag() Instead of listing out all the possible paths and identifying backdoors by hand, you can use the adjustmentSets() function in the dagitty package to programmatically find all the nodes that need to be adjusted. Here we see that both a and b need to be controlled for to isolate the x -\u0026gt; y relationship.\nadjustmentSets(simple_dag) ## { a, b } You can also visualize the adjustment sets with ggdag_adjustment_set() in the ggdag package. Make sure you set shadow = TRUE to draw the arrows coming out of the adjusted nodes—by default, those are not included.\nggdag_adjustment_set(simple_dag_with_coords, shadow = TRUE) + theme_dag() R will find minimally sufficient adjustment sets, which includes the fewest number of adjustments needed to close all backdoors between x and y. In this example DAG there was only one set of variables (a and b), but in other situations there could be many possible sets, or none if the causal effect is not identifiable.\nPlot DAG from dagitty.net with ggdag() If you use dagitty.net to draw a DAG, you\u0026rsquo;ll notice that it generates some code for you in the model code section:\nYou can copy that dag{ ... } code and paste it into R to define a DAG object rather than use the dagify() function. To do this, use the dagitty() function from the dagitty library and include the whole generated model code in single quotes (''):\nmodel_from_dagitty \u0026lt;- dagitty(\u0026#39;dag { bb=\u0026#34;0,0,1,1\u0026#34; \u0026#34;A confounder\u0026#34; [pos=\u0026#34;0.809,0.306\u0026#34;] \u0026#34;Another confounder\u0026#34; [pos=\u0026#34;0.810,0.529\u0026#34;] \u0026#34;Some outcome\u0026#34; [outcome,pos=\u0026#34;0.918,0.432\u0026#34;] \u0026#34;Some treatment\u0026#34; [exposure,pos=\u0026#34;0.681,0.426\u0026#34;] \u0026#34;A confounder\u0026#34; -\u0026gt; \u0026#34;Some outcome\u0026#34; \u0026#34;A confounder\u0026#34; -\u0026gt; \u0026#34;Some treatment\u0026#34; \u0026#34;Another confounder\u0026#34; -\u0026gt; \u0026#34;Some outcome\u0026#34; \u0026#34;Another confounder\u0026#34; -\u0026gt; \u0026#34;Some treatment\u0026#34; \u0026#34;Some treatment\u0026#34; -\u0026gt; \u0026#34;Some outcome\u0026#34; } \u0026#39;) ggdag(model_from_dagitty) + theme_dag() By default it\u0026rsquo;s going to look ugly because (1) the node labels don\u0026rsquo;t fit, and (2) slight differences in the coordinates make it so the nodes don\u0026rsquo;t perfectly align with each other. To fix coordinate alignment, you can modify the numbers in the generated DAG code. Here I rounded the numbers so that they\u0026rsquo;re all 0.3, 0.8, etc.\nTo fix the label issue, you can add the use_labels argument like normally. Only here, you can\u0026rsquo;t specify use_labels = \u0026quot;label\u0026quot;. Instead, when you specify a DAG using dagitty\u0026rsquo;s code like this, the column in the underlying dataset that contains the labels is named name, so you need to use use_labels = \u0026quot;name\u0026quot;.\nOther ggdag() variations like ggdag_status() will still work fine.\nmodel_from_dagitty_rounded \u0026lt;- dagitty(\u0026#39;dag { bb=\u0026#34;0,0,1,1\u0026#34; \u0026#34;A confounder\u0026#34; [pos=\u0026#34;0.8,0.3\u0026#34;] \u0026#34;Another confounder\u0026#34; [pos=\u0026#34;0.8,0.5\u0026#34;] \u0026#34;Some outcome\u0026#34; [outcome,pos=\u0026#34;0.9,0.4\u0026#34;] \u0026#34;Some treatment\u0026#34; [exposure,pos=\u0026#34;0.7,0.4\u0026#34;] \u0026#34;A confounder\u0026#34; -\u0026gt; \u0026#34;Some outcome\u0026#34; \u0026#34;A confounder\u0026#34; -\u0026gt; \u0026#34;Some treatment\u0026#34; \u0026#34;Another confounder\u0026#34; -\u0026gt; \u0026#34;Some outcome\u0026#34; \u0026#34;Another confounder\u0026#34; -\u0026gt; \u0026#34;Some treatment\u0026#34; \u0026#34;Some treatment\u0026#34; -\u0026gt; \u0026#34;Some outcome\u0026#34; } \u0026#39;) ggdag_status(model_from_dagitty_rounded, text = FALSE, use_labels = \u0026#34;name\u0026#34;) + guides(color = FALSE) + # Turn off legend theme_dag() Mosquito net example Conditional independencies You can test if your stated relationships are correct by looking at the conditional independencies that are implied by the DAG. In dagitty.net, these appear in the sidebar in the \u0026ldquo;Testable implications\u0026rdquo; section. To show how this works, we\u0026rsquo;ll use a simulated dataset that I generated about a fictional mosquito net program. Download the data here if you want to follow along:\n  mosquito_nets.csv  Researchers are interested in whether using mosquito nets decreases an individual\u0026rsquo;s risk of contracting malaria. They have collected data from 1,752 households in an unnamed country and have variables related to environmental factors, individual health, and household characteristics. Additionally, this country has a special government program that provides free mosquito nets to households that meet specific requirements: to qualify for the program, there must be more than 4 members of the household, and the household\u0026rsquo;s monthly income must be lower than $700 a month. Households are not automatically enrolled in the program, and many do not use it. The data is not experimental—researchers have no control over who uses mosquito nets, and individual households make their own choices over whether to apply for free nets or buy their own nets, as well as whether they use the nets if they have them.\nmosquito_dag \u0026lt;- dagify( malaria_risk ~ net + income + health + temperature + resistance, net ~ income + health + temperature + eligible + household, eligible ~ income + household, health ~ income, exposure = \u0026#34;net\u0026#34;, outcome = \u0026#34;malaria_risk\u0026#34;, coords = list(x = c(malaria_risk = 7, net = 3, income = 4, health = 5, temperature = 6, resistance = 8.5, eligible = 2, household = 1), y = c(malaria_risk = 2, net = 2, income = 3, health = 1, temperature = 3, resistance = 2, eligible = 3, household = 2)), labels = c(malaria_risk = \u0026#34;Risk of malaria\u0026#34;, net = \u0026#34;Mosquito net\u0026#34;, income = \u0026#34;Income\u0026#34;, health = \u0026#34;Health\u0026#34;, temperature = \u0026#34;Nighttime temperatures\u0026#34;, resistance = \u0026#34;Insecticide resistance\u0026#34;, eligible = \u0026#34;Eligible for program\u0026#34;, household = \u0026#34;Number in household\u0026#34;) ) ggdag_status(mosquito_dag, use_labels = \u0026#34;label\u0026#34;, text = FALSE) + guides(fill = FALSE, color = FALSE) + # Disable the legend theme_dag() The causal graph above outlines the complete relationship between mosquito net use and risk of malaria. Each node in the DAG is a column in the dataset collected by the researchers, and includes the following:\n Malaria risk (malaria_risk): The likelihood that someone in the household will be infected with malaria. Measured on a scale of 0–100, with higher values indicating higher risk. Mosquito net (net and net_num): A binary variable indicating if the household used mosquito nets. Eligible for program (eligible): A binary variable indicating if the household is eligible for the free net program. Income (income): The household\u0026rsquo;s monthly income, in US dollars. Nighttime temperatures (temperature): The average temperature at night, in Celsius. Health (health): Self-reported healthiness in the household. Measured on a scale of 0–100, with higher values indicating better health. Number in household (household): Number of people living in the household. Insecticide resistance (resistance): Some strains of mosquitoes are more resistant to insecticide and thus pose a higher risk of infecting people with malaria. This is measured on a scale of 0–100, with higher values indicating higher resistance.  According to the DAG, malaria risk is caused by income, temperatures, health, insecticide resistance, and mosquito net use. People who live in hotter regions, have lower incomes, have worse health, are surrounded by mosquitoes with high resistance to insecticide, and who do not use mosquito nets are at higher risk of contracting malaria than those who do not. Mosquito net use is caused by income, nighttime temperatures, health, the number of people living in the house, and eligibility for the free net program. People who live in areas that are cooler at night, have higher incomes, have better health, have more people in the home, and are eligible for free government nets are more likely to regularly use nets than those who do not. The DAG also shows that eligibility for the free net program is caused by income and household size, since households must meet specific thresholds to qualify.\nFirst, let\u0026rsquo;s download the dataset, put in a folder named data, and load it:\n  mosquito_nets.csv  # Load the data. # It\u0026#39;d be a good idea to click on the \u0026#34;mosquito_nets.csv\u0026#34; object in the # Environment panel in RStudio to see what the data looks like after you load it mosquito_nets \u0026lt;- read_csv(\u0026#34;data/mosquito_nets.csv\u0026#34;) We can use this data to check if the relationships defined by our DAG reflect reality. Recall that d-separation implies that nodes are statistically independent of each other and do not transfer associational information. If you draw the mosquito net DAG with dagitty.net, or if you run impliedConditionalIndependencies() in R, you can see a list of all the implied conditional independencies.\nimpliedConditionalIndependencies(mosquito_dag) ## elgb _||_ hlth | incm ## elgb _||_ mlr_ | hlth, incm, net, tmpr ## elgb _||_ rsst ## elgb _||_ tmpr ## hlth _||_ hshl ## hlth _||_ rsst ## hlth _||_ tmpr ## hshl _||_ incm ## hshl _||_ mlr_ | hlth, incm, net, tmpr ## hshl _||_ rsst ## hshl _||_ tmpr ## incm _||_ rsst ## incm _||_ tmpr ## net _||_ rsst ## rsst _||_ tmpr The _||_ symbol in the output here is the \\(\\perp\\) symbol, which means \u0026ldquo;independent of\u0026rdquo;. The | in the output means \u0026ldquo;given\u0026rdquo;.\nIn the interest of space, we will not verify all these implied independencies, but we can test a few of them:\n  \\(\\text{Health} \\perp \\text{Household members}\\): (Read as \u0026ldquo;Health is independent of household member count\u0026rdquo;.) Health should be independent of the number of people in each household. In the data, the two variables should not be correlated. This is indeed the case:\ncor(mosquito_nets$health, mosquito_nets$household) ## [1] 9.8e-05   \\(\\text{Income} \\perp \\text{Insecticide resistance}\\): (Read as \u0026ldquo;Income is independent of insecticide resistance\u0026rdquo;.) Income should be independent of insecticide resistance. This is again true:\ncor(mosquito_nets$income, mosquito_nets$resistance) ## [1] 0.014   \\(\\text{Malaria risk} \\perp \\text{Household members}\\ |\\ \\text{Health, Income, Bed net use, Temperature}\\): (Read as \u0026ldquo;Malaria risk is independent of house member count given health, income, bed net use, and temperature\u0026rdquo;.) Malaria risk should be independent of the number of household members given similar levels of health, income, mosquito net use, and nighttime temperatures. We cannot use cor() to test this implication, since there are many variables involved, but we can use a regression model to check if the number of household members is significantly related to malaria risk. It is not significant ($t = -0.17$, \\(p = 0.863\\)), which means the two are independent, as expected.\nlm(malaria_risk ~ household + health + income + net + temperature, data = mosquito_nets) %\u0026gt;% broom::tidy() ## # A tibble: 6 x 5 ## term estimate std.error statistic p.value ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 (Intercept) 76.2 0.966 78.9 0.  ## 2 household -0.0155 0.0893 -0.173 8.63e- 1 ## 3 health 0.148 0.0107 13.9 9.75e- 42 ## 4 income -0.0751 0.00104 -72.6 0.  ## 5 netTRUE -10.4 0.266 -39.2 2.63e-241 ## 6 temperature 1.01 0.0310 32.5 1.88e-181   We can check all the other conditional independencies to see if the DAG captures the reality of the full system of factors that influence mosquito net use and malaria risk. If there are substantial and significant correlations between nodes that should be independent, there is likely an issue with the specification of the DAG. Return to the theory of how the phenomena are generated and refine the DAG more.\nMosquito net adjustment sets There is a direct path between mosquito net use and the risk of malaria, but the effect is not causally identified due to several other open paths. We can either list out all the paths and find which open paths have arrows pointing backwards into the mosquito net node (run paths(mosquito_dag) to see these results), or we can let R find the appropriate adjustment sets automatically:\nadjustmentSets(mosquito_dag) ## { health, income, temperature } Based on the relationships between all the nodes in the DAG, adjusting for health, income, and temperature is enough to close all backdoors and identify the relationship between net use and malaria risk. Importantly, we do not need to worry about any of the nodes related to the government program for free nets, since those nodes are not d-connected to malaria risk. We only need to worry about confounding relationships.\nWe can confirm this graphically with ggdag_adjustment_set():\nggdag_adjustment_set(mosquito_dag, shadow = TRUE, use_labels = \u0026#34;label\u0026#34;, text = FALSE) + theme_dag() ","date":1630281600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1630281600,"objectID":"f3d97bcfb35ff647d4d9f85dd4966420","permalink":"/example/04-practico/","publishdate":"2021-08-30T00:00:00Z","relpermalink":"/example/04-practico/","section":"example","summary":"DAGs with dagitty.net The easiest way to quickly build DAGs and find adjustment sets and testable implications is to use dagitty.net.\nThis video shows how to use it:\n\r\rDAGs with R, ggdag, and dagitty You can use the ggdag and dagitty packages in R to build and work with DAGs too.","tags":null,"title":"Tranformar y seleccionar variables","type":"docs"},{"authors":null,"categories":null,"content":"Readings Measurement   The witch trial scene from Monty Python and the Holy Grail  Chapter 5 in Evaluation: A Systematic Approach.1 This is available on iCollege.  DAGs   Julia M. Rohrer, “Thinking Clearly About Correlations and Causation: Graphical Causal Models for Observational Data”2 This will be posted on iCollege.  Section 2 only (pp. 4–11) from Julian Schuessler and Peter Selb, “Graphical Causal Models for Survey Inference.”3 The PDF is available at SocArXiv.  Chapter 3, “Directed Acyclic Graphs” in Causal Inference: The Mixtape4  DAG example page  The example page on DAGs shows how to draw and analyze DAGs with both dagitty.net and R + ggdag  Slides The slides for today’s lesson are available online as an HTML file. Use the buttons below to open the slides either as an interactive website or as a static PDF (for printing or storing for later). You can also click in the slides below and navigate through them with your left and right arrow keys.\n View all slides in new window  Download PDF of all slides\nIntroduction\r\rAbstraction, stretching, and validity\r\rCausal models\r\rPaths, doors, and adjustment\r\r\r\r\r\r\r\r\r\r\r\r\r\r\r\rFun fact: If you type ? (or shift + /) while going through the slides, you can see a list of special slide-specific commands.\r\rVideos Videos for each section of the lecture are available at this YouTube playlist.\n Introduction Abstraction, stretching, and validity Causal models Paths, doors, and adjustment  You can also watch the playlist (and skip around to different sections) here:\n\r\r  Peter H. Rossi, Mark W. Lipsey, and Gary T. Henry, Evaluation: A Systematic Approach, 8th ed. (Los Angeles: Sage, 2019). \u0026#x21a9;\u0026#xfe0e;\n Julia M. Rohrer, “Thinking Clearly about Correlations and Causation: Graphical Causal Models for Observational Data,” Advances in Methods and Practices in Psychological Science 1, no. 1 (March 2018): 27–42, doi:10.1177/2515245917745629. \u0026#x21a9;\u0026#xfe0e;\n Julian Schuessler and Peter Selb, “Graphical Causal Models for Survey Inference” (Working Paper, SocArXiv, December 17, 2019), doi:10.31235/osf.io/hbg3m. \u0026#x21a9;\u0026#xfe0e;\n Scott Cunningham, Causal Inference: The Mixtape (New Haven, CT: Yale University Press, 2021), https://mixtape.scunning.com/. \u0026#x21a9;\u0026#xfe0e;\n   ","date":1630281600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1630281600,"objectID":"b0405fb0975b8cdddb0454677282c9f2","permalink":"/content/04-content/","publishdate":"2021-08-30T00:00:00Z","relpermalink":"/content/04-content/","section":"content","summary":"Readings Measurement   The witch trial scene from Monty Python and the Holy Grail  Chapter 5 in Evaluation: A Systematic Approach.1 This is available on iCollege.  DAGs   Julia M.","tags":null,"title":"Transformación y selección de variables","type":"docs"},{"authors":null,"categories":null,"content":"In the example guide for generating random numbers, we explored how to use a bunch of different statistical distributions to create variables that had reasonable values. However, each of the columns that we generated there were completely independent of each other. In the final example, we made some data with columns like age, education, and income, but none of those were related, though in real life they would be.\nGenerating random variables is fairly easy: choose some sort of distributional shape, set parameters like a mean and standard deviation, and let randomness take over. Forcing variables to be related is a little trickier and involves a little math. But don\u0026rsquo;t worry! That math is all just regression stuff!\nlibrary(tidyverse) library(broom) library(patchwork) library(scales) library(ggdag) Basic example Relationships and regression Let\u0026rsquo;s pretend we want to predict someone\u0026rsquo;s happiness on a 10-point scale based on the number of cookies they\u0026rsquo;ve eaten and whether or not their favorite color is blue.\n$$ \\text{Happiness} = \\beta_0 + \\beta_1 \\text{Cookies eaten} + \\beta_2 \\text{Favorite color is blue} + \\varepsilon $$\nWe can generate a fake dataset with columns for happiness (Beta distribution clustered around 7ish), cookies (Poisson distribution), and favorite color (binomial distribution for blue/not blue):\nset.seed(1234) n_people \u0026lt;- 1000 happiness_simple \u0026lt;- tibble( id = 1:n_people, happiness = rbeta(n_people, shape1 = 7, shape2 = 3), cookies = rpois(n_people, lambda = 1), color_blue = sample(c(\u0026#34;Blue\u0026#34;, \u0026#34;Not blue\u0026#34;), n_people, replace = TRUE) ) %\u0026gt;% # Adjust some of the columns mutate(happiness = round(happiness * 10, 1), cookies = cookies + 1, color_blue = fct_relevel(factor(color_blue), \u0026#34;Not blue\u0026#34;)) head(happiness_simple) ## # A tibble: 6 x 4 ## id happiness cookies color_blue ## \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;fct\u0026gt;  ## 1 1 8.7 2 Blue  ## 2 2 6.5 2 Not blue  ## 3 3 4.8 2 Blue  ## 4 4 9.6 3 Not blue  ## 5 5 6.2 1 Not blue  ## 6 6 6.1 2 Blue We have a neat dataset now, so let\u0026rsquo;s run a regression. Is eating more cookies or liking blue associated with greater happiness?\nmodel_happiness1 \u0026lt;- lm(happiness ~ cookies + color_blue, data = happiness_simple) tidy(model_happiness1) ## # A tibble: 3 x 5 ## term estimate std.error statistic p.value ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 (Intercept) 7.06 0.105 67.0 0  ## 2 cookies -0.00884 0.0419 -0.211 0.833 ## 3 color_blueBlue -0.0202 0.0861 -0.235 0.815 Not really. The coefficients for both cookies and color_blueBlue are basically 0 and not statistically significant. That makes sense since the three columns are completely independent of each other. If there were any significant effects, that\u0026rsquo;d be strange and solely because of random chance.\nFor the sake of your final project, you can just leave all the columns completely independent of each other if you want. None of your results will be significant and you won\u0026rsquo;t see any effects anywhere, but you can still build models, run all the pre-model diagnostics, and create graphs and tables based on this data.\nHOWEVER, it will be far more useful to you if you generate relationships. The whole goal of this class is to find causal effects in observational, non-experimental data. If you can generate synthetic non-experimental data and bake in a known causal effect, you can know if your different methods for recovering that effect are working.\nSo how do we bake in correlations and causal effects?\nExplanatory variables linked to outcome; no connection between explanatory variables To help with the intuition of how to link these columns, think about the model we\u0026rsquo;re building:\n$$ \\text{Happiness} = \\beta_0 + \\beta_1 \\text{Cookies eaten} + \\beta_2 \\text{Favorite color is blue} + \\varepsilon $$\nThis model provides estimates for all those betas. Throughout the semester, we\u0026rsquo;ve used the analogy of sliders and switches to describe regression coefficients. Here we have both:\n \\(\\beta_0\\): The average baseline happiness. \\(\\beta_1\\): The additional change in happiness that comes from eating one cookie. This is a slider: move cookies up by one and happiness changes by \\(\\beta_1\\). \\(\\beta_2\\): The change in happiness that comes from having your favorite color be blue. This is a switch: turn on \u0026ldquo;blue\u0026rdquo; for someone and their happiness changes by \\(\\beta_2\\).  We can invent our own coefficients and use some math to build them into the dataset. Let\u0026rsquo;s use these numbers as our targets:\n \\(\\beta_0\\): Average happiness is 7 \\(\\beta_1\\): Eating one more cookie boosts happiness by 0.25 points \\(\\beta_2\\): People who like blue have 0.75 higher happiness  When generating the data, we can\u0026rsquo;t just use rbeta() by itself to generate happiness, since happiness depends on both cookies and favorite color (that\u0026rsquo;s why we call it a dependent variable). To build in this effect, we can add a new column that uses math and modifies the underlying rbeta()-based happiness score:\nhappiness_with_effect \u0026lt;- happiness_simple %\u0026gt;% # Turn the categorical favorite color column into TRUE/FALSE so we can do math with it mutate(color_blue_binary = ifelse(color_blue == \u0026#34;Blue\u0026#34;, TRUE, FALSE)) %\u0026gt;% # Make a new happiness column that uses coefficients for cookies and favorite color mutate(happiness_modified = happiness + (0.25 * cookies) + (0.75 * color_blue_binary)) head(happiness_with_effect) ## # A tibble: 6 x 6 ## id happiness cookies color_blue color_blue_binary happiness_modified ## \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;lgl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 1 8.7 2 Blue TRUE 9.95 ## 2 2 6.5 2 Not blue FALSE 7  ## 3 3 4.8 2 Blue TRUE 6.05 ## 4 4 9.6 3 Not blue FALSE 10.4  ## 5 5 6.2 1 Not blue FALSE 6.45 ## 6 6 6.1 2 Blue TRUE 7.35 Now that we have a new happiness_modified column we can run a model using it as the outcome:\nmodel_happiness2 \u0026lt;- lm(happiness_modified ~ cookies + color_blue, data = happiness_with_effect) tidy(model_happiness2) ## # A tibble: 3 x 5 ## term estimate std.error statistic p.value ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 (Intercept) 7.06 0.105 67.0 0.  ## 2 cookies 0.241 0.0419 5.76 1.13e- 8 ## 3 color_blueBlue 0.730 0.0861 8.48 8.25e-17 Whoa! Look at those coefficients! They\u0026rsquo;re exactly what we tried to build in! The baseline happiness (intercept) is ≈7, eating one cookie is associated with a ≈0.25 increase in happiness, and liking blue is associated with a ≈0.75 increase in happiness.\nHowever, we originally said that happiness was a 0-10 point scale. After boosting it with extra happiness for cookies and liking blue, there are some people who score higher than 10:\n# Original scale ggplot(happiness_with_effect, aes(x = happiness)) + geom_histogram(binwidth = 1, color = \u0026#34;white\u0026#34;) + scale_x_continuous(breaks = 0:11) + coord_cartesian(xlim = c(0, 11)) # Scaled up ggplot(happiness_with_effect, aes(x = happiness_modified)) + geom_histogram(binwidth = 1, color = \u0026#34;white\u0026#34;) + scale_x_continuous(breaks = 0:11) + coord_cartesian(xlim = c(0, 11)) To fix that, we can use the rescale() function from the scales package to force the new happiness_modified variable to fit back in its original range:\nhappiness_with_effect \u0026lt;- happiness_with_effect %\u0026gt;% mutate(happiness_rescaled = rescale(happiness_modified, to = c(3, 10))) ggplot(happiness_with_effect, aes(x = happiness_rescaled)) + geom_histogram(binwidth = 1, color = \u0026#34;white\u0026#34;) + scale_x_continuous(breaks = 0:11) + coord_cartesian(xlim = c(0, 11)) Everything is back in the 3–10 range now. However, the rescaling also rescaled our built-in effects. Look what happens if we use the happiness_rescaled in the model:\nmodel_happiness3 \u0026lt;- lm(happiness_rescaled ~ cookies + color_blue, data = happiness_with_effect) tidy(model_happiness3) ## # A tibble: 3 x 5 ## term estimate std.error statistic p.value ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 (Intercept) 6.34 0.0910 69.6 0.  ## 2 cookies 0.208 0.0362 5.76 1.13e- 8 ## 3 color_blueBlue 0.631 0.0744 8.48 8.25e-17 Now the baseline happiness is 6.3, the cookies effect is 0.2, and the blue effect is 0.63. These effects shrunk because we shrunk the data back down to have a maximum of 10.\nThere are probably fancy mathy ways to rescale data and keep the coefficients the same size, but rather than figure that out (who even wants to do that?!), my strategy is just to play with numbers until the results look good. Instead of using a 0.25 cookie effect and 0.75 blue effect, I make those effects bigger so that the rescaled version is roughly what I really want. There\u0026rsquo;s no systematic way to do this—I ran this chunk below a bunch of times until the numbers worked.\nset.seed(1234) n_people \u0026lt;- 1000 happiness_real_effect \u0026lt;- tibble( id = 1:n_people, happiness_baseline = rbeta(n_people, shape1 = 7, shape2 = 3), cookies = rpois(n_people, lambda = 1), color_blue = sample(c(\u0026#34;Blue\u0026#34;, \u0026#34;Not blue\u0026#34;), n_people, replace = TRUE) ) %\u0026gt;% # Adjust some of the columns mutate(happiness_baseline = round(happiness_baseline * 10, 1), cookies = cookies + 1, color_blue = fct_relevel(factor(color_blue), \u0026#34;Not blue\u0026#34;)) %\u0026gt;% # Turn the categorical favorite color column into TRUE/FALSE so we can do math with it mutate(color_blue_binary = ifelse(color_blue == \u0026#34;Blue\u0026#34;, TRUE, FALSE)) %\u0026gt;% # Make a new happiness column that uses coefficients for cookies and favorite color mutate(happiness_effect = happiness_baseline + (0.31 * cookies) + # Cookie effect (0.91 * color_blue_binary)) %\u0026gt;% # Blue effect # Rescale to 3-10, since that\u0026#39;s what the original happiness column looked like mutate(happiness = rescale(happiness_effect, to = c(3, 10))) model_does_this_work_yet \u0026lt;- lm(happiness ~ cookies + color_blue, data = happiness_real_effect) tidy(model_does_this_work_yet) ## # A tibble: 3 x 5 ## term estimate std.error statistic p.value ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 (Intercept) 6.15 0.0886 69.4 0.  ## 2 cookies 0.253 0.0352 7.19 1.25e-12 ## 3 color_blueBlue 0.749 0.0724 10.3 7.44e-24 There\u0026rsquo;s nothing magical about the 0.31 and 0.91 numbers I used here; I just kept changing those to different things until the regression coefficients ended up at ≈0.25 and ≈0.75. Also, I gave up on trying to make the baseline happiness 7. It\u0026rsquo;s possible to do—you\u0026rsquo;d just need to also shift the underlying Beta distribution up (like shape1 = 9, shape2 = 2 or something). But then you\u0026rsquo;d also need to change the coefficients more. You\u0026rsquo;ll end up with 3 moving parts and it can get complicated, so I don\u0026rsquo;t worry too much about it, since what we care about the most here is the effect of cookies and favorite color, not baseline levels of happiness.\nPhew. We successfully connected cookies and favorite color to happiness and we have effects that are measurable with regression! One last thing that I would do is get rid of some of the intermediate columns like color_blue_binary or happiness_effect—we only used those for the behind-the-scenes math of creating the effect. Here\u0026rsquo;s our final synthetic dataset:\nhappiness \u0026lt;- happiness_real_effect %\u0026gt;% select(id, happiness, cookies, color_blue) head(happiness) ## # A tibble: 6 x 4 ## id happiness cookies color_blue ## \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;fct\u0026gt;  ## 1 1 8.81 2 Blue  ## 2 2 6.20 2 Not blue  ## 3 3 5.53 2 Blue  ## 4 4 9.07 3 Not blue  ## 5 5 5.68 1 Not blue  ## 6 6 6.63 2 Blue We can save that as a CSV file with write_csv():\nwrite_csv(happiness, \u0026#34;data/happiness_fake_data.csv\u0026#34;) Explanatory variables linked to outcome; connection between explanatory variables In that cookie example, we assumed that both cookie consumption and favorite color are associated with happiness. We also assumed that cookie consumption and favorite color are not related to each other. But what if they are? What if people who like blue eat more cookies?\nWe\u0026rsquo;ve already used regression-based math to connect explanatory variables to outcome variables. We can use that same intuition to connect explanatory variables to each other.\nThe easiest way to think about this is with DAGs. Here\u0026rsquo;s the DAG for the model we just ran:\nhappiness_dag1 \u0026lt;- dagify(hap ~ cook + blue, coords = list(x = c(hap = 3, cook = 1, blue = 2), y = c(hap = 1, cook = 1, blue = 2))) ggdag(happiness_dag1) + theme_dag() Both cookies and favorite color cause happiness, but there\u0026rsquo;s no link between them. Notice that dagify() uses the same model syntax that lm() does: hap ~ cook + blue. If we think of this just like a regression model, we can pretend that there are coefficients there too: hap ~ 0.25*cook + 0.75*blue. We don\u0026rsquo;t actually include any coefficients in the DAG or anything, but it helps with the intuition.\nBut what if people who like blue eat more cookies on average? For whatever reason, let\u0026rsquo;s pretend that liking blue causes you to eat 0.5 more cookies, on average. Here\u0026rsquo;s the new DAG:\nhappiness_dag2 \u0026lt;- dagify(hap ~ cook + blue, cook ~ blue, coords = list(x = c(hap = 3, cook = 1, blue = 2), y = c(hap = 1, cook = 1, blue = 2))) ggdag(happiness_dag2) + theme_dag() Now we have two different equations: hap ~ cook + blue and cook ~ blue. Conveniently, these both translate to models, and we know the coefficients we want!\n hap ~ 0.25*cook + 0.75*blue: This is what we built before—cookies boost happiness by 0.25 and liking blue boosts happiness by 0.75 cook ~ 0.3*blue: This is what we just proposed—liking blue boosts cookies by 0.5  We can follow the same process we did when building the cookie and blue effects into happiness to also build a blue effect into cookies!\nset.seed(1234) n_people \u0026lt;- 1000 happiness_cookies_blue \u0026lt;- tibble( id = 1:n_people, happiness_baseline = rbeta(n_people, shape1 = 7, shape2 = 3), cookies = rpois(n_people, lambda = 1), color_blue = sample(c(\u0026#34;Blue\u0026#34;, \u0026#34;Not blue\u0026#34;), n_people, replace = TRUE) ) %\u0026gt;% # Adjust some of the columns mutate(happiness_baseline = round(happiness_baseline * 10, 1), cookies = cookies + 1, color_blue = fct_relevel(factor(color_blue), \u0026#34;Not blue\u0026#34;)) %\u0026gt;% # Turn the categorical favorite color column into TRUE/FALSE so we can do math with it mutate(color_blue_binary = ifelse(color_blue == \u0026#34;Blue\u0026#34;, TRUE, FALSE)) %\u0026gt;% # Make blue have an effect on cookie consumption mutate(cookies = cookies + (0.5 * color_blue_binary)) %\u0026gt;% # Make a new happiness column that uses coefficients for cookies and favorite color mutate(happiness_effect = happiness_baseline + (0.31 * cookies) + # Cookie effect (0.91 * color_blue_binary)) %\u0026gt;% # Blue effect # Rescale to 3-10, since that\u0026#39;s what the original happiness column looked like mutate(happiness = rescale(happiness_effect, to = c(3, 10))) head(happiness_cookies_blue) ## # A tibble: 6 x 7 ## id happiness_baseline cookies color_blue color_blue_binary happiness_effect happiness ## \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;lgl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 1 8.7 2.5 Blue TRUE 10.4 8.84 ## 2 2 6.5 2 Not blue FALSE 7.12 6.14 ## 3 3 4.8 2.5 Blue TRUE 6.48 5.61 ## 4 4 9.6 3 Not blue FALSE 10.5 8.96 ## 5 5 6.2 1 Not blue FALSE 6.51 5.63 ## 6 6 6.1 2.5 Blue TRUE 7.78 6.69 Notice now that people who like blue eat partial cookies, as expected. We can verify that there\u0026rsquo;s a relationship between liking blue and cookies by running a model:\nlm(cookies ~ color_blue, data = happiness_cookies_blue) %\u0026gt;% tidy() ## # A tibble: 2 x 5 ## term estimate std.error statistic p.value ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 (Intercept) 2.07 0.0451 45.9 3.51e-248 ## 2 color_blueBlue 0.460 0.0651 7.07 2.96e- 12 Yep. Liking blue is associated with 0.46 more cookies on average (it\u0026rsquo;s not quite 0.5, but that\u0026rsquo;s because of randomness).\nNow let\u0026rsquo;s do some neat DAG magic. Let\u0026rsquo;s say we\u0026rsquo;re interested in the causal effect of cookies on happiness. We could run a naive model:\nmodel_happiness_naive \u0026lt;- lm(happiness ~ cookies, data = happiness_cookies_blue) tidy(model_happiness_naive) ## # A tibble: 2 x 5 ## term estimate std.error statistic p.value ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 (Intercept) 6.27 0.0894 70.1 0.  ## 2 cookies 0.325 0.0354 9.18 2.40e-19 Based on this, eating a cookie causes you to have 0.325 more happiness points. But that\u0026rsquo;s wrong! Liking the color blue is a confounder and opens a path between cookies and happiness. You can see the confounding both in the DAG (since blue points to both the cookie node and the happiness node) and in the math (liking blue boosts happiness + liking blue boosts cookie consumption, which boosts happiness).\nTo fix this confounding, we need to statistically adjust for liking blue and close the backdoor path. Ordinarily we\u0026rsquo;d do this with something like matching or inverse probability weighting, but here we can just include liking blue as a control variable (since it\u0026rsquo;s linearly related to both cookies and happiness):\nmodel_happiness_ate \u0026lt;- lm(happiness ~ cookies + color_blue, data = happiness_cookies_blue) tidy(model_happiness_ate) ## # A tibble: 3 x 5 ## term estimate std.error statistic p.value ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 (Intercept) 6.09 0.0870 70.0 0.  ## 2 cookies 0.249 0.0346 7.19 1.25e-12 ## 3 color_blueBlue 0.739 0.0729 10.1 4.70e-23 After adjusting for backdoor confounding, eating one additional cookie causes a 0.249 point increase in happiness. This is the effect we originally built into the data!\nIf you wanted, we could rescale the number of cookies just like we rescaled happiness before, since sometimes adding effects to columns changes their reasonable ranges.\nNow that we have a good working dataset, we can keep the columns we care about and save it as a CSV file for later use:\nhappiness \u0026lt;- happiness_cookies_blue %\u0026gt;% select(id, happiness, cookies, color_blue) head(happiness) ## # A tibble: 6 x 4 ## id happiness cookies color_blue ## \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;fct\u0026gt;  ## 1 1 8.84 2.5 Blue  ## 2 2 6.14 2 Not blue  ## 3 3 5.61 2.5 Blue  ## 4 4 8.96 3 Not blue  ## 5 5 5.63 1 Not blue  ## 6 6 6.69 2.5 Blue write_csv(happiness, \u0026#34;data/happiness_fake_data.csv\u0026#34;) Adding extra noise We\u0026rsquo;ve got columns that follow specific distributions, and we\u0026rsquo;ve got columns that are statistically related to each other. We can add one more wrinkle to make our fake data even more fun (and even more reflective of real life). We can add some noise.\nRight now, the effects we\u0026rsquo;re finding are too perfect. When we used mutate() to add a 0.25 boost in happiness for every cookie people ate, we added exactly 0.25 happiness points. If someone ate 2 cookies, they got 0.5 more happiness; if they ate 5, they got 1.25 more.\nWhat if the cookie effect isn\u0026rsquo;t exactly 0.25, but somewhere around 0.25? For some people it\u0026rsquo;s 0.1, for others it\u0026rsquo;s 0.3, for others it\u0026rsquo;s 0.22. We can use the same ideas we talked about in the random numbers example to generate a distribution of an effect. For instance, let\u0026rsquo;s say that the average cookie effect is 0.25, but it can vary somewhat with a standard deviation of 0.15:\ntemp_data \u0026lt;- tibble(x = rnorm(10000, mean = 0.25, sd = 0.15)) ggplot(temp_data, aes(x = x)) + geom_histogram(binwidth = 0.05, boundary = 0, color = \u0026#34;white\u0026#34;) Sometimes it can go as low as −0.25; sometimes it can go as high as 0.75; normally it\u0026rsquo;s around 0.25.\nNothing in the model explains why it\u0026rsquo;s higher or lower for some people—it\u0026rsquo;s just random noise. Remember that the model accounts for that! This random variation is what the \\(\\varepsilon\\) is for in this model equation:\n$$ \\text{Happiness} = \\beta_0 + \\beta_1 \\text{Cookies eaten} + \\beta_2 \\text{Favorite color is blue} + \\varepsilon $$\nWe can build that uncertainty into the fake column! Instead of using 0.31 * cookies when generating happiness (which is technically 0.25, but shifted up to account for rescaling happiness back down after), we\u0026rsquo;ll make a column for the cookie effect and then multiply that by the number of cookies.\nset.seed(1234) n_people \u0026lt;- 1000 happiness_cookies_noisier \u0026lt;- tibble( id = 1:n_people, happiness_baseline = rbeta(n_people, shape1 = 7, shape2 = 3), cookies = rpois(n_people, lambda = 1), cookie_effect = rnorm(n_people, mean = 0.31, sd = 0.2), color_blue = sample(c(\u0026#34;Blue\u0026#34;, \u0026#34;Not blue\u0026#34;), n_people, replace = TRUE) ) %\u0026gt;% # Adjust some of the columns mutate(happiness_baseline = round(happiness_baseline * 10, 1), cookies = cookies + 1, color_blue = fct_relevel(factor(color_blue), \u0026#34;Not blue\u0026#34;)) %\u0026gt;% # Turn the categorical favorite color column into TRUE/FALSE so we can do math with it mutate(color_blue_binary = ifelse(color_blue == \u0026#34;Blue\u0026#34;, TRUE, FALSE)) %\u0026gt;% # Make blue have an effect on cookie consumption mutate(cookies = cookies + (0.5 * color_blue_binary)) %\u0026gt;% # Make a new happiness column that uses coefficients for cookies and favorite # color. Importantly, instead of using 0.31 * cookies, we\u0026#39;ll use the random # cookie effect we generated earlier mutate(happiness_effect = happiness_baseline + (cookie_effect * cookies) + (0.91 * color_blue_binary)) %\u0026gt;% # Rescale to 3-10, since that\u0026#39;s what the original happiness column looked like mutate(happiness = rescale(happiness_effect, to = c(3, 10))) head(happiness_cookies_noisier) ## # A tibble: 6 x 8 ## id happiness_baseline cookies cookie_effect color_blue color_blue_binary happiness_effect happiness ## \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;lgl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 1 8.7 2.5 0.124 Blue TRUE 9.92 8.16 ## 2 2 6.5 2.5 0.370 Blue TRUE 8.34 7.02 ## 3 3 4.8 2.5 0.326 Blue TRUE 6.53 5.72 ## 4 4 9.6 3.5 0.559 Blue TRUE 12.5 9.98 ## 5 5 6.2 1.5 0.0631 Blue TRUE 7.20 6.21 ## 6 6 6.1 2 0.222 Not blue FALSE 6.54 5.73 Now let\u0026rsquo;s look at the cookie effect in this noisier data:\nmodel_noisier \u0026lt;- lm(happiness ~ cookies + color_blue, data = happiness_cookies_noisier) tidy(model_noisier) ## # A tibble: 3 x 5 ## term estimate std.error statistic p.value ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 (Intercept) 6.11 0.0779 78.4 0.  ## 2 cookies 0.213 0.0314 6.79 1.92e-11 ## 3 color_blueBlue 0.650 0.0671 9.68 3.01e-21 The effect is a little smaller now because of the extra noise, so we\u0026rsquo;d need to mess with the 0.31 and 0.91 coefficients more to get those numbers back up to 0.25 and 0.75.\nWhile this didn\u0026rsquo;t influence the findings too much here, it can have consequences for other variables. For instance, in the previous section we said that the color blue influences cookie consumption. If the blue effect on cookies isn\u0026rsquo;t precisely 0.5 but follows some sort of distribution (sometimes small, sometimes big, sometimes negative, sometimes zero), that will influence cookies differently. That random effect on cookie consumption will then work together with the random effect of cookies on happiness, resulting in multiple varied values.\nFor instance, imagine the average effect of liking blue on cookies is 0.5, and the average effect of cookies on happiness is 0.25. For one person, their blue-on-cookie effect might be 0.392, which changes the number of cookies they eat. Their cookie-on-happiness effect is 0.573, which changes their happiness. Both of those random effects work together to generate the final happiness.\nIf you want more realistic-looking synthetic data, it\u0026rsquo;s a good idea to add some random noise wherever you can.\nVisualizing variables and relationships Going through this process requires a ton of trial and error. You will change all sorts of numbers to make sure the relationships you\u0026rsquo;re building work. This is especially the case if you rescale things, since that rescales your effects. There are a lot of moving parts and this is a complicated process.\nYou\u0026rsquo;ll run your data generation chunks lots and lots and lots of times, tinkering with the numbers as you go. This example makes it look easy, since it\u0026rsquo;s the final product, but I ran all these chunks over and over again until I got the causal effect and relationships just right.\nIt\u0026rsquo;s best if you also create plots and models to see what the relationships look like\nVisualizing one variable We covered a bunch of distributions in the random number generation example, but it\u0026rsquo;s hard to think about what a standard deviation of 2 vs 10 looks like, or what happens when you mess with the shape parameters in a Beta distribution.\nIt\u0026rsquo;s best to visualize these variables. You could build the variable into your official dataset and then look at it, but I find it\u0026rsquo;s often faster to just look at what a general distribution looks like first. The easiest way to do this is generate a dataset with just one column in it and look at it, either with a histogram or a density plot.\nFor instance, what does a Beta distribution with shape1 = 3 and shape2 = 16 look like? The math says it should peak around 0.15ish ($\\frac{3}{3 + 16}$), and that looks like the case:\ntemp_data \u0026lt;- tibble(x = rbeta(10000, shape1 = 3, shape2 = 16)) plot1 \u0026lt;- ggplot(temp_data, aes(x = x)) + geom_histogram(binwidth = 0.05, boundary = 0, color = \u0026#34;white\u0026#34;) plot2 \u0026lt;- ggplot(temp_data, aes(x = x)) + geom_density() plot1 + plot2 What if we want a normal distribution centered around 100, with most values range from 50 to 150. That\u0026rsquo;s range of ±50, but that doesn\u0026rsquo;t mean the sd will be 50—it\u0026rsquo;ll be much smaller than that, like 25ish. Tinker with the numbers until it looks right.\ntemp_data \u0026lt;- tibble(x = rnorm(10000, mean = 100, sd = 25)) plot1 \u0026lt;- ggplot(temp_data, aes(x = x)) + geom_histogram(binwidth = 10, boundary = 0, color = \u0026#34;white\u0026#34;) plot2 \u0026lt;- ggplot(temp_data, aes(x = x)) + geom_density() plot1 + plot2 Visualizing two continuous variables If you have two continuous/numeric columns, it\u0026rsquo;s best to use a scatterplot. For instance, let\u0026rsquo;s make two columns based on the Beta and normal distributions above, and we\u0026rsquo;ll make it so that y goes up by 0.25 for every increase in x, along with some noise:\nset.seed(1234) temp_data \u0026lt;- tibble( x = rnorm(1000, mean = 100, sd = 25) ) %\u0026gt;% mutate(y = rbeta(1000, shape1 = 3, shape2 = 16) + # Baseline distribution (0.25 * x) + # Effect of x rnorm(1000, mean = 0, sd = 10)) # Add some noise ggplot(temp_data, aes(x = x, y = y)) + geom_point() + geom_smooth(method = \u0026#34;lm\u0026#34;) We can confirm the effect with a model:\nlm(y ~ x, data = temp_data) %\u0026gt;% tidy() ## # A tibble: 2 x 5 ## term estimate std.error statistic p.value ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 (Intercept) 1.98 1.28 1.54 1.24e- 1 ## 2 x 0.235 0.0125 18.8 1.49e-67 Visualizing a binary variable and a continuous variable If you have one binary column and one continuous/numeric column, it\u0026rsquo;s generally best to not use a scatterplot. Instead, either look at the distribution of the continuous variable across the binary variable with a faceted histogram or overlaid density plot, or look at the average of the continuous variable across the different values of the binary variable with a point range.\nLet\u0026rsquo;s make two columns: a continuous outcome (y) and a binary treatment (x). Being in the treatment group causes an increase of 20 points, on average.\nset.seed(1234) temp_data \u0026lt;- tibble( treatment = rbinom(1000, size = 1, prob = 0.5) # Make 1000 0/1 values with 50% chance of each ) %\u0026gt;% mutate(outcome = rbeta(1000, shape1 = 3, shape2 = 16) + # Baseline distribution (20 * treatment) + # Effect of treatment rnorm(1000, mean = 0, sd = 20)) %\u0026gt;% # Add some noise mutate(treatment = factor(treatment)) # Make treatment a factor/categorical variable We can check the numbers with a model:\nlm(outcome ~ treatment, data = temp_data) %\u0026gt;% tidy() ## # A tibble: 2 x 5 ## term estimate std.error statistic p.value ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 (Intercept) 0.244 0.935 0.261 7.94e- 1 ## 2 treatment1 20.9 1.30 16.1 4.70e-52 Here\u0026rsquo;s what that looks like as a histogram:\nggplot(temp_data, aes(x = outcome, fill = treatment)) + geom_histogram(binwidth = 5, color = \u0026#34;white\u0026#34;, boundary = 0) + guides(fill = FALSE) + # Turn off the fill legend since it\u0026#39;s redundant facet_wrap(vars(treatment), ncol = 1) And as overlapping densities:\nggplot(temp_data, aes(x = outcome, fill = treatment)) + geom_density(alpha = 0.5) And with a point range:\n# hahaha these error bars are tiny ggplot(temp_data, aes(x = treatment, y = outcome, color = treatment)) + stat_summary(geom = \u0026#34;pointrange\u0026#34;, fun.data = \u0026#34;mean_se\u0026#34;) + guides(color = FALSE) # Turn off the color legend since it\u0026#39;s redundant Specific examples tl;dr: The general process Those previous sections go into a lot of detail. In general, here\u0026rsquo;s the process you should follow when building relationships in synthetic data:\n Draw a DAG that maps out how all the columns you care about are related. Specify how those nodes are measured. Specify the relationships between the nodes based on the DAG equations. Generate random columns that stand alone. Generate related columns using regression math. Consider adding random noise. This is an entirely trial and error process until you get numbers that look good. Rely heavily on plots as you try different coefficients and parameters. Optionally rescale any columns that go out of reasonable bounds. If you rescale, you\u0026rsquo;ll need to tinker with the coefficients you used since the final effects will also get rescaled. Verify all relationships with plots and models. Try it out! Save the data.  Creating an effect in an observational DAG   Draw a DAG that maps out how all the columns you care about are related.\nHere\u0026rsquo;s a simple DAG that shows the causal effect of mosquito net usage on malaria risk. Income and health both influence and confound net use and malaria risk, and income also influences health.\nmosquito_dag \u0026lt;- dagify(mal ~ net + inc + hlth, net ~ inc + hlth, hlth ~ inc, coords = list(x = c(mal = 4, net = 1, inc = 2, hlth = 3), y = c(mal = 1, net = 1, inc = 2, hlth = 2))) ggdag(mosquito_dag) + theme_dag()   Specify how those nodes are measured.\nFor the sake of this example, we\u0026rsquo;ll measure these nodes like so. See the random number example for more details about the distributions.\n  Malaria risk: scale from 0–100, mostly around 40, but ranging from 10ish to 80ish. Best to use a Beta distribution.\n  Net use: binary 0/1, TRUE/FALSE variable, where 50% of people use nets. Best to use a binomial distribution. However, since we want to use other variables that increase the likelihood of using a net, we\u0026rsquo;ll do some cool tricky stuff, explained later.\n  Income: weekly income, measured in dollars, mostly around 500 ± 300. Best to use a normal distribution.\n  Health: scale from 0–100, mostly around 70, but ranging from 50ish to 100. Best to use a Beta distribution.\n    Specify the relationships between the nodes based on the DAG equations.\nThere are three models in this DAG:\n  hlth ~ inc: Income influences health. We\u0026rsquo;ll assume that every 10 dollars/week is associated with a 1 point increase in health (so a 1 dollar incrrease is associated with a 0.02 point increase in health)\n  net ~ inc + hlth: Income and health both increase the probability of net usage. This is where we do some cool tricky stuff.\nBoth income and health have an effect on the probability of bed net use, but bed net use is measured as a 0/1, TRUE/FALSE variable. If we run a regression with net as the outcome, we can\u0026rsquo;t really interpret the coefficients like \u0026ldquo;a 1 point increase in health is associated with a 0.42 point increase in bed net being TRUE.\u0026rdquo; That doesn\u0026rsquo;t even make sense.\nOrdinarily, when working with binary outcome variables, you use logistic regression models (see the crash course we had when talking about propensity scores here). In this kind of regression, the coefficients in the model represent changes in the log odds of using a net. As we discuss in the crash course section, log odds are typically impossible to interpet. If you exponentiate them, you get odds ratios, which let you say things like \u0026ldquo;a 1 point increase in health is associated with a 15% increase in the likelihood of using a net.\u0026rdquo; Technically we could include coefficients for a logistic regression model and simulate probabilities of using a net or not using log odds and odds ratios (and that\u0026rsquo;s what I do in the rain barrel data from Problem Set 3 (see code here)), but that\u0026rsquo;s really hard to wrap your head around since you\u0026rsquo;re dealing with strange uninterpretable coefficients. So we won\u0026rsquo;t do that here.\nInstead, we\u0026rsquo;ll do some fun trickery. We\u0026rsquo;ll create something called a \u0026ldquo;bed net score\u0026rdquo; that gets bigger as income and health increase. We\u0026rsquo;ll say that a 1 point increase in health score is associated with a 1.5 point increase in bed net score, and a 1 dollar increase in income is associated with a 0.5 point increase in bed net score. This results in a column that ranges all over the place, from 200 to 500 (in this case; that won\u0026rsquo;t always be true). This column definitely doesn\u0026rsquo;t look like a TRUE/FALSE binary column—it\u0026rsquo;s just a bunch of numbers. That\u0026rsquo;s okay!\nWe\u0026rsquo;ll then use the rescale() function from the scales package to take this bed net score and scale it down so that it goes from 0.05 to 0.95. This represents a person\u0026rsquo;s probability of using a bed net.\nFinally, we\u0026rsquo;ll use that probability in the rbinom() function to generate a 0 or 1 for each person. Some people will have a high probability because of their income and health, like 0.9, and will most likely use a net. Some people might have a 0.15 probability and will likely not use a net.\nWhen you generate binary variables like this, it\u0026rsquo;s hard to know the exact effect you\u0026rsquo;ll get, so it\u0026rsquo;s best to tinker with the numbers until you see relationships that you want.\n  mal ~ net + inc + hlth: Finally net use, income, and health all have an effect on the risk of malaria. Building this relationship is easy since it\u0026rsquo;s just a regular linear regression model (since malaria risk is not binary). We\u0026rsquo;ll say that a 1 dollar increase in income is associated with a decrease in risk, a 1 point increase in health is associated with a decrease in risk, and using a net is associated with a 15 point decrease in risk. That\u0026rsquo;s the casual effect we\u0026rsquo;re building in to the DAG.\n    Generate random columns that stand alone. Generate related columns using regression math. Consider adding random noise. This is an entirely trial and error process until you get numbers that look good. Rely heavily on plots as you try different coefficients and parameters. Optionally rescale any columns that go out of reasonable bounds. If you rescale, you\u0026rsquo;ll need to tinker with the coefficients you used since the final effects will also get rescaled.\nHere we go! Let\u0026rsquo;s make some data. I\u0026rsquo;ll comment the code below so you can see what\u0026rsquo;s happening at each step.\n# Make this randomness consistent set.seed(1234) # Simulate 1138 people (just for fun) n_people \u0026lt;- 1138 net_data \u0026lt;- tibble( # Make an ID column (not necessary, but nice to have) id = 1:n_people, # Generate income variable: normal, 500 ± 300 income = rnorm(n_people, mean = 500, sd = 75) ) %\u0026gt;% # Generate health variable: beta, centered around 70ish mutate(health_base = rbeta(n_people, shape1 = 7, shape2 = 4) * 100, # Health increases by 0.02 for every dollar in income health_income_effect = income * 0.02, # Make the final health score and add some noise health = health_base + health_income_effect + rnorm(n_people, mean = 0, sd = 3), # Rescale so it doesn\u0026#39;t go above 100 health = rescale(health, to = c(min(health), 100))) %\u0026gt;% # Generate net variable based on income, health, and random noise mutate(net_score = (0.5 * income) + (1.5 * health) + rnorm(n_people, mean = 0, sd = 15), # Scale net score down to 0.05 to 0.95 to create a probability of using a net net_probability = rescale(net_score, to = c(0.05, 0.95)), # Randomly generate a 0/1 variable using that probability net = rbinom(n_people, 1, net_probability)) %\u0026gt;% # Finally generate a malaria risk variable based on income, health, net use, # and random noise mutate(malaria_risk_base = rbeta(n_people, shape1 = 4, shape2 = 5) * 100, # Risk goes down by 10 when using a net. Because we rescale things, # though, we have to make the effect a lot bigger here so it scales # down to -10. Risk also decreases as health and income go up. I played # with these numbers until they created reasonable coefficients. malaria_effect = (-30 * net) + (-1.9 * health) + (-0.1 * income), # Make the final malaria risk score and add some noise malaria_risk = malaria_risk_base + malaria_effect + rnorm(n_people, 0, sd = 3), # Rescale so it doesn\u0026#39;t go below 0, malaria_risk = rescale(malaria_risk, to = c(5, 70))) # Look at all these columns! head(net_data) ## # A tibble: 6 x 11 ## id income health_base health_income_effect health net_score net_probability net malaria_risk_base malaria_effect malaria_risk ## \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 1 409. 61.2 8.19 63.1 301. 0.369 0 37.9 -161. 45.1 ## 2 2 521. 83.9 10.4 83.5 409. 0.684 1 55.0 -241. 23.4 ## 3 3 581. 64.6 11.6 73.0 426. 0.735 0 53.0 -197. 36.5 ## 4 4 324. 62.0 6.48 60.6 255. 0.235 0 68.4 -148. 58.7 ## 5 5 532. 69.2 10.6 73.4 373. 0.578 1 63.2 -223. 32.7 ## 6 6 538. 36.6 10.8 42.6 295. 0.351 0 38.6 -135. 52.5   Verify all relationships with plots and models.\nLet\u0026rsquo;s see if we have the relationships we want. Income looks like it\u0026rsquo;s associated with health:\nggplot(net_data, aes(x = income, y = health)) + geom_point() + geom_smooth(method = \u0026#34;lm\u0026#34;) lm(health ~ income, data = net_data) %\u0026gt;% tidy() ## # A tibble: 2 x 5 ## term estimate std.error statistic p.value ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 (Intercept) 59.1 2.54 23.3 2.24e-98 ## 2 income 0.0169 0.00504 3.36 8.15e- 4 It looks like richer and healthier people are more likely to use nets:\nnet_income \u0026lt;- ggplot(net_data, aes(x = income, fill = as.factor(net))) + geom_density(alpha = 0.7) + theme(legend.position = \u0026#34;bottom\u0026#34;) net_health \u0026lt;- ggplot(net_data, aes(x = health, fill = as.factor(net))) + geom_density(alpha = 0.7) + theme(legend.position = \u0026#34;bottom\u0026#34;) net_income + net_health Income increasing makes it 1% more likely to use a net; health increasing make it 2% more likely to use a net:\nglm(net ~ income + health, family = binomial(link = \u0026#34;logit\u0026#34;), data = net_data) %\u0026gt;% tidy(exponentiate = TRUE) ## # A tibble: 3 x 5 ## term estimate std.error statistic p.value ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 (Intercept) 0.0186 0.532 -7.49 6.64e-14 ## 2 income 1.01 0.000864 6.47 9.93e-11 ## 3 health 1.02 0.00491 3.89 9.92e- 5   Try it out!\nIs the effect in there? Let\u0026rsquo;s try finding it by controlling for our two backdoors: health and income. Ordinarily we should do something like matching or inverse probability weighting, but we\u0026rsquo;ll just do regular regression here (which is okay-ish, since all these variables are indeed linearly related with each other—we made them that way!)\nIf we just look at the effect of nets on malaria risk without any statistical adjustment, we see that net cause a decrease of 13 points in malaria risk. This is wrong though becuase there\u0026rsquo;s confounding.\n# Wrong correlation-is-not-causation effect model_net_naive \u0026lt;- lm(malaria_risk ~ net, data = net_data) tidy(model_net_naive) ## # A tibble: 2 x 5 ## term estimate std.error statistic p.value ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 (Intercept) 41.9 0.413 102. 0.  ## 2 net -13.6 0.572 -23.7 2.90e-101 If we control for the confounders, we get the 10 point ATE. It works!\n# Correctly adjusted ATE effect model_net_ate \u0026lt;- lm(malaria_risk ~ net + health + income, data = net_data) tidy(model_net_ate) ## # A tibble: 4 x 5 ## term estimate std.error statistic p.value ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 (Intercept) 97.3 1.28 76.1 0.  ## 2 net -10.5 0.317 -33.2 1.32e-169 ## 3 health -0.608 0.0123 -49.4 1.25e-284 ## 4 income -0.0320 0.00213 -15.0 1.20e- 46   Save the data.\nSince it works, let\u0026rsquo;s save it:\n# In the end, all we need is id, income, health, net, and malaria risk: net_data_final \u0026lt;- net_data %\u0026gt;% select(id, income, health, net, malaria_risk) head(net_data_final) ## # A tibble: 6 x 5 ## id income health net malaria_risk ## \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; ## 1 1 409. 63.1 0 45.1 ## 2 2 521. 83.5 1 23.4 ## 3 3 581. 73.0 0 36.5 ## 4 4 324. 60.6 0 58.7 ## 5 5 532. 73.4 1 32.7 ## 6 6 538. 42.6 0 52.5 # Save it as a CSV file write_csv(net_data_final, \u0026#34;data/bed_nets.csv\u0026#34;)   Brief pep talk intermission Generating data for a full complete observational DAG like the example above is complicated and hard. These other forms of causal inference are design-based (i.e. tied to specific contexts like before/after treatment/control or arbitrary cutoffs) instead of model-based, so they\u0026rsquo;re actually a lot easier to simulate! So don\u0026rsquo;t be scared away yet!\nCreating an effect for RCTs   Draw a DAG that maps out how all the columns you care about are related.\nRCTs are great because they make DAGs really easy! In a well-randomized RCT, you get to delete all arrows going into the treatment node in a DAG. We\u0026rsquo;ll stick with the same mosquito net situation we just used, but make it randomized:\nrct_dag \u0026lt;- dagify(mal ~ net + inc + hlth, hlth ~ inc, coords = list(x = c(mal = 4, net = 1, inc = 2, hlth = 3), y = c(mal = 1, net = 1, inc = 2, hlth = 2))) ggdag(rct_dag) + theme_dag()   Specify how those nodes are measured.\nWe\u0026rsquo;ll measure these nodes the same way as before:\n  Malaria risk: scale from 0–100, mostly around 40, but ranging from 10ish to 80ish. Best to use a Beta distribution.\n  Net use: binary 0/1, TRUE/FALSE variable, where 50% of people use nets. Best to use a binomial distribution.\n  Income: weekly income, measured in dollars, mostly around 500 ± 300. Best to use a normal distribution.\n  Health: scale from 0–100, mostly around 70, but ranging from 50ish to 100. Best to use a Beta distribution.\n    Specify the relationships between the nodes based on the DAG equations.\nThis is where RCTs are great. Because we removed all the arrows going into net, we don\u0026rsquo;t need to build any relationships that influence net use. Net use is randomized! We don\u0026rsquo;t need to make strange \u0026ldquo;bed net scores\u0026rdquo; and give people boosts according to income or health or anything. There are only two models in this DAG:\n  hlth ~ inc: Income influences health. We\u0026rsquo;ll assume that every 10 dollars/week is associated with a 1 point increase in health (so a 1 dollar incrrease is associated with a 0.02 point increase in health)\n  mal ~ net + inc + hlth: Net use, income, and health all have an effect on the risk of malaria. We\u0026rsquo;ll say that a 1 dollar increase in income is associated with a decrease in risk, a 1 point increase in health is associated with a decrease in risk, and using a net is associated with a 15 point decrease in risk. That\u0026rsquo;s the casual effect we\u0026rsquo;re building in to the DAG.\n    Generate random columns that stand alone. Generate related columns using regression math. Consider adding random noise. This is an entirely trial and error process until you get numbers that look good. Rely heavily on plots as you try different coefficients and parameters. Optionally rescale any columns that go out of reasonable bounds. If you rescale, you\u0026rsquo;ll need to tinker with the coefficients you used since the final effects will also get rescaled.\nLet\u0026rsquo;s make this data. It\u0026rsquo;ll be a lot easier than the full DAG we did before. Again, I\u0026rsquo;ll comment the code below so you can see what\u0026rsquo;s happening at each step.\n# Make this randomness consistent set.seed(1234) # Simulate 793 people (just for fun) n_people \u0026lt;- 793 rct_data \u0026lt;- tibble( # Make an ID column (not necessary, but nice to have) id = 1:n_people, # Generate income variable: normal, 500 ± 300 income = rnorm(n_people, mean = 500, sd = 75) ) %\u0026gt;% # Generate health variable: beta, centered around 70ish mutate(health_base = rbeta(n_people, shape1 = 7, shape2 = 4) * 100, # Health increases by 0.02 for every dollar in income health_income_effect = income * 0.02, # Make the final health score and add some noise health = health_base + health_income_effect + rnorm(n_people, mean = 0, sd = 3), # Rescale so it doesn\u0026#39;t go above 100 health = rescale(health, to = c(min(health), 100))) %\u0026gt;% # Randomly assign people to use a net (this is nice and easy!) mutate(net = rbinom(n_people, 1, 0.5)) %\u0026gt;% # Finally generate a malaria risk variable based on income, health, net use, # and random noise mutate(malaria_risk_base = rbeta(n_people, shape1 = 4, shape2 = 5) * 100, # Risk goes down by 10 when using a net. Because we rescale things, # though, we have to make the effect a lot bigger here so it scales # down to -10. Risk also decreases as health and income go up. I played # with these numbers until they created reasonable coefficients. malaria_effect = (-35 * net) + (-1.9 * health) + (-0.1 * income), # Make the final malaria risk score and add some noise malaria_risk = malaria_risk_base + malaria_effect + rnorm(n_people, 0, sd = 3), # Rescale so it doesn\u0026#39;t go below 0, malaria_risk = rescale(malaria_risk, to = c(5, 70))) # Look at all these columns! head(rct_data) ## # A tibble: 6 x 9 ## id income health_base health_income_effect health net malaria_risk_base malaria_effect malaria_risk ## \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 1 409. 57.2 8.19 61.3 1 37.4 -192. 35.1 ## 2 2 521. 63.3 10.4 69.4 0 33.0 -184. 37.6 ## 3 3 581. 61.8 11.6 71.9 1 36.4 -230. 24.4 ## 4 4 324. 42.2 6.48 45.5 1 52.7 -154. 52.8 ## 5 5 532. 72.1 10.6 78.5 1 41.9 -237. 23.6 ## 6 6 538. 82.0 10.8 89.1 0 46.6 -223. 29.9   Verify all relationships with plots and models.\nIncome still looks like it\u0026rsquo;s associated with health (which isn\u0026rsquo;t surprising, since it\u0026rsquo;s the same code we used for the full DAG earlier):\nggplot(net_data, aes(x = income, y = health)) + geom_point() + geom_smooth(method = \u0026#34;lm\u0026#34;) lm(health ~ income, data = net_data) %\u0026gt;% tidy() ## # A tibble: 2 x 5 ## term estimate std.error statistic p.value ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 (Intercept) 59.1 2.54 23.3 2.24e-98 ## 2 income 0.0169 0.00504 3.36 8.15e- 4   Try it out!\nIs the effect in there? With an RCT, all we really need to do is compare the outcome across treatment and control groups—because there\u0026rsquo;s no confounding, we don\u0026rsquo;t need to control for anything. Ordinarily we should check for balance across characteristics like health and income (and maybe generate other demographic columns) like we did in the RCT example, but we\u0026rsquo;ll skip all that here since we\u0026rsquo;re just checking to see if the effect is there.\nIt looks like using nets causes an average decrease of 10 risk points. Great!\n# Correct RCT-based ATE model_rct \u0026lt;- lm(malaria_risk ~ net, data = rct_data) tidy(model_rct) ## # A tibble: 2 x 5 ## term estimate std.error statistic p.value ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 (Intercept) 40.8 0.463 88.0 0.  ## 2 net -10.2 0.653 -15.7 2.22e-48 Just for fun, if we control for health and income, we\u0026rsquo;ll get basically the same effect, since they don\u0026rsquo;t actualy confound the relationship and don\u0026rsquo;t really explain anything useful.\n# Controlling for stuff even though we don\u0026#39;t need to model_rct_controls \u0026lt;- lm(malaria_risk ~ net + health + income, data = rct_data) tidy(model_rct_controls) ## # A tibble: 4 x 5 ## term estimate std.error statistic p.value ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 (Intercept) 97.7 1.45 67.3 0.  ## 2 net -10.8 0.344 -31.3 2.27e-140 ## 3 health -0.586 0.0140 -41.8 1.67e-202 ## 4 income -0.0310 0.00230 -13.5 1.75e- 37   Save the data.\nThe data works, so let\u0026rsquo;s get rid of the intermediate columns we don\u0026rsquo;t need and save it as a CSV file.\n# In the end, all we need is id, income, health, net, and malaria risk: rct_data_final \u0026lt;- rct_data %\u0026gt;% select(id, income, health, net, malaria_risk) head(rct_data_final) ## # A tibble: 6 x 5 ## id income health net malaria_risk ## \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; ## 1 1 409. 61.3 1 35.1 ## 2 2 521. 69.4 0 37.6 ## 3 3 581. 71.9 1 24.4 ## 4 4 324. 45.5 1 52.8 ## 5 5 532. 78.5 1 23.6 ## 6 6 538. 89.1 0 29.9 # Save it as a CSV file write_csv(rct_data_final, \u0026#34;data/bed_nets_rct.csv\u0026#34;)   Creating an effect for diff-in-diff   Draw a DAG that maps out how all the columns you care about are related.\nDifference-in-differences approaches to causal inference are not based on models but on context or research design. You need comparable treatment and control groups before and after some policy or program is implemented.\nWe\u0026rsquo;ll keep with our mosquito net example and pretend that two cities in some country are dealing with malaria infections. City B rolls out a free net program in 2017; City A does not. Here\u0026rsquo;s what the DAG looks like:\ndid_dag \u0026lt;- dagify(mal ~ net + year + city, net ~ year + city, coords = list(x = c(mal = 3, net = 1, year = 2, city = 2), y = c(mal = 2, net = 2, year = 3, city = 1))) ggdag(did_dag) + theme_dag()   Specify how those nodes are measured.\nHere\u0026rsquo;s how we\u0026rsquo;ll measure these nodes:\n  Malaria risk: scale from 0–100, mostly around 60, but ranging from 30ish to 90ish. Best to use a Beta distribution.\n  Net use: binary 0/1, TRUE/FALSE variable. This is technically binomial, but we don\u0026rsquo;t need to simulate it since it will only happen for people who are in the treatment city after the universal net rollout.\n  Year: year ranging from 2013 to 2020. Best to use a uniform distribution.\n  City: binary 0/1, City A/City B variable. Best to use a binomial distribution.\n    Specify the relationships between the nodes based on the DAG equations.\nThere are two models in the DAG:\n  net ~ year + city: Net use is determined by being in City B and being after 2017. We\u0026rsquo;ll assume perfect compliance here (but it\u0026rsquo;s fairly easy to simulate non-compliance and have some people in City A use nets after 2017, and some people in both cities use nets before 2017).\n  mal ~ net + year + city: Malaria risk is determined by net use, year, and city. It\u0026rsquo;s determined by lots of other things too (like we saw in the previous DAGs), but since we\u0026rsquo;re assuming that the two cities are comparable treatment and control groups, we don\u0026rsquo;t need to worry about things like health, income, age, etc.\nWe\u0026rsquo;ll pretend that in general, City B has historicallly had a problem with malaria and people there have had higher risk: being in City B increases malaria risk by 5 points, on average. Over time, both cities have worked on mosquito abatement, so average malaria risk has decreased by 2 points per year (in both cities, because we believe in parallel trends). Using a mosquito net causes a decrease of 10 points on average. That\u0026rsquo;s our causal effect.\n    Generate random columns that stand alone. Generate related columns using regression math. Consider adding random noise. This is an entirely trial and error process until you get numbers that look good. Rely heavily on plots as you try different coefficients and parameters. Optionally rescale any columns that go out of reasonable bounds. If you rescale, you\u0026rsquo;ll need to tinker with the coefficients you used since the final effects will also get rescaled.\nGeneration time! Heavily annotated code below:\n# Make this randomness consistent set.seed(1234) # Simulate 2567 people (just for fun) n_people \u0026lt;- 2567 did_data \u0026lt;- tibble( # Make an ID column (not necessary, but nice to have) id = 1:n_people, # Generate year variable: uniform, between 2013 and 2020. Round so it\u0026#39;s whole. year = round(runif(n_people, min = 2013, max = 2020), 0), # Generate city variable: binomial, 50% chance of being in a city. We\u0026#39;ll use # sample() instead of rbinom() city = sample(c(\u0026#34;City A\u0026#34;, \u0026#34;City B\u0026#34;), n_people, replace = TRUE) ) %\u0026gt;% # Generate net variable. We\u0026#39;re assuming perfect compliance, so this will only # be TRUE for people in City B after 2017 mutate(net = ifelse(city == \u0026#34;City B\u0026#34; \u0026amp; year \u0026gt; 2017, TRUE, FALSE)) %\u0026gt;% # Generate a malaria risk variable based on year, city, net use, and random noise mutate(malaria_risk_base = rbeta(n_people, shape1 = 6, shape2 = 3) * 100, # Risk goes up if you\u0026#39;re in City B because they have a worse problem. # We could just say \u0026#34;city_effect = 5\u0026#34; and give everyone in City A an # exact 5-point boost, but to add some noise, we\u0026#39;ll give people an # average boost using rnorm(). Some people might go up 7, some might go # up 1, some might go down 2 city_effect = ifelse(city == \u0026#34;City B\u0026#34;, rnorm(n_people, mean = 5, sd = 2), 0), # Risk goes down by 2 points on average every year. Creating this # effect with regression would work fine (-2 * year), except the years # are huge here (-2 * 2013 and -2 * 2020, etc.) So first we create a # smaller year column where 2013 is year 1, 2014 is year 2, and so on, # that way we can say -2 * 1 and -2 * 6, or whatever. # Also, rather than make risk go down by *exactly* 2 every year, we\u0026#39;ll # add some noise with rnorm(), so for some people it\u0026#39;ll go down by 1 or # 4 or up by 1, and so on year_smaller = year - 2012, year_effect = rnorm(n_people, mean = -2, sd = 0.1) * year_smaller, # Using a net causes a decrease of 10 points, on average. Again, rather # than use exactly 10, we\u0026#39;ll use a distribution around 10. People only # get a net boost if they\u0026#39;re in City B after 2017. net_effect = ifelse(city == \u0026#34;City B\u0026#34; \u0026amp; year \u0026gt; 2017, rnorm(n_people, mean = -10, sd = 1.5), 0), # Finally combine all these effects to create the malaria risk variable malaria_risk = malaria_risk_base + city_effect + year_effect + net_effect, # Rescale so it doesn\u0026#39;t go below 0 or above 100 malaria_risk = rescale(malaria_risk, to = c(0, 100))) %\u0026gt;% # Make an indicator variable showing if the row is after 2017 mutate(after = year \u0026gt; 2017) head(did_data) ## # A tibble: 6 x 11 ## id year city net malaria_risk_base city_effect year_smaller year_effect net_effect malaria_risk after ## \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;lgl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;lgl\u0026gt; ## 1 1 2014 City B FALSE 53.0 6.25 2 -3.97 0 54.2 FALSE ## 2 2 2017 City B FALSE 89.7 3.32 5 -9.17 0 84.1 FALSE ## 3 3 2017 City A FALSE 49.5 0 5 -10.1 0 37.6 FALSE ## 4 4 2017 City B FALSE 37.4 7.11 5 -9.44 0 33.1 FALSE ## 5 5 2019 City A FALSE 76.6 0 7 -14.7 0 61.1 TRUE  ## 6 6 2017 City B FALSE 65.7 5.38 5 -10.4 0 59.9 FALSE   Verify all relationships with plots and models.\nIs risk higher in City B? Yep.\nggplot(did_data, aes(x = city, y = malaria_risk, color = city)) + stat_summary(geom = \u0026#34;pointrange\u0026#34;, fun.data = \u0026#34;mean_se\u0026#34;) + guides(color = FALSE) Does risk decrease over time? And are the trends parallel? There was a weird random spike in City B in 2017 for whatever reason, but in general, the trends in the two cities are pretty parallel from 2013 to 2017.\nplot_data \u0026lt;- did_data %\u0026gt;% group_by(year, city) %\u0026gt;% summarize(mean_risk = mean(malaria_risk), se_risk = sd(malaria_risk) / sqrt(n()), upper = mean_risk + (1.96 * se_risk), lower = mean_risk + (-1.96 * se_risk)) ggplot(plot_data, aes(x = year, y = mean_risk, color = city)) + geom_vline(xintercept = 2017.5) + geom_ribbon(aes(ymin = lower, ymax = upper, fill = city), alpha = 0.3, color = FALSE) + geom_line() + theme(legend.position = \u0026#34;bottom\u0026#34;)   Try it out!\nLet\u0026rsquo;s see if it works! For diff-in-diff we need to use this model:\n$$ \\text{Malaria risk} = \\alpha + \\beta\\ \\text{City B} + \\gamma\\ \\text{After 2017} + \\delta\\ (\\text{City B} \\times \\text{After 2017}) + \\varepsilon $$\nmodel_did \u0026lt;- lm(malaria_risk ~ city + after + city * after, data = did_data) tidy(model_did) ## # A tibble: 4 x 5 ## term estimate std.error statistic p.value ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 (Intercept) 59.2 0.549 108. 0.  ## 2 cityCity B 5.17 0.778 6.64 3.71e-11 ## 3 afterTRUE -7.47 0.939 -7.96 2.61e-15 ## 4 cityCity B:afterTRUE -10.2 1.32 -7.79 9.67e-15 It works! Being in City B is associated with a 5-point higher risk on average; being after 2017 is associated with a 7.5-point lower risk on average, and being in City B after 2017 causes risk to drop by −10. The number isn\u0026rsquo;t exactly −10 here, since we rescaled the malaria_risk column a little, but still, it\u0026rsquo;s close. It\u0026rsquo;d probably be a good idea to build in some more noise and noncompliance, since the p-values are really really tiny here, but this is good enough for now.\nHere\u0026rsquo;s an obligatory diff-in-diff visualization:\nplot_data \u0026lt;- did_data %\u0026gt;% group_by(after, city) %\u0026gt;% summarize(mean_risk = mean(malaria_risk), se_risk = sd(malaria_risk) / sqrt(n()), upper = mean_risk + (1.96 * se_risk), lower = mean_risk + (-1.96 * se_risk)) # Extract parts of the model results for adding annotations model_results \u0026lt;- tidy(model_did) before_treatment \u0026lt;- filter(model_results, term == \u0026#34;(Intercept)\u0026#34;)$estimate + filter(model_results, term == \u0026#34;cityCity B\u0026#34;)$estimate diff_diff \u0026lt;- filter(model_results, term == \u0026#34;cityCity B:afterTRUE\u0026#34;)$estimate after_treatment \u0026lt;- before_treatment + diff_diff + filter(model_results, term == \u0026#34;afterTRUE\u0026#34;)$estimate ggplot(plot_data, aes(x = after, y = mean_risk, color = city, group = city)) + geom_pointrange(aes(ymin = lower, ymax = upper)) + geom_line() + annotate(geom = \u0026#34;segment\u0026#34;, x = FALSE, xend = TRUE, y = before_treatment, yend = after_treatment - diff_diff, linetype = \u0026#34;dashed\u0026#34;, color = \u0026#34;grey50\u0026#34;) + annotate(geom = \u0026#34;segment\u0026#34;, x = 2.1, xend = 2.1, y = after_treatment, yend = after_treatment - diff_diff, linetype = \u0026#34;dotted\u0026#34;, color = \u0026#34;blue\u0026#34;) + theme(legend.position = \u0026#34;bottom\u0026#34;)   Save the data.\nThe data works, so let\u0026rsquo;s get rid of the intermediate columns we don\u0026rsquo;t need and save it as a CSV file.\ndid_data_final \u0026lt;- did_data %\u0026gt;% select(id, year, city, net, malaria_risk) head(did_data_final) ## # A tibble: 6 x 5 ## id year city net malaria_risk ## \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;lgl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 1 2014 City B FALSE 54.2 ## 2 2 2017 City B FALSE 84.1 ## 3 3 2017 City A FALSE 37.6 ## 4 4 2017 City B FALSE 33.1 ## 5 5 2019 City A FALSE 61.1 ## 6 6 2017 City B FALSE 59.9 # Save data write_csv(did_data_final, \u0026#34;data/diff_diff.csv\u0026#34;)   Creating an effect for regression discontinuity   Draw a DAG that maps out how all the columns you care about are related.\nRegression discontinuity designs are also based on context instead of models, so the DAG is pretty simple. We\u0026rsquo;ll keep with our mosquito net example and pretend that families that earn less than $450 a week qualify for a free net. Here\u0026rsquo;s the DAG:\nrdd_dag \u0026lt;- dagify(mal ~ net + inc, net ~ cut, cut ~ inc, coords = list(x = c(mal = 4, net = 1, inc = 3, cut = 2), y = c(mal = 1, net = 1, inc = 2, cut = 1.75))) ggdag(rdd_dag) + theme_dag()   Specify how those nodes are measured.\nHere\u0026rsquo;s how we\u0026rsquo;ll measure these nodes:\n  Malaria risk: scale from 0–100, mostly around 60, but ranging from 30ish to 90ish. Best to use a Beta distribution.\n  Net use: binary 0/1, TRUE/FALSE variable. This is technically binomial, but we don\u0026rsquo;t need to simulate it since it will only happen for people who below the cutoff.\n  Income: weekly income, measured in dollars, mostly around 500 ± 300. Best to use a normal distribution.\n  Cutoff: binary 0/1, below/above $450 variable. This is technically binomial, but we don\u0026rsquo;t need to simulate it since it is entirely based on income.\n    Specify the relationships between the nodes based on the DAG equations.\nThere are three models in the DAG:\n  cut ~ inc: Being above or below the cutpoint is determined by income. We know the cutoff is 450, so we just make an indicator showing if people are below that.\n  net ~ cut: Net usage is determined by the cutpoint. If people are below the cutpoint, they\u0026rsquo;ll use a net; if not, they won\u0026rsquo;t. We can build in noncompliance here if we want and use fuzzy regression discontinuity. For the sake of this example, we\u0026rsquo;ll do it both ways, just so you can see both sharp and fuzzy synthetic data.\n  mal ~ net + inc: Malaria risk is determined by both net usage and income. It\u0026rsquo;s also determined by lots of other things (age, education, city, etc.), but we don\u0026rsquo;t need to include those in the DAG because we\u0026rsquo;re using RDD to say that we have good treatment and control groups right around the cutoff.\nWe\u0026rsquo;ll pretend that a 1 dollar increase in income is associated with a drop in risk of 0.01, and that using a mosquito net causes a decrease of 10 points on average. That\u0026rsquo;s our causal effect.\n    Generate random columns that stand alone. Generate related columns using regression math. Consider adding random noise. This is an entirely trial and error process until you get numbers that look good. Rely heavily on plots as you try different coefficients and parameters. Optionally rescale any columns that go out of reasonable bounds. If you rescale, you\u0026rsquo;ll need to tinker with the coefficients you used since the final effects will also get rescaled.\nLet\u0026rsquo;s fake some data! Heavily annotated code below:\n# Make this randomness consistent set.seed(1234) # Simulate 5441 people (we need a lot bc we\u0026#39;re throwing most away) n_people \u0026lt;- 5441 rdd_data \u0026lt;- tibble( # Make an ID column (not necessary, but nice to have) id = 1:n_people, # Generate income variable: normal, 500 ± 300 income = rnorm(n_people, mean = 500, sd = 75) ) %\u0026gt;% # Generate cutoff variable mutate(below_cutoff = ifelse(income \u0026lt; 450, TRUE, FALSE)) %\u0026gt;% # Generate net variable. We\u0026#39;ll make two: one that\u0026#39;s sharp and has perfect # compliance, and one that\u0026#39;s fuzzy # Here\u0026#39;s the sharp one. It\u0026#39;s easy. If you\u0026#39;re below the cutoff you use a net. mutate(net_sharp = ifelse(below_cutoff == TRUE, TRUE, FALSE)) %\u0026gt;% # Here\u0026#39;s the fuzzy one, which is a little trickier. If you\u0026#39;re far away from # the cutoff, you follow what you\u0026#39;re supposed to do (like if your income is # 800, you don\u0026#39;t use the program; if your income is 200, you definitely use # the program). But if you\u0026#39;re close to the cutoff, we\u0026#39;ll pretend that there\u0026#39;s # an 80% chance that you\u0026#39;ll do what you\u0026#39;re supposed to do. mutate(net_fuzzy = case_when( # If your income is between 450 and 500, there\u0026#39;s a 20% chance of using the program income \u0026gt;= 450 \u0026amp; income \u0026lt;= 500 ~ sample(c(TRUE, FALSE), n_people, replace = TRUE, prob = c(0.2, 0.8)), # If your income is above 500, you definitely don\u0026#39;t use the program income \u0026gt; 500 ~ FALSE, # If your income is between 400 and 450, there\u0026#39;s an 80% chance of using the program income \u0026lt; 450 \u0026amp; income \u0026gt;= 400 ~ sample(c(TRUE, FALSE), n_people, replace = TRUE, prob = c(0.8, 0.2)), # If your income is below 400, you definitely use the program income \u0026lt; 400 ~ TRUE )) %\u0026gt;% # Finally we can make the malaria risk score, based on income, net use, and # random noise. We\u0026#39;ll make two outcomes: one using the sharp net use and one # using the fuzzy net use. They have the same effect built in, we just have to # use net_sharp and net_fuzzy separately. mutate(malaria_risk_base = rbeta(n_people, shape1 = 4, shape2 = 5) * 100) %\u0026gt;% # Make the sharp version. There\u0026#39;s really a 10 point decrease, but because of # rescaling, we use 15. I only chose 15 through lots of trial and error (i.e. # I used -11, ran the RDD model, and the effect was too small; I used -20, ran # the model, and the effect was too big; I kept changing numbers until landing # on -15). Risk also goes down as income increases. mutate(malaria_effect_sharp = (-15 * net_sharp) + (-0.01 * income), malaria_risk_sharp = malaria_risk_base + malaria_effect_sharp + rnorm(n_people, 0, sd = 3), malaria_risk_sharp = rescale(malaria_risk_sharp, to = c(5, 70))) %\u0026gt;% # Do the same thing, but with net_fuzzy mutate(malaria_effect_fuzzy = (-15 * net_fuzzy) + (-0.01 * income), malaria_risk_fuzzy = malaria_risk_base + malaria_effect_fuzzy + rnorm(n_people, 0, sd = 3), malaria_risk_fuzzy = rescale(malaria_risk_fuzzy, to = c(5, 70))) %\u0026gt;% # Make a version of income that\u0026#39;s centered at the cutpoint mutate(income_centered = income - 450) head(rdd_data) ## # A tibble: 6 x 11 ## id income below_cutoff net_sharp net_fuzzy malaria_risk_base malaria_effect_sharp malaria_risk_sharp malaria_effect_fuzzy malaria_risk_fuzzy income_centered ## \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;lgl\u0026gt; \u0026lt;lgl\u0026gt; \u0026lt;lgl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 1 409. TRUE TRUE FALSE 56.5 -19.1 38.0 -4.09 47.5 -40.5 ## 2 2 521. FALSE FALSE FALSE 26.1 -5.21 28.6 -5.21 27.6 70.8 ## 3 3 581. FALSE FALSE FALSE 84.4 -5.81 64.7 -5.81 65.5 131.  ## 4 4 324. TRUE TRUE TRUE 32.9 -18.2 25.9 -18.2 23.4 -126.  ## 5 5 532. FALSE FALSE FALSE 53.1 -5.32 46.5 -5.32 44.3 82.2 ## 6 6 538. FALSE FALSE FALSE 45.7 -5.38 43.2 -5.38 40.2 88.0   Verify all relationships with plots and models.\nIs there a cutoff in the running variable when we use the sharp net variable? Yep!\nggplot(rdd_data, aes(x = income, y = net_sharp, color = below_cutoff)) + geom_vline(xintercept = 450) + geom_point(alpha = 0.3, position = position_jitter(width = NULL, height = 0.2)) + guides(color = FALSE) Is there a cutoff in the running variable when we use the fuzzy net variable? Yep! There are some richer people using the program and some poorer people not using it.\nggplot(rdd_data, aes(x = income, y = net_fuzzy, color = below_cutoff)) + geom_vline(xintercept = 450) + geom_point(alpha = 0.3, position = position_jitter(width = NULL, height = 0.2)) + guides(color = FALSE)   Try it out!\nLet\u0026rsquo;s test it! For sharp RDD we need to use this model:\n$$ \\text{Malaria risk} = \\beta_0 + \\beta_1 \\text{Income}_\\text{centered} + \\beta_2 \\text{Net} + \\varepsilon $$\nWe\u0026rsquo;ll use a bandwidth of ±$50, because why not. In real life you\u0026rsquo;d be more careful about bandwidth selection (or use rdbwselect() from the rdrobust package to find the optimal bandwidth)\nggplot(rdd_data, aes(x = income, y = malaria_risk_sharp, color = net_sharp)) + geom_vline(xintercept = 450) + geom_point(alpha = 0.2, size = 0.5) + # Add lines for the full range geom_smooth(data = filter(rdd_data, income_centered \u0026lt;= 0), method = \u0026#34;lm\u0026#34;, se = FALSE, size = 1, linetype = \u0026#34;dashed\u0026#34;) + geom_smooth(data = filter(rdd_data, income_centered \u0026gt; 0), method = \u0026#34;lm\u0026#34;, se = FALSE, size = 1, linetype = \u0026#34;dashed\u0026#34;) + # Add lines for bandwidth = 50 geom_smooth(data = filter(rdd_data, income_centered \u0026gt;= -50 \u0026amp; income_centered \u0026lt;= 0), method = \u0026#34;lm\u0026#34;, se = FALSE, size = 2) + geom_smooth(data = filter(rdd_data, income_centered \u0026gt; 0 \u0026amp; income_centered \u0026lt;= 50), method = \u0026#34;lm\u0026#34;, se = FALSE, size = 2) + theme(legend.position = \u0026#34;bottom\u0026#34;) model_sharp \u0026lt;- lm(malaria_risk_sharp ~ income_centered + net_sharp, data = filter(rdd_data, income_centered \u0026gt;= -50 \u0026amp; income_centered \u0026lt;= 50)) tidy(model_sharp) ## # A tibble: 3 x 5 ## term estimate std.error statistic p.value ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 (Intercept) 40.7 0.462 88.1 0.  ## 2 income_centered -0.0197 0.0144 -1.37 1.72e- 1 ## 3 net_sharpTRUE -10.6 0.815 -13.0 1.67e-37 There\u0026rsquo;s an effect! For people in the bandwidth, the local average treatment effect of nets is a 10.6 point reduction in malaria risk.\nLet\u0026rsquo;s check if it works with the fuzzy version where there are noncompliers:\nggplot(rdd_data, aes(x = income, y = malaria_risk_fuzzy, color = net_fuzzy)) + geom_vline(xintercept = 450) + geom_point(alpha = 0.2, size = 0.5) + # Add lines for the full range geom_smooth(data = filter(rdd_data, income_centered \u0026lt;= 0), method = \u0026#34;lm\u0026#34;, se = FALSE, size = 1, linetype = \u0026#34;dashed\u0026#34;) + geom_smooth(data = filter(rdd_data, income_centered \u0026gt; 0), method = \u0026#34;lm\u0026#34;, se = FALSE, size = 1, linetype = \u0026#34;dashed\u0026#34;) + # Add lines for bandwidth = 50 geom_smooth(data = filter(rdd_data, income_centered \u0026gt;= -50 \u0026amp; income_centered \u0026lt;= 0), method = \u0026#34;lm\u0026#34;, se = FALSE, size = 2) + geom_smooth(data = filter(rdd_data, income_centered \u0026gt; 0 \u0026amp; income_centered \u0026lt;= 50), method = \u0026#34;lm\u0026#34;, se = FALSE, size = 2) + theme(legend.position = \u0026#34;bottom\u0026#34;) There\u0026rsquo;s a gap, but it\u0026rsquo;s hard to measure since there are noncompliers on both sides. We can deal with the noncompliance if we use above/below the cutoff as an instrument (see the fuzzy regression discontinuity guide for a complete example). We should run this set of models:\n$$ \\begin{aligned} \\widehat{\\text{Net}} \u0026amp;= \\gamma_0 + \\gamma_1 \\text{Income}_{\\text{centered}} + \\gamma_2 \\text{Below 450} + \\omega \\\\\n\\text{Malaria risk} \u0026amp;= \\beta_0 + \\beta_1 \\text{Income}_{\\text{centered}} + \\beta_2 \\widehat{\\text{Net}} + \\epsilon \\end{aligned} $$\nInstead of doing these two stages by hand (ugh), we\u0026rsquo;ll do the 2SLS regression with the iv_robust() function from the estimatr package:\nlibrary(estimatr) ## Warning: package \u0026#39;estimatr\u0026#39; was built under R version 4.0.5 model_fuzzy \u0026lt;- iv_robust(malaria_risk_fuzzy ~ income_centered + net_fuzzy | income_centered + below_cutoff, data = filter(rdd_data, income_centered \u0026gt;= -50 \u0026amp; income_centered \u0026lt;= 50)) tidy(model_fuzzy) ## term estimate std.error statistic p.value conf.low conf.high df outcome ## 1 (Intercept) 40.73241 0.69045 58.994 0.000e+00 39.37843 42.086402 2220 malaria_risk_fuzzy ## 2 income_centered -0.01921 0.01423 -1.350 1.772e-01 -0.04711 0.008696 2220 malaria_risk_fuzzy ## 3 net_fuzzyTRUE -11.21929 1.31033 -8.562 2.034e-17 -13.78889 -8.649700 2220 malaria_risk_fuzzy The effect is slightly larger now (−11.2), but that\u0026rsquo;s because we are looking at a doubly local ATE: compliers in the bandwidth. But still, it\u0026rsquo;s close to −10, so that\u0026rsquo;s good. And we could probably get it closer if we did other mathy shenanigans like adding squared and cubed terms or using nonparametric estimation with rdrobust() in the rdrobust package.\n  Save the data.\nThe data works, so let\u0026rsquo;s get rid of the intermediate columns we don\u0026rsquo;t need and save it as a CSV file. We\u0026rsquo;ll make two separate CSV files for fuzzy and sharp, just because.\nrdd_data_final_sharp \u0026lt;- rdd_data %\u0026gt;% select(id, income, net = net_sharp, malaria_risk = malaria_risk_sharp) head(rdd_data_final_sharp) ## # A tibble: 6 x 4 ## id income net malaria_risk ## \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;lgl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 1 409. TRUE 38.0 ## 2 2 521. FALSE 28.6 ## 3 3 581. FALSE 64.7 ## 4 4 324. TRUE 25.9 ## 5 5 532. FALSE 46.5 ## 6 6 538. FALSE 43.2 rdd_data_final_fuzzy \u0026lt;- rdd_data %\u0026gt;% select(id, income, net = net_fuzzy, malaria_risk = malaria_risk_fuzzy) head(rdd_data_final_fuzzy) ## # A tibble: 6 x 4 ## id income net malaria_risk ## \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;lgl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 1 409. FALSE 47.5 ## 2 2 521. FALSE 27.6 ## 3 3 581. FALSE 65.5 ## 4 4 324. TRUE 23.4 ## 5 5 532. FALSE 44.3 ## 6 6 538. FALSE 40.2 # Save data write_csv(rdd_data_final_sharp, \u0026#34;data/rdd_sharp.csv\u0026#34;) write_csv(rdd_data_final_fuzzy, \u0026#34;data/rdd_fuzzy.csv\u0026#34;)   Creating an effect for instrumental variables   Draw a DAG that maps out how all the columns you care about are related.\nAs with diff-in-diff and regression discontinuity, instrumental variables are a design-based approach to causal inference and thus don\u0026rsquo;t require complicated models (but you can still add control variables!), so their DAGs are simpler. Once again we\u0026rsquo;ll look at the effect of mosquito nets on malaria risk, but this time we\u0026rsquo;ll say that we cannot possibly measure all the confounding factors between net use and malaria risk, so we\u0026rsquo;ll use an instrument to extract the exogeneity from net use.\nAs we talked about in Session 11, good plausible instruments are hard to find: they have to cause bed net use and not be related to malaria risk except through bed net use.\nFor this example, we\u0026rsquo;ll pretend that free bed nets are distributed from town halls around the country. We\u0026rsquo;ll use \u0026ldquo;distance to town hall\u0026rdquo; as our instrument, since it could arguably maybe work perhaps. Being closer to a town hall makes you more likely to use a net, but being closer to a town halls doesn\u0026rsquo;t make put you at higher or lower risk for malaria on its own—it does that only because it changes your likelihood of getting a net.\nThis is where the story for the instrument falls apart, actually; in real life, if you live far away from a town hall, you probably live further from health services and live in more rural places with worse mosquito abatement policies, so you\u0026rsquo;re probably at higher risk of malaria. It\u0026rsquo;s probably a bad instrument, but just go with it.\nHere\u0026rsquo;s the DAG:\niv_dag \u0026lt;- dagify(mal ~ net + U, net ~ dist + U, coords = list(x = c(mal = 4, net = 2, U = 3, dist = 1), y = c(mal = 1, net = 1, U = 2, dist = 1.5)), latent = \u0026#34;U\u0026#34;) ggdag_status(iv_dag) + guides(color = FALSE) + theme_dag()   Specify how those nodes are measured.\nHere\u0026rsquo;s how we\u0026rsquo;ll measure these nodes:\n  Malaria risk: scale from 0–100, mostly around 60, but ranging from 30ish to 90ish. Best to use a Beta distribution.\n  Net use: binary 0/1, TRUE/FALSE variable. However, since we want to use other variables that increase the likelihood of using a net, we\u0026rsquo;ll do some cool tricky stuff with a bed net score, like we did in the observational DAG example earlier.\n  Distance: distance to nearest town hall, measured in kilometers, mostly around 3, with a left skewed long tail (i.e. most people live fairly close, some people live far away). Best to use a Beta distribution (to get the skewed shape) that we then rescale.\n  Unobserved: who knows?! There are a lot of unknown confounders. We could generate columns like income, age, education, and health, make them mathematically related to malaria risk and net use, and then throw those columns away in the final data so they\u0026rsquo;re unobserved. That would be fairly easy and intuitive.\nFor the sake of simplicity here, we\u0026rsquo;ll make a column called \u0026ldquo;risk factors,\u0026rdquo; kind of like we did with the \u0026ldquo;ability\u0026rdquo; column in the instrumental variables example—it\u0026rsquo;s a magical column that is unmeasurable, but it\u0026rsquo;ll open a backdoor path between net use and malaria risk and thus create endogeneity. It\u0026rsquo;ll be normally distributed around 50, with a standard deviation of 25.\n    Specify the relationships between the nodes based on the DAG equations.\nThere are two models in the DAG:\n  net ~ dist + U: Net usage is determined by both distance and our magical unobserved risk factor column. Net use is technically binomial, but in order to change the likelihood of net use based on distance to town hall and unobserved stuff, we\u0026rsquo;ll do the fancy tricky stuff we did in the observational DAG section above: we\u0026rsquo;ll create a bed net score, increase or decrease that score based on risk factors and distance, scale that score to a 0-1 scale of probabilities, and then draw a binomial 0/1 outcome using those probabilities.\nWe\u0026rsquo;ll say that a one kilometer increase in the distance to a town halls reduces the bed net score and a one point increase in risk factors reduces the bed net score.\n  mal ~ net + U: Malaria risk is determined by both net usage and unkown stuff, or the magical column we\u0026rsquo;re calling \u0026ldquo;risk factors.\u0026rdquo; We\u0026rsquo;ll say that a one point increase in risk factors increases malaria risk, and that using a mosquito net causes a decrease of 10 points on average. That\u0026rsquo;s our causal effect.\n    Generate random columns that stand alone. Generate related columns using regression math. Consider adding random noise. This is an entirely trial and error process until you get numbers that look good. Rely heavily on plots as you try different coefficients and parameters. Optionally rescale any columns that go out of reasonable bounds. If you rescale, you\u0026rsquo;ll need to tinker with the coefficients you used since the final effects will also get rescaled.\nFake data time! Here\u0026rsquo;s some heavily annotated code:\n# Make this randomness consistent set.seed(1234) # Simulate 1578 people (just for fun) n_people \u0026lt;- 1578 iv_data \u0026lt;- tibble( # Make an ID column (not necessary, but nice to have) id = 1:n_people, # Generate magical unobserved risk factor variable: normal, 500 ± 300 risk_factors = rnorm(n_people, mean = 100, sd = 25), # Generate distance to town hall variable distance = rbeta(n_people, shape1 = 1, shape2 = 4) ) %\u0026gt;% # Scale up distance to be 0.1-15 instead of 0-1 mutate(distance = rescale(distance, to = c(0.1, 15))) %\u0026gt;% # Generate net variable based on distance, risk factors, and random noise # Note: These -40 and -2 effects are entirely made up and I got them through a # lot of trial and error and rerunning this stupid chunk dozens of times mutate(net_score = 0 + (-40 * distance) + # Distance effect (-2 * risk_factors) + # Risk factor effect rnorm(n_people, mean = 0, sd = 50), # Random noise net_probability = rescale(net_score, to = c(0.15, 1)), # Randomly generate a 0/1 variable using that probability net = rbinom(n_people, 1, net_probability)) %\u0026gt;% # Generate malaria risk variable based on net use, risk factors, and random noise mutate(malaria_risk_base = rbeta(n_people, shape1 = 7, shape2 = 5) * 100, # We\u0026#39;re aiming for a -10 net effect, but need to boost it because of rescaling malaria_effect = (-20 * net) + (0.5 * risk_factors), # Make the final malaria risk score malaria_risk = malaria_risk_base + malaria_effect, # Rescale so it doesn\u0026#39;t go below 0 malaria_risk = rescale(malaria_risk, to = c(5, 80))) iv_data ## # A tibble: 1,578 x 9 ## id risk_factors distance net_score net_probability net malaria_risk_base malaria_effect malaria_risk ## \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 1 69.8 3.98 -202. 0.766 1 71.1 14.9 36.5 ## 2 2 107. 2.14 -284. 0.686 1 75.4 33.5 49.5 ## 3 3 127. 5.47 -469. 0.505 0 57.4 63.6 56.4 ## 4 4 41.4 9.61 -414. 0.558 0 37.2 20.7 20.4 ## 5 5 111. 4.66 -376. 0.596 0 38.5 55.4 40.9 ## 6 6 113. 2.29 -284. 0.686 1 75.7 36.3 51.3 ## 7 7 85.6 0.922 -215. 0.753 0 28.7 42.8 28.2 ## 8 8 86.3 12.5 -608. 0.368 1 50.6 23.2 29.4 ## 9 9 85.9 3.11 -267. 0.703 1 38.4 22.9 22.4 ## 10 10 77.7 1.35 -157. 0.810 1 69.1 18.9 37.6 ## # ... with 1,568 more rows   Verify all relationships with plots and models.\nIs there a relationship between unobserved risk factors and malaria risk? Yep.\nggplot(iv_data, aes(x = risk_factors, y = malaria_risk)) + geom_point(aes(color = as.factor(net))) + geom_smooth(method = \u0026#34;lm\u0026#34;) Is there a relationship between distance to town hall and net use? Yeah, those who live further away are less likely to use a net.\nggplot(iv_data, aes(x = distance, fill = as.factor(net))) + geom_density(alpha = 0.7) Is there a relationship between net use and malaria risk? Haha, yeah, that\u0026rsquo;s a huge highly significant effect. Probably too perfect. We could increase those error bars if we tinker with some of the numbers in the code, but for the sake of this example, we\u0026rsquo;ll leave them like this.\nggplot(iv_data, aes(x = as.factor(net), y = malaria_risk, color = as.factor(net))) + stat_summary(geom = \u0026#34;pointrange\u0026#34;, fun.data = \u0026#34;mean_se\u0026#34;)   Try it out!\nCool, let\u0026rsquo;s see if this works. Remember, we can\u0026rsquo;t actually use the risk_factors column in real life, but we will here just to make sure the effect we built in exists. Here\u0026rsquo;s the true effect, where using a net causes a decrease of 10.9 malaria risk points\nmodel_forbidden \u0026lt;- lm(malaria_risk ~ net + risk_factors, data = iv_data) tidy(model_forbidden) ## # A tibble: 3 x 5 ## term estimate std.error statistic p.value ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 (Intercept) 20.6 0.873 23.6 6.98e-106 ## 2 net -10.8 0.400 -27.1 3.93e-133 ## 3 risk_factors 0.283 0.00788 35.9 1.16e-206 Since we can\u0026rsquo;t actually use that column, we\u0026rsquo;ll use distance to town hall as an instrument. We should run this set of models:\n$$ \\begin{aligned} \\widehat{\\text{Net}} \u0026amp;= \\gamma_0 + \\gamma_1 \\text{Distance to town hall} + \\omega \\\\\n\\text{Malaria risk} \u0026amp;= \\beta_0 + \\beta_1 \\widehat{\\text{Net}} + \\epsilon \\end{aligned} $$\nWe\u0026rsquo;ll run this 2SLS model with the iv_robust() function from the estimatr package:\nlibrary(estimatr) model_iv \u0026lt;- iv_robust(malaria_risk ~ net | distance, data = iv_data) tidy(model_iv) ## term estimate std.error statistic p.value conf.low conf.high df outcome ## 1 (Intercept) 47.202 1.576 29.95 2.344e-156 44.11 50.294 1576 malaria_risk ## 2 net -8.236 2.474 -3.33 8.889e-04 -13.09 -3.385 1576 malaria_risk …and it\u0026rsquo;s relatively close, I guess, at −8.2. Getting instrumental variables to find exact causal effects is tricky, but I\u0026rsquo;m fine with this for simulated data.\n  Save the data.\nThe data works well enough, so we\u0026rsquo;ll get rid of the extra intermediate columns and save it as a CSV file. We\u0026rsquo;ll keep the forbidden risk_factors column just for fun.\niv_data_final \u0026lt;- iv_data %\u0026gt;% select(id, net, distance, malaria_risk, risk_factors) head(iv_data_final) ## # A tibble: 6 x 5 ## id net distance malaria_risk risk_factors ## \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 1 1 3.98 36.5 69.8 ## 2 2 1 2.14 49.5 107.  ## 3 3 0 5.47 56.4 127.  ## 4 4 0 9.61 20.4 41.4 ## 5 5 0 4.66 40.9 111.  ## 6 6 1 2.29 51.3 113. # Save data write_csv(iv_data_final, \u0026#34;data/bed_nets_iv.csv\u0026#34;)   Use synthetic data packages There are several R packages that let you generate synthetic data with built-in relationships in a more automatic way. They all work a little differently, and if you\u0026rsquo;re interested in trying them out, make sure you check the documentation for details.\nfabricatr The fabricatr package is a very powerful package for simulating data. It was invented specifically for using in preregistered studies, so it can handle a ton of different data structures like panel data and time series data. You can build in causal effects and force columns to be correlated with each other.\nfabricatr has exceptionally well-written documentation with like a billion detailed examples (see the right sidebar here). This is a gold standard package and you should most definitely check it out.\nHere\u0026rsquo;s a simple example of simulating a bunch of voters and making older ones more likely to vote:\nlibrary(fabricatr) ## Warning: package \u0026#39;fabricatr\u0026#39; was built under R version 4.0.5 set.seed(1234) fake_voters \u0026lt;- fabricate( # Make 100 people N = 100, # Age uniformly distributed between 18 and 85 age = round(runif(N, 18, 85)), # Older people more likely to vote turnout = draw_binary(prob = ifelse(age \u0026lt; 40, 0.4, 0.7), N = N) ) head(fake_voters) ## ID age turnout ## 1 001 26 0 ## 2 002 60 1 ## 3 003 59 1 ## 4 004 60 1 ## 5 005 76 1 ## 6 006 61 1 And here\u0026rsquo;s an example of country-year panel data where there are country-specific and year-specific effects on GDP:\nset.seed(1234) panel_global_data \u0026lt;- fabricate( years = add_level( N = 10, ts_year = 0:9, year_shock = rnorm(N, 0, 0.3) ), countries = add_level( N = 5, base_gdp = runif(N, 15, 22), growth_units = runif(N, 0.25, 0.5), growth_error = runif(N, 0.15, 0.5), nest = FALSE ), country_years = cross_levels( by = join(years, countries), gdp_measure = base_gdp + year_shock + (ts_year * growth_units) + rnorm(N, sd = growth_error) ) ) %\u0026gt;% # Scale up the years to be actual years instead of 1, 2, 3, etc. mutate(year = ts_year + 2010) head(panel_global_data) ## years ts_year year_shock countries base_gdp growth_units growth_error country_years gdp_measure year ## 1 01 0 -0.36212 1 17.22 0.4526 0.3096 01 17.07 2010 ## 2 02 1 0.08323 1 17.22 0.4526 0.3096 02 17.55 2011 ## 3 03 2 0.32533 1 17.22 0.4526 0.3096 03 18.72 2012 ## 4 04 3 -0.70371 1 17.22 0.4526 0.3096 04 17.99 2013 ## 5 05 4 0.12874 1 17.22 0.4526 0.3096 05 19.25 2014 ## 6 06 5 0.15182 1 17.22 0.4526 0.3096 06 19.63 2015 ggplot(panel_global_data, aes(x = year, y = gdp_measure, color = countries)) + geom_line() + labs(x = \u0026#34;Year\u0026#34;, y = \u0026#34;Log GDP\u0026#34;, color = \u0026#34;Countries\u0026#34;) That all just scratches the surface of what fabricatr can do. Again, check the examples and documentation and play around with it to see what else it can do.\nwakefield The wakefield package is jokingly named after Andrew Wakefield, the British researcher who invented fake data to show that the MMR vaccine causes autism. This package lets you quickly generate random fake datasets. It has a bunch of pre-set column possibilities, like age, color, Likert scales, political parties, religion, and so on, and you can also use standard R functions like rnorm(), rbinom(), or rbeta(). It also lets you create repeated measures (1st grade score, 2nd grade score, 3rd grade score, etc.) and build correlations between variables.\nYou should definitely look at the documentation to see a ton of examples of how it all works. Here\u0026rsquo;s a basic example:\nlibrary(wakefield) set.seed(1234) wakefield_data \u0026lt;- r_data_frame( n = 500, id, treatment = rbinom(1, 0.3), # 30% chance of being in treatment outcome = rnorm(mean = 500, sd = 100), race, age = age(x = 18:45), sex = sex_inclusive(), survey_question_1 = likert(), survey_question_2 = likert() ) head(wakefield_data) ## # A tibble: 6 x 8 ## ID treatment outcome Race age sex survey_question_1 survey_question_2 ## \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;ord\u0026gt; \u0026lt;ord\u0026gt;  ## 1 001 0 544. White 35 Intersex Disagree Agree  ## 2 002 0 606. White 38 Male Disagree Strongly Disagree ## 3 003 0 545. White 38 Female Neutral Strongly Agree  ## 4 004 0 566. Black 45 Female Strongly Agree Disagree  ## 5 005 1 386. Black 41 Male Disagree Agree  ## 6 006 0 463. Hispanic 20 Female Disagree Agree faux The faux package does some really neat things. We can create data that has built-in correlations without going through all the math. For instance, let\u0026rsquo;s say we have 3 variables A, B, and C that are normally distributed with these parameters:\n A: mean = 10, sd = 2 B: mean = 5, sd = 1 C: mean = 20, sd = 5  We want A to correlate with B at r = 0.8 (highly correlated), A to correlate with C at r = 0.3 (less correlated), and B to correlate with C at r = 0.4 (moderately correlated). Here\u0026rsquo;s how to create that data with faux:\nlibrary(faux) set.seed(1234) faux_data \u0026lt;- rnorm_multi(n = 100, mu = c(10, 5, 20), sd = c(2, 1, 5), r = c(0.8, 0.3, 0.4), varnames = c(\u0026#34;A\u0026#34;, \u0026#34;B\u0026#34;, \u0026#34;C\u0026#34;), empirical = FALSE) head(faux_data) ## A B C ## 1 11.742 5.612 25.89 ## 2 9.060 4.177 18.79 ## 3 9.373 4.466 14.57 ## 4 10.913 5.341 31.88 ## 5 8.221 4.039 18.14 ## 6 10.095 4.517 17.43 # Check averages and standard deviations faux_data %\u0026gt;% # Convert to long/tidy so we can group and summarize pivot_longer(cols = everything(), names_to = \u0026#34;variable\u0026#34;, values_to = \u0026#34;value\u0026#34;) %\u0026gt;% group_by(variable) %\u0026gt;% summarize(mean = mean(value), sd = sd(value)) ## # A tibble: 3 x 3 ## variable mean sd ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 A 10.2 2.08 ## 2 B 5.02 1.00 ## 3 C 20.8 5.01 # Check correlations cor(faux_data$A, faux_data$B) ## [1] 0.808 cor(faux_data$A, faux_data$C) ## [1] 0.301 cor(faux_data$B, faux_data$C) ## [1] 0.4598 faux can do a ton of other things too, so make sure you check out the documentation and all the articles with examples here.\n","date":1629676800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1629676800,"objectID":"9dc6815e2265e49aa0cceab0d96ea4f7","permalink":"/example/01-datos/","publishdate":"2021-08-23T00:00:00Z","relpermalink":"/example/01-datos/","section":"example","summary":"In the example guide for generating random numbers, we explored how to use a bunch of different statistical distributions to create variables that had reasonable values. However, each of the columns that we generated there were completely independent of each other.","tags":null,"title":"The ultimate guide to generating synthetic data for causal inference","type":"docs"},{"authors":null,"categories":null,"content":"Readings   Chapter 2 in Impact Evaluation in Practice1  Chapter 2 in Evaluation: A Systematic Approach.2 This is available on iCollege.  Chapter 3 in Evaluation: A Systematic Approach.3 This is available on iCollege.  Slides The slides for today’s lesson are available online as an HTML file. Use the buttons below to open the slides either as an interactive website or as a static PDF (for printing or storing for later). You can also click in the slides below and navigate through them with your left and right arrow keys.\n View all slides in new window  Download PDF of all slides\nIntroduction\r\rReproducibility\r\rProgram theories\r\rLogic models \u0026 results chains\r\r\r\r\r\r\r\r\r\r\r\r\r\r\r\rFun fact: If you type ? (or shift + /) while going through the slides, you can see a list of special slide-specific commands.\r\rVideos Videos for each section of the lecture are available at this YouTube playlist.\n Introduction Reproducibility Program theories Logic models \u0026amp; results chains  You can also watch the playlist (and skip around to different sections) here:\n\r\rOptional lab Here are all the materials we used in the lab:\n RStudio.cloud project Slides Project .zip file (the slides are in here too)    Paul J. Gertler et al., Impact Evaluation in Practice, 2nd ed. (Inter-American Development Bank; World Bank, 2016), https://openknowledge.worldbank.org/handle/10986/25030. \u0026#x21a9;\u0026#xfe0e;\n Peter H. Rossi, Mark W. Lipsey, and Gary T. Henry, Evaluation: A Systematic Approach, 8th ed. (Los Angeles: Sage, 2019). \u0026#x21a9;\u0026#xfe0e;\n Ibid. \u0026#x21a9;\u0026#xfe0e;\n   ","date":1629676800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1629676800,"objectID":"0316e4026587433def89bf10f5c614d0","permalink":"/content/03-content/","publishdate":"2021-08-23T00:00:00Z","relpermalink":"/content/03-content/","section":"content","summary":"Readings   Chapter 2 in Impact Evaluation in Practice1  Chapter 2 in Evaluation: A Systematic Approach.2 This is available on iCollege.  Chapter 3 in Evaluation: A Systematic Approach.","tags":null,"title":"Importar y explorar datos","type":"docs"},{"authors":null,"categories":null,"content":"DAGs with dagitty.net The easiest way to quickly build DAGs and find adjustment sets and testable implications is to use dagitty.net.\nThis video shows how to use it:\n\r\rDAGs with R, ggdag, and dagitty You can use the ggdag and dagitty packages in R to build and work with DAGs too. I typically draw an initial DAG in my browser with dagitty.net and then I rewrite it in code in R so that it\u0026rsquo;s more official and formal and reproducible.\nLive coding example \r\rBasic DAGs (This is a heavily cleaned up and annotated version of the code from the video.)\n# Load packages library(tidyverse) # For dplyr, ggplot, and friends library(ggdag) # For plotting DAGs library(dagitty) # For working with DAG logic The general process for making and working with DAGs in R is to create a DAG object with dagify() and then plot it with ggdag(). The documentation for ggdag is really good and helpful and full of examples. Check these pages for all sorts of additional details:\n \u0026ldquo;An introduction to ggdag\u0026rdquo; \u0026ldquo;An introduction to directed acyclic graphs\u0026rdquo; List of all ggdag-related functions  The syntax for dagify() is similar to the formula syntax you\u0026rsquo;ve been using for building regression models with lm(). The formulas you use in dagify() indicate the relationships between nodes.\nFor instance, in this DAG, y is caused by x, a, and b (y ~ x + a + b), and x is caused by a and b (x ~ a + b). This means that a and b are confounders. Use the exposure and outcome arguments to specify which nodes are the exposure (or treatment/intervention/program) and the outcome.\n# Create super basic DAG simple_dag \u0026lt;- dagify( y ~ x + a + b, x ~ a + b, exposure = \u0026#34;x\u0026#34;, outcome = \u0026#34;y\u0026#34; ) # Adding a theme_dag() layer to the plot makes it have a white background with no axis labels ggdag(simple_dag) + theme_dag() If you want to plot which nodes are the exposure and outcome, use ggdag_status() instead:\nggdag_status(simple_dag) + theme_dag() Layouts and manual coordinates Notice how the layout is different in both of those graphs. By default, ggdag() positions the nodes randomly every time using a network algorithm. You can change the algorithm by using the layout argument: ggdag(simple_dag, layout = \u0026quot;nicely\u0026quot;). You can see a full list of possible algorithms by running ?layout_tbl_graph_igraph in the console.\nAlternatively, you can specify your own coordinates so that the nodes are positioned in the same place every time. Do this with the coords argument in dagify().\nThe best way to figure out what these numbers should be is to draw the DAG on paper or on a whiteboard and add a grid to it and then figure out the coordinates. For instance, in this DAG there are three rows and three columns: x and y go in the middle row (row 2) while a and b go in the middle column (column 2). It can also be helpful to not include theme_dag() at first so you can see the numbers for the underlying grid. Once you have everything positioned correctly, add theme_dag() to clean it up.\nsimple_dag_with_coords \u0026lt;- dagify( y ~ x + a + b, x ~ a + b, exposure = \u0026#34;x\u0026#34;, outcome = \u0026#34;y\u0026#34;, coords = list(x = c(x = 1, a = 2, b = 2, y = 3), y = c(x = 2, a = 1, b = 3, y = 2)) ) ggdag_status(simple_dag_with_coords) + theme_dag() Node names and labels The variable names you use do not have to be limited to just x, y, and other lowercase letters. You can any names you want, as long as there are no spaces.\ndag_with_var_names \u0026lt;- dagify( outcome ~ treatment + confounder1 + confounder2, treatment ~ confounder1 + confounder2, exposure = \u0026#34;treatment\u0026#34;, outcome = \u0026#34;outcome\u0026#34; ) ggdag_status(dag_with_var_names) + theme_dag() However, unless you use very short names, it is likely that the text will not fit inside the nodes. To get around this, you can add labels to the nodes using the labels argument in dagify(). Plot the labels by setting use_labels = \u0026quot;label\u0026quot; in ggdag(). You can turn off the text in the nodes with text = FALSE in ggdag().\nsimple_dag_with_coords_and_labels \u0026lt;- dagify( y ~ x + a + b, x ~ a + b, exposure = \u0026#34;x\u0026#34;, outcome = \u0026#34;y\u0026#34;, labels = c(y = \u0026#34;Outcome\u0026#34;, x = \u0026#34;Treatment\u0026#34;, a = \u0026#34;Confounder 1\u0026#34;, b = \u0026#34;Confounder 2\u0026#34;), coords = list(x = c(x = 1, a = 2, b = 2, y = 3), y = c(x = 2, a = 1, b = 3, y = 2)) ) ggdag_status(simple_dag_with_coords_and_labels, use_labels = \u0026#34;label\u0026#34;, text = FALSE) + guides(fill = FALSE, color = FALSE) + # Disable the legend theme_dag() Paths and adjustment sets R can also perform analysis on DAG objects. For example, we can find all the testable implications from the DAG using the impliedConditionalIndependencies() function from the dagitty package. For this simple DAG, there is only one: a should be independent of b. If we had a dataset with columns for each of these variables, we could check if this is true by running cor(a, b) to see if the two are related.\nimpliedConditionalIndependencies(simple_dag) ## a _||_ b We can also find all the paths between x and y using the paths() function from the dagitty package. We can see that there are three open paths between x and y:\npaths(simple_dag) ## $paths ## [1] \u0026#34;x -\u0026gt; y\u0026#34; \u0026#34;x \u0026lt;- a -\u0026gt; y\u0026#34; \u0026#34;x \u0026lt;- b -\u0026gt; y\u0026#34; ##  ## $open ## [1] TRUE TRUE TRUE The first open path is fine—we want a single d-connected relationship between treatment and outcome—but the other two indicate that there is confounding from a and b. We can see what each of these paths are with the ggdag_paths() function from the ggdag package:\nggdag_paths(simple_dag_with_coords) + theme_dag() Instead of listing out all the possible paths and identifying backdoors by hand, you can use the adjustmentSets() function in the dagitty package to programmatically find all the nodes that need to be adjusted. Here we see that both a and b need to be controlled for to isolate the x -\u0026gt; y relationship.\nadjustmentSets(simple_dag) ## { a, b } You can also visualize the adjustment sets with ggdag_adjustment_set() in the ggdag package. Make sure you set shadow = TRUE to draw the arrows coming out of the adjusted nodes—by default, those are not included.\nggdag_adjustment_set(simple_dag_with_coords, shadow = TRUE) + theme_dag() R will find minimally sufficient adjustment sets, which includes the fewest number of adjustments needed to close all backdoors between x and y. In this example DAG there was only one set of variables (a and b), but in other situations there could be many possible sets, or none if the causal effect is not identifiable.\nPlot DAG from dagitty.net with ggdag() If you use dagitty.net to draw a DAG, you\u0026rsquo;ll notice that it generates some code for you in the model code section:\nYou can copy that dag{ ... } code and paste it into R to define a DAG object rather than use the dagify() function. To do this, use the dagitty() function from the dagitty library and include the whole generated model code in single quotes (''):\nmodel_from_dagitty \u0026lt;- dagitty(\u0026#39;dag { bb=\u0026#34;0,0,1,1\u0026#34; \u0026#34;A confounder\u0026#34; [pos=\u0026#34;0.809,0.306\u0026#34;] \u0026#34;Another confounder\u0026#34; [pos=\u0026#34;0.810,0.529\u0026#34;] \u0026#34;Some outcome\u0026#34; [outcome,pos=\u0026#34;0.918,0.432\u0026#34;] \u0026#34;Some treatment\u0026#34; [exposure,pos=\u0026#34;0.681,0.426\u0026#34;] \u0026#34;A confounder\u0026#34; -\u0026gt; \u0026#34;Some outcome\u0026#34; \u0026#34;A confounder\u0026#34; -\u0026gt; \u0026#34;Some treatment\u0026#34; \u0026#34;Another confounder\u0026#34; -\u0026gt; \u0026#34;Some outcome\u0026#34; \u0026#34;Another confounder\u0026#34; -\u0026gt; \u0026#34;Some treatment\u0026#34; \u0026#34;Some treatment\u0026#34; -\u0026gt; \u0026#34;Some outcome\u0026#34; } \u0026#39;) ggdag(model_from_dagitty) + theme_dag() By default it\u0026rsquo;s going to look ugly because (1) the node labels don\u0026rsquo;t fit, and (2) slight differences in the coordinates make it so the nodes don\u0026rsquo;t perfectly align with each other. To fix coordinate alignment, you can modify the numbers in the generated DAG code. Here I rounded the numbers so that they\u0026rsquo;re all 0.3, 0.8, etc.\nTo fix the label issue, you can add the use_labels argument like normally. Only here, you can\u0026rsquo;t specify use_labels = \u0026quot;label\u0026quot;. Instead, when you specify a DAG using dagitty\u0026rsquo;s code like this, the column in the underlying dataset that contains the labels is named name, so you need to use use_labels = \u0026quot;name\u0026quot;.\nOther ggdag() variations like ggdag_status() will still work fine.\nmodel_from_dagitty_rounded \u0026lt;- dagitty(\u0026#39;dag { bb=\u0026#34;0,0,1,1\u0026#34; \u0026#34;A confounder\u0026#34; [pos=\u0026#34;0.8,0.3\u0026#34;] \u0026#34;Another confounder\u0026#34; [pos=\u0026#34;0.8,0.5\u0026#34;] \u0026#34;Some outcome\u0026#34; [outcome,pos=\u0026#34;0.9,0.4\u0026#34;] \u0026#34;Some treatment\u0026#34; [exposure,pos=\u0026#34;0.7,0.4\u0026#34;] \u0026#34;A confounder\u0026#34; -\u0026gt; \u0026#34;Some outcome\u0026#34; \u0026#34;A confounder\u0026#34; -\u0026gt; \u0026#34;Some treatment\u0026#34; \u0026#34;Another confounder\u0026#34; -\u0026gt; \u0026#34;Some outcome\u0026#34; \u0026#34;Another confounder\u0026#34; -\u0026gt; \u0026#34;Some treatment\u0026#34; \u0026#34;Some treatment\u0026#34; -\u0026gt; \u0026#34;Some outcome\u0026#34; } \u0026#39;) ggdag_status(model_from_dagitty_rounded, text = FALSE, use_labels = \u0026#34;name\u0026#34;) + guides(color = FALSE) + # Turn off legend theme_dag() Mosquito net example Conditional independencies You can test if your stated relationships are correct by looking at the conditional independencies that are implied by the DAG. In dagitty.net, these appear in the sidebar in the \u0026ldquo;Testable implications\u0026rdquo; section. To show how this works, we\u0026rsquo;ll use a simulated dataset that I generated about a fictional mosquito net program. Download the data here if you want to follow along:\n  mosquito_nets.csv  Researchers are interested in whether using mosquito nets decreases an individual\u0026rsquo;s risk of contracting malaria. They have collected data from 1,752 households in an unnamed country and have variables related to environmental factors, individual health, and household characteristics. Additionally, this country has a special government program that provides free mosquito nets to households that meet specific requirements: to qualify for the program, there must be more than 4 members of the household, and the household\u0026rsquo;s monthly income must be lower than $700 a month. Households are not automatically enrolled in the program, and many do not use it. The data is not experimental—researchers have no control over who uses mosquito nets, and individual households make their own choices over whether to apply for free nets or buy their own nets, as well as whether they use the nets if they have them.\nmosquito_dag \u0026lt;- dagify( malaria_risk ~ net + income + health + temperature + resistance, net ~ income + health + temperature + eligible + household, eligible ~ income + household, health ~ income, exposure = \u0026#34;net\u0026#34;, outcome = \u0026#34;malaria_risk\u0026#34;, coords = list(x = c(malaria_risk = 7, net = 3, income = 4, health = 5, temperature = 6, resistance = 8.5, eligible = 2, household = 1), y = c(malaria_risk = 2, net = 2, income = 3, health = 1, temperature = 3, resistance = 2, eligible = 3, household = 2)), labels = c(malaria_risk = \u0026#34;Risk of malaria\u0026#34;, net = \u0026#34;Mosquito net\u0026#34;, income = \u0026#34;Income\u0026#34;, health = \u0026#34;Health\u0026#34;, temperature = \u0026#34;Nighttime temperatures\u0026#34;, resistance = \u0026#34;Insecticide resistance\u0026#34;, eligible = \u0026#34;Eligible for program\u0026#34;, household = \u0026#34;Number in household\u0026#34;) ) ggdag_status(mosquito_dag, use_labels = \u0026#34;label\u0026#34;, text = FALSE) + guides(fill = FALSE, color = FALSE) + # Disable the legend theme_dag() The causal graph above outlines the complete relationship between mosquito net use and risk of malaria. Each node in the DAG is a column in the dataset collected by the researchers, and includes the following:\n Malaria risk (malaria_risk): The likelihood that someone in the household will be infected with malaria. Measured on a scale of 0–100, with higher values indicating higher risk. Mosquito net (net and net_num): A binary variable indicating if the household used mosquito nets. Eligible for program (eligible): A binary variable indicating if the household is eligible for the free net program. Income (income): The household\u0026rsquo;s monthly income, in US dollars. Nighttime temperatures (temperature): The average temperature at night, in Celsius. Health (health): Self-reported healthiness in the household. Measured on a scale of 0–100, with higher values indicating better health. Number in household (household): Number of people living in the household. Insecticide resistance (resistance): Some strains of mosquitoes are more resistant to insecticide and thus pose a higher risk of infecting people with malaria. This is measured on a scale of 0–100, with higher values indicating higher resistance.  According to the DAG, malaria risk is caused by income, temperatures, health, insecticide resistance, and mosquito net use. People who live in hotter regions, have lower incomes, have worse health, are surrounded by mosquitoes with high resistance to insecticide, and who do not use mosquito nets are at higher risk of contracting malaria than those who do not. Mosquito net use is caused by income, nighttime temperatures, health, the number of people living in the house, and eligibility for the free net program. People who live in areas that are cooler at night, have higher incomes, have better health, have more people in the home, and are eligible for free government nets are more likely to regularly use nets than those who do not. The DAG also shows that eligibility for the free net program is caused by income and household size, since households must meet specific thresholds to qualify.\nFirst, let\u0026rsquo;s download the dataset, put in a folder named data, and load it:\n  mosquito_nets.csv  # Load the data. # It\u0026#39;d be a good idea to click on the \u0026#34;mosquito_nets.csv\u0026#34; object in the # Environment panel in RStudio to see what the data looks like after you load it mosquito_nets \u0026lt;- read_csv(\u0026#34;data/mosquito_nets.csv\u0026#34;) We can use this data to check if the relationships defined by our DAG reflect reality. Recall that d-separation implies that nodes are statistically independent of each other and do not transfer associational information. If you draw the mosquito net DAG with dagitty.net, or if you run impliedConditionalIndependencies() in R, you can see a list of all the implied conditional independencies.\nimpliedConditionalIndependencies(mosquito_dag) ## elgb _||_ hlth | incm ## elgb _||_ mlr_ | hlth, incm, net, tmpr ## elgb _||_ rsst ## elgb _||_ tmpr ## hlth _||_ hshl ## hlth _||_ rsst ## hlth _||_ tmpr ## hshl _||_ incm ## hshl _||_ mlr_ | hlth, incm, net, tmpr ## hshl _||_ rsst ## hshl _||_ tmpr ## incm _||_ rsst ## incm _||_ tmpr ## net _||_ rsst ## rsst _||_ tmpr The _||_ symbol in the output here is the \\(\\perp\\) symbol, which means \u0026ldquo;independent of\u0026rdquo;. The | in the output means \u0026ldquo;given\u0026rdquo;.\nIn the interest of space, we will not verify all these implied independencies, but we can test a few of them:\n  \\(\\text{Health} \\perp \\text{Household members}\\): (Read as \u0026ldquo;Health is independent of household member count\u0026rdquo;.) Health should be independent of the number of people in each household. In the data, the two variables should not be correlated. This is indeed the case:\ncor(mosquito_nets$health, mosquito_nets$household) ## [1] 9.8e-05   \\(\\text{Income} \\perp \\text{Insecticide resistance}\\): (Read as \u0026ldquo;Income is independent of insecticide resistance\u0026rdquo;.) Income should be independent of insecticide resistance. This is again true:\ncor(mosquito_nets$income, mosquito_nets$resistance) ## [1] 0.014   \\(\\text{Malaria risk} \\perp \\text{Household members}\\ |\\ \\text{Health, Income, Bed net use, Temperature}\\): (Read as \u0026ldquo;Malaria risk is independent of house member count given health, income, bed net use, and temperature\u0026rdquo;.) Malaria risk should be independent of the number of household members given similar levels of health, income, mosquito net use, and nighttime temperatures. We cannot use cor() to test this implication, since there are many variables involved, but we can use a regression model to check if the number of household members is significantly related to malaria risk. It is not significant ($t = -0.17$, \\(p = 0.863\\)), which means the two are independent, as expected.\nlm(malaria_risk ~ household + health + income + net + temperature, data = mosquito_nets) %\u0026gt;% broom::tidy() ## # A tibble: 6 x 5 ## term estimate std.error statistic p.value ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 (Intercept) 76.2 0.966 78.9 0.  ## 2 household -0.0155 0.0893 -0.173 8.63e- 1 ## 3 health 0.148 0.0107 13.9 9.75e- 42 ## 4 income -0.0751 0.00104 -72.6 0.  ## 5 netTRUE -10.4 0.266 -39.2 2.63e-241 ## 6 temperature 1.01 0.0310 32.5 1.88e-181   We can check all the other conditional independencies to see if the DAG captures the reality of the full system of factors that influence mosquito net use and malaria risk. If there are substantial and significant correlations between nodes that should be independent, there is likely an issue with the specification of the DAG. Return to the theory of how the phenomena are generated and refine the DAG more.\nMosquito net adjustment sets There is a direct path between mosquito net use and the risk of malaria, but the effect is not causally identified due to several other open paths. We can either list out all the paths and find which open paths have arrows pointing backwards into the mosquito net node (run paths(mosquito_dag) to see these results), or we can let R find the appropriate adjustment sets automatically:\nadjustmentSets(mosquito_dag) ## { health, income, temperature } Based on the relationships between all the nodes in the DAG, adjusting for health, income, and temperature is enough to close all backdoors and identify the relationship between net use and malaria risk. Importantly, we do not need to worry about any of the nodes related to the government program for free nets, since those nodes are not d-connected to malaria risk. We only need to worry about confounding relationships.\nWe can confirm this graphically with ggdag_adjustment_set():\nggdag_adjustment_set(mosquito_dag, shadow = TRUE, use_labels = \u0026#34;label\u0026#34;, text = FALSE) + theme_dag() ","date":1629676800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1629676800,"objectID":"63ac3bb2c13d1c040f2504a8ef534581","permalink":"/example/03-practico/","publishdate":"2021-08-23T00:00:00Z","relpermalink":"/example/03-practico/","section":"example","summary":"DAGs with dagitty.net The easiest way to quickly build DAGs and find adjustment sets and testable implications is to use dagitty.net.\nThis video shows how to use it:\n\r\rDAGs with R, ggdag, and dagitty You can use the ggdag and dagitty packages in R to build and work with DAGs too.","tags":null,"title":"Importar, explorar y limpiar datos","type":"docs"},{"authors":null,"categories":null,"content":"\rIf you want to follow along with this example, you can download the data below:\n  sat_gpa.csv  You can also download a complete .zip file with a finished R Markdown file that you can knit and play with on your own:\n  regression-example.zip  Live coding example \r\rComplete code (This is a heavily cleaned up and annotated version of the code from the video.)\nIntroduction SAT scores have long been a major factor in college admissions, under the assumption that students with higher test scores will perform better in college and receive a higher GPA. The SAT\u0026rsquo;s popularity has dropped in recent years, though, and this summer, the University of Chicago announced that it would stop requiring SAT scores for all prospective undergraduates.\nEducational Testing Service (ETS), the creator of the SAT, collected SAT scores, high school GPAs, and freshman-year-college GPAs for 1,000 students at an unnamed university.1\nYou are a university admissions officer and you are curious if SAT scores really do predict college performance. You\u0026rsquo;re also interested in other factors that could influence student performance.\nThe data contains 6 variables:\n sex: The sex of the student (male or female; female is the base case) sat_verbal: The student\u0026rsquo;s percentile score in the verbal section of the SAT sat_math: The student\u0026rsquo;s percentile score in the math section of the SAT sat_total: sat_verbal + sat_math gpa_hs: The student\u0026rsquo;s GPA in high school at graduation gpa_fy: The student\u0026rsquo;s GPA at the end of their freshman year  # First we load the libraries and data library(tidyverse) # This lets you create plots with ggplot, manipulate data, etc. library(broom) # This lets you convert regression models into nice tables library(modelsummary) # This lets you combine multiple regression models into a single table # Load the data. # It\u0026#39;d be a good idea to click on the \u0026#34;sat_gpa\u0026#34; object in the Environment panel # in RStudio to see what the data looks like after you load it. sat_gpa \u0026lt;- read_csv(\u0026#34;data/sat_gpa.csv\u0026#34;) Exploratory questions How well do SAT scores correlate with freshman GPA? # Note the syntax here with the $. That lets you access columns. The general # pattern is name_of_data_set$name_of_column cor(sat_gpa$gpa_fy, sat_gpa$sat_total) ## [1] 0.46 SAT scores and first-year college GPA are moderately positively correlated (r = 0.46). As one goes up, the other also tends to go up.\nHere\u0026rsquo;s what that relationship looks like:\nggplot(sat_gpa, aes(x = sat_total, y = gpa_fy)) + geom_point(size = 0.5) + geom_smooth(method = \u0026#34;lm\u0026#34;, se = FALSE) + labs(x = \u0026#34;Total SAT score\u0026#34;, y = \u0026#34;Freshman GPA\u0026#34;) How well does high school GPA correlate with freshman GPA? cor(sat_gpa$gpa_fy, sat_gpa$gpa_hs) ## [1] 0.54 High school and freshman GPAs are also moderately correlated (r = 0.54), but with a slightly stronger relationship.\nHere\u0026rsquo;s what that relationship looks like:\nggplot(sat_gpa, aes(x = gpa_hs, y = gpa_fy)) + geom_point(size = 0.5) + geom_smooth(method = \u0026#34;lm\u0026#34;, se = FALSE) + labs(x = \u0026#34;High school GPA\u0026#34;, y = \u0026#34;Freshman GPA\u0026#34;) Is the correlation between SAT scores and freshman GPA stronger for men or for women? # We can calculate the correlation for subgroups within the data with slightly # different syntax. Notice how this uses the pipe (%\u0026gt;%), which makes it read # like English. We can say \u0026#34;Take the sat_gpa data set, split it into groups # based on sex, and calculate the correlation between sat_total and gpa_fy in # each of the groups sat_gpa %\u0026gt;% group_by(sex) %\u0026gt;% summarize(correlation = cor(sat_total, gpa_fy)) ## # A tibble: 2 x 2 ## sex correlation ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; ## 1 Female 0.493 ## 2 Male 0.481 We can calculate the correlation between SAT scores and freshman GPA for both sexes to see if there are any differences. The correlation is slightly stronger for women, but it\u0026rsquo;s hardly noticeable (r = 0.49 for females, r = 0.48 for males)\nThis is apparent visually if we include a trendline for each sex. The lines are essentially parallel:\n# The only difference between this graph and the earlier two is that it is # coloring by sex ggplot(sat_gpa, aes(x = gpa_hs, y = gpa_fy, color = sex)) + geom_point(size = 0.5) + geom_smooth(method = \u0026#34;lm\u0026#34;, se = FALSE) + labs(x = \u0026#34;High school GPA\u0026#34;, y = \u0026#34;Freshman GPA\u0026#34;) Is the correlation between high school GPA and freshman GPA stronger for men or for women? sat_gpa %\u0026gt;% group_by(sex) %\u0026gt;% summarize(correlation = cor(gpa_hs, gpa_fy)) ## # A tibble: 2 x 2 ## sex correlation ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; ## 1 Female 0.597 ## 2 Male 0.483 There is a noticeable difference between men and women in the correlation between high school and college GPA. For men, the two are moderately correlated (r = 0.48), while for women the relationship is much stronger (r = 0.60). High school grades might be a better predictor of college grades for women than for men.\nModels Do SAT scores predict freshman GPAs? We can build a model that predicts college GPAs (our outcome variable, or dependent variable) using SAT scores (our main explanatory variable):\n$$ \\text{freshman GPA} = \\beta_0 + \\beta_1 \\text{SAT total} + \\epsilon $$\nmodel_sat_gpa \u0026lt;- lm(gpa_fy ~ sat_total, data = sat_gpa) # Look at the model results and include confidence intervals for the coefficients tidy(model_sat_gpa, conf.int = TRUE) ## # A tibble: 2 x 7 ## term estimate std.error statistic p.value conf.low conf.high ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 (Intercept) 0.00193 0.152 0.0127 9.90e- 1 -0.296 0.300  ## 2 sat_total 0.0239 0.00146 16.4 1.39e-53 0.0210 0.0267 Here\u0026rsquo;s what these coefficients mean:\n The intercept (or \\(\\beta_0\\)) is 0.002, which means that the average freshman GPA will be 0.002 when the total SAT percentile score is 0. This is a pretty nonsensical number (nobody has a score that low), so we can ignore it. The slope of sat_total (or \\(\\beta_1\\)) is 0.024, which means that a 1 percentile increase in SAT score is associated with a 0.024 point increase in GPA, on average.  We can look at the summary table of the regression to check the \\(R^2\\):\nglance(model_sat_gpa) ## # A tibble: 1 x 12 ## r.squared adj.r.squared sigma statistic p.value df logLik AIC BIC deviance df.residual nobs ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; ## 1 0.212 0.211 0.658 268. 1.39e-53 1 -999. 2005. 2019. 432. 998 1000 The \\(R^2\\) here is 0.212, which means that SAT scores explain 21% of the variation in freshman GPA. It\u0026rsquo;s not a fantastic model, but it explains some stuff.\nDoes a certain type of SAT score have a larger effect on freshman GPAs? The sat_total variable combines both sat_math and sat_verbal. We can disaggregate the total score to see the effect of each portion of the test on freshman GPA, using the following model:\n$$ \\text{freshman GPA} = \\beta_0 + \\beta_1 \\text{SAT verbal} + \\beta_2 \\text{SAT math} + \\epsilon $$\nmodel_sat_gpa_types \u0026lt;- lm(gpa_fy ~ sat_verbal + sat_math, data = sat_gpa) tidy(model_sat_gpa_types, conf.int = TRUE) ## # A tibble: 3 x 7 ## term estimate std.error statistic p.value conf.low conf.high ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 (Intercept) 0.00737 0.152 0.0484 9.61e- 1 -0.291 0.306  ## 2 sat_verbal 0.0254 0.00286 8.88 3.07e-18 0.0198 0.0310 ## 3 sat_math 0.0224 0.00279 8.04 2.58e-15 0.0169 0.0279 Again, the intercept is meaningless since no student has a zero on both the verbal and the math test. The slope for sat_verbal (or \\(\\beta_1\\)) is 0.025, so a one percentile point increase in the verbal SAT is associated with a 0.025 point increase in GPA, on average, controlling for math scores. Meanwhile, a one percentile point increase in the math SAT ($\\beta_2$) is associated with a 0.022 point increase in GPA, on average, controlling for verbal scores. These are essentially the same, so at first glance, it doesn\u0026rsquo;t seem like the type of test has substantial bearing on college GPAs.\nThe adjusted \\(R^2\\) (which we need to look at because we\u0026rsquo;re using more than one explanatory variable) is 0.211, which means that this model explains 21% of the variation in college GPA. Like before, this isn\u0026rsquo;t great, but it tells us a little bit about the importance of SAT scores.\nglance(model_sat_gpa_types) ## # A tibble: 1 x 12 ## r.squared adj.r.squared sigma statistic p.value df logLik AIC BIC deviance df.residual nobs ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; ## 1 0.212 0.211 0.658 134. 2.36e-52 2 -999. 2006. 2026. 432. 997 1000 Do high school GPAs predict freshman GPAs? We can also use high school GPA to predict freshman GPA, using the following model:\n$$ \\text{freshman GPA} = \\beta_0 + \\beta_1 \\text{high school GPA} + \\epsilon $$\nmodel_sat_gpa_hs \u0026lt;- lm(gpa_fy ~ gpa_hs, data = sat_gpa) tidy(model_sat_gpa_hs) ## # A tibble: 2 x 5 ## term estimate std.error statistic p.value ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 (Intercept) 0.0913 0.118 0.775 4.39e- 1 ## 2 gpa_hs 0.743 0.0363 20.4 6.93e-78 The intercept here ($\\beta_0$) is 0.091, which means that a student with a high school GPA of zero would have a predicted freshman GPA of 0.091, on average. This is nonsensical, so we can ignore it. The slope for gpa_hs ($\\beta_1$), on the other hand, is helpful. For every 1 point increase in GPA (i.e. moving from 2.0 to 3.0, or 3.0 to 4.0), there\u0026rsquo;s an associated increase in college GPA of 0.743 points, on average.\nThe \\(R^2\\) value is 0.295, which means that nearly 30% of the variation in college GPA can be explained by high school GPA. Neat.\nglance(model_sat_gpa_hs) ## # A tibble: 1 x 12 ## r.squared adj.r.squared sigma statistic p.value df logLik AIC BIC deviance df.residual nobs ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; ## 1 0.295 0.295 0.622 418. 6.93e-78 1 -943. 1893. 1908. 386. 998 1000 College GPA ~ SAT + sex Next, we can see how both SAT scores and sex affect variation in college GPA with the following model:\n$$ \\text{freshman GPA} = \\beta_0 + \\beta_1 \\text{SAT total} + \\beta_2 \\text{sex} + \\epsilon $$\nmodel_sat_sex \u0026lt;- lm(gpa_fy ~ sat_total + sex, data = sat_gpa) tidy(model_sat_sex, conf.int = TRUE) ## # A tibble: 3 x 7 ## term estimate std.error statistic p.value conf.low conf.high ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 (Intercept) -0.0269 0.149 -0.181 8.57e- 1 -0.319 0.265  ## 2 sat_total 0.0255 0.00145 17.6 1.14e-60 0.0227 0.0284 ## 3 sexMale -0.274 0.0414 -6.62 6.05e-11 -0.355 -0.193 Here, stuff gets interesting. The intercept ($\\beta_0$) is once again nonsensical—females with a 0 score on their SAT would have a -0.027 college GPA on average. There\u0026rsquo;s a positive effect with our \\(\\beta_1\\) (or sat_total), since controlling for sex, a one percentile point increase in SAT scores is associated with a 0.026 point increase in freshman GPA, on average. If we control for SAT scores, males see an average drop of 0.274 points ($\\beta_2$) in their college GPAs.\nThe combination of these two variables, however, doesn\u0026rsquo;t boost the model\u0026rsquo;s explanatory power that much. The adjusted \\(R^2\\) (which we must look at because we\u0026rsquo;re using more than one explanatory variable) is 0.243, meaning that the model explains a little over 24% of the variation in college GPAs.\nglance(model_sat_sex) ## # A tibble: 1 x 12 ## r.squared adj.r.squared sigma statistic p.value df logLik AIC BIC deviance df.residual nobs ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; ## 1 0.245 0.243 0.644 162. 1.44e-61 2 -978. 1964. 1983. 414. 997 1000 College GPA ~ SAT + high school GPA + sex Finally we can see what the effect of SAT scores, high school GPA, and sex is on college GPAs all at the same time, using the following model:\n$$ \\text{freshman GPA} = \\beta_0 + \\beta_1 \\text{SAT total} + \\beta_2 \\text{high school GPA} + \\beta_3 \\text{sex} + \\epsilon $$\nmodel_sat_hs_sex \u0026lt;- lm(gpa_fy ~ sat_total + gpa_hs + sex, data = sat_gpa) tidy(model_sat_hs_sex, conf.int = TRUE) ## # A tibble: 4 x 7 ## term estimate std.error statistic p.value conf.low conf.high ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 (Intercept) -0.836 0.148 -5.63 2.35e- 8 -1.13 -0.544  ## 2 sat_total 0.0158 0.00150 10.5 9.72e-25 0.0129 0.0188 ## 3 gpa_hs 0.545 0.0394 13.8 6.61e-40 0.467 0.622  ## 4 sexMale -0.143 0.0391 -3.66 2.66e- 4 -0.220 -0.0664 We can say the following things about these results:\n Yet again, the intercept ($\\beta_0$) can be safely ignored. Here it means that a female with a 0.0 high school GPA and an SAT score of 0 would have a college GPA of -0.84, on average. That\u0026rsquo;s pretty impossible. The \\(\\beta_1\\) coefficient for sat_total indicates that taking into account high school GPA and sex, a one percentile point increase in a student\u0026rsquo;s SAT score is associated with a 0.016 point increase in their college GPA, on average. Controlling for SAT scores and sex, a one point increase in high school GPA is associated with a 0.545 point (this is \\(\\beta_2\\)) increase in college GPA, on average. This coefficient is lower than the 0.74 point coefficient we found previously. That\u0026rsquo;s because SAT scores and sex soaked up some of high school GPA\u0026rsquo;s explanatory power. Taking SAT scores and high school GPAs into account, males have a 0.143 point lower GPA in college, on average (this is \\(\\beta_3\\))  As always, the adjusted \\(R^2\\) shows us how well the model fits overall (again, we have to look at the adjusted \\(R^2\\) because we have more than one explanatory variable). In this case, the model explains 36.5% of the variation in college GPA, which is higher than any of the previous models (but not phenomenal, in the end).\nglance(model_sat_hs_sex) ## # A tibble: 1 x 12 ## r.squared adj.r.squared sigma statistic p.value df logLik AIC BIC deviance df.residual nobs ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; ## 1 0.367 0.365 0.590 192. 2.67e-98 3 -890. 1790. 1815. 347. 996 1000 Which model best predicts freshman GPA? How do you know? As you\u0026rsquo;ve learned in previous stats classes, adjusted \\(R^2\\) generally shows the strength of a model\u0026rsquo;s fit, or how well the model will predict future values of the outcome variable. If we compare the adjusted \\(R^2\\) for each of the models, we see that the \u0026ldquo;best\u0026rdquo; model is the last one.\n# The modelsummary() function takes a bunch of different regression models and # puts them in a neat side-by-side table. In a normal report or analysis, you\u0026#39;d # include all of these once instead of going one by one like we did above. modelsummary(list(model_sat_gpa, model_sat_gpa_types, model_sat_gpa_hs, model_sat_sex, model_sat_hs_sex)) \r\r\rModel 1 \rModel 2 \rModel 3 \rModel 4 \rModel 5 \r\r\r\r\r(Intercept) \r0.002 \r0.007 \r0.091 \r-0.027 \r-0.836 \r\r\r\r(0.152) \r(0.152) \r(0.118) \r(0.149) \r(0.148) \r\r\rsat_total \r0.024 \r\r\r0.026 \r0.016 \r\r\r\r(0.001) \r\r\r(0.001) \r(0.002) \r\r\rsat_verbal \r\r0.025 \r\r\r\r\r\r\r\r(0.003) \r\r\r\r\r\rsat_math \r\r0.022 \r\r\r\r\r\r\r\r(0.003) \r\r\r\r\r\rgpa_hs \r\r\r0.743 \r\r0.545 \r\r\r\r\r\r(0.036) \r\r(0.039) \r\r\rsexMale \r\r\r\r-0.274 \r-0.143 \r\r\r\r\r\r\r(0.041) \r(0.039) \r\r\rNum.Obs. \r1000 \r1000 \r1000 \r1000 \r1000 \r\r\rR2 \r0.212 \r0.212 \r0.295 \r0.245 \r0.367 \r\r\rR2 Adj. \r0.211 \r0.211 \r0.295 \r0.243 \r0.365 \r\r\rAIC \r2004.8 \r2006.4 \r1893.0 \r1963.8 \r1790.2 \r\r\rBIC \r2019.5 \r2026.0 \r1907.7 \r1983.4 \r1814.8 \r\r\rLog.Lik. \r-999.382 \r-999.189 \r-943.477 \r-977.904 \r-890.108 \r\r\rF \r268.270 \r134.244 \r418.071 \r161.762 \r192.141 \r\r\r\r  This is real data about real students, compiled and cleaned by a professor at Dartmouth. \u0026#x21a9;\u0026#xfe0e;\n   ","date":1629072000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1629072000,"objectID":"1acd8542f65cf3cff8f2eec98b604361","permalink":"/example/02-practico/","publishdate":"2021-08-16T00:00:00Z","relpermalink":"/example/02-practico/","section":"example","summary":"If you want to follow along with this example, you can download the data below:\n  sat_gpa.csv  You can also download a complete .zip file with a finished R Markdown file that you can knit and play with on your own:","tags":null,"title":"Bienvenido/a a R, RStudio y tidyverse","type":"docs"},{"authors":null,"categories":null,"content":"Readings  The syllabus, content, examples, and assignments pages for this class  Chapter 1 in Impact Evaluation in Practice1  DJ Patil, “What Makes a Radical and Revolutionary Technology?”  (DJ Patil is the former Chief Data Scientist of the United States under President Obama. He gave this forum address at Brigham Young University on February 13, 2018.)    Stephen Goldsmith, “Next Generation of Public Employees Must Understand Data and Policy”  Hadley Wickham, “Data Science: How is it Different To Statistics?”  Slides The slides for today’s lesson are available online as an HTML file. Use the buttons below to open the slides either as an interactive website or as a static PDF (for printing or storing for later). You can also click in the slides below and navigate through them with your left and right arrow keys.\n View all slides in new window  Download PDF of all slides\nIntroduction\r\rData science and public service\r\rEvidence, evaluation, and causation (1)\r\rEvidence, evaluation, and causation (2)\r\rClass details\r\r\r\r\r\r\r\r\r\r\r\r\r\r\r\r\r\r\rFun fact: If you type ? (or shift + /) while going through the slides, you can see a list of special slide-specific commands.\r\rVideos Videos for each section of the lecture are available at this YouTube playlist.\n Introduction Data science and public service Evidence, evaluation, and causation (1) Evidence, evaluation, and causation (2) Class details  You can also watch the playlist (and skip around to different sections) here:\n\r\r  Paul J. Gertler et al., Impact Evaluation in Practice, 2nd ed. (Inter-American Development Bank; World Bank, 2016), https://openknowledge.worldbank.org/handle/10986/25030. \u0026#x21a9;\u0026#xfe0e;\n   ","date":1628467200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1628467200,"objectID":"24bdef858b9ebc83bb0134c283e06cf1","permalink":"/content/01-content/","publishdate":"2021-08-09T00:00:00Z","relpermalink":"/content/01-content/","section":"content","summary":"Readings  The syllabus, content, examples, and assignments pages for this class  Chapter 1 in Impact Evaluation in Practice1  DJ Patil, “What Makes a Radical and Revolutionary Technology?”  (DJ Patil is the former Chief Data Scientist of the United States under President Obama.","tags":null,"title":"Herramientas para el uso de R","type":"docs"},{"authors":null,"categories":null,"content":"Part 1: The basics of R and dplyr For this week\u0026rsquo;s problem set, you need to work through a few of RStudio\u0026rsquo;s introductory primers. You\u0026rsquo;ll do these in your browser and type code and see results there.\nYou\u0026rsquo;ll learn some of the basics of R, as well as some powerful methods for manipulating data with the dplyr package.\nComplete these primers. It seems like there are a lot, but they\u0026rsquo;re short and go fairly quickly (especially as you get the hang of the syntax). Also, I have no way of seeing what you do or what you get wrong or right, and that\u0026rsquo;s totally fine! If you get stuck and want to skip some (or if it gets too easy), go right ahead and skip them!\n The Basics  Visualization Basics Programming Basics   Work with Data  Working with Tibbles Isolating Data with dplyr Deriving Information with dplyr   Visualize Data  Exploratory Data Analysis Bar Charts Histograms Boxplots and Counts Scatterplots Line plots Overplotting and Big Data Customize Your Plots   Tidy Your Data  Reshape Data    Recent versions of tidyr have renamed these core functions: gather() is now pivot_longer() and spread() is now pivot_wider(). The syntax for these pivot_*() functions is slightly different from what it was in gather() and spread(), so you can\u0026rsquo;t just replace the names. Fortunately, both gather() and spread() still work and won\u0026rsquo;t go away for a while, so you can still use them as you learn about reshaping and tidying data. It would be worth learning how the newer pivot_*() functions work, eventually, though (see here for examples).\r\rThe content from these primers comes from the (free and online!) book R for Data Science by Garrett Grolemund and Hadley Wickham. I highly recommend the book as a reference and for continuing to learn and use R in the future.\nPart 2: Getting familiar with RStudio The RStudio primers you just worked through are a great introduction to writing and running R code, but you typically won\u0026rsquo;t type code in a browser when you work with R. Instead, you\u0026rsquo;ll use a nicer programming environment like RStudio, which lets you type and save code in scripts, run code from those scripts, and see the output of that code, all in the same program.\nTo get familiar with RStudio, watch this video (it\u0026rsquo;s from PMAP 8921, but the content still applies here):\n\r\rPart 3: RStudio Projects One of the most powerful and useful aspects of RStudio is its ability to manage projects.\nWhen you first open R, it is \u0026ldquo;pointed\u0026rdquo; at some folder on your computer, and anything you do will be relative to that folder. The technical term for this is a \u0026ldquo;working directory.\u0026rdquo;\nWhen you first open RStudio, look in the area right at the top of the Console pane to see your current working directory. Most likely you\u0026rsquo;ll see something cryptic: ~/\nThat tilde sign (~) is a shortcut that stands for your user directory. On Windows this is C:\\Users\\your_user_name\\; on macOS this is /Users/your_user_name/. With the working directory set to ~/, R is \u0026ldquo;pointed\u0026rdquo; at that folder, and anything you save will end up in that folder, and R will expect any data that you load to be there too.\nIt\u0026rsquo;s always best to point R at some other directory. If you don\u0026rsquo;t use RStudio, you need to manually set the working directory to where you want it with setwd(), and many R scripts in the wild include something like setwd(\u0026quot;C:\\\\Users\\\\bill\\\\Desktop\\\\Important research project\u0026quot;) at the beginning to change the directory. THIS IS BAD THOUGH (see here for an explanation). If you ever move that directory somewhere else, or run the script on a different computer, or share the project with someone, the path will be wrong and nothing will run and you will be sad.\nThe best way to deal with working directories with RStudio is to use RStudio Projects. These are special files that RStudio creates for you that end in a .Rproj extension. When you open one of these special files, a new RStudio instance will open up and be pointed at the correct directory automatically. If you move the folder later or open it on a different computer, it will work just fine and you will not be sad.\nRead this super short chapter on RStudio projects to learn how to create and use them\nIn general, you can create a new project by going to File \u0026gt; New Project \u0026gt; New Directory \u0026gt; Empty Project, which will create a new folder on your computer that is empty except for a single .Rproj file. Double click on that file to open an RStudio instance that is pointed at the correct folder.\nPart 4: Getting familiar with R Markdown To ensure that the analysis and graphics you make are reproducible, you\u0026rsquo;ll do the majority of your work in this class using R Markdown files.\nDo the following things:\n Watch this video:  \r\r  Skim through the content at these pages:\n Using Markdown Using R Markdown How it Works Code Chunks Inline Code Markdown Basics (The R Markdown Reference Guide is super useful here.) Output Formats    Watch this video (again, it\u0026rsquo;s from PMAP 8921, but the content works for this class):\n\r\r  ","date":1628467200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1628467200,"objectID":"1c54211c51c2690aaec09991c5ab4895","permalink":"/example/01-practico/","publishdate":"2021-08-09T00:00:00Z","relpermalink":"/example/01-practico/","section":"example","summary":"Part 1: The basics of R and dplyr For this week\u0026rsquo;s problem set, you need to work through a few of RStudio\u0026rsquo;s introductory primers. You\u0026rsquo;ll do these in your browser and type code and see results there.","tags":null,"title":"Bienvenido/a a R, RStudio y Github","type":"docs"},{"authors":null,"categories":null,"content":"Readings   Chapter 2 in Mastering ’Metrics1  Chapter 2, “Probability and Regression Review” in Causal Inference: The Mixtape (this is long; skim through this as a review)2  Recommended readings Look through your notes on regression from your last stats class. Also, you can skim through these resources:\n  6.1–6.4 in ModernDive3  7.1–7.4 in ModernDive4  7.1–7.3 in OpenIntro Statistics5  8.1 in OpenIntro Statistics6  We’ll review all this regression stuff in the videos, so don’t panic if this all looks terrifying! Also, take advantage of the videos that accompany the OpenIntro chapters. And also, the OpenIntro chapters are heavier on the math—don’t worry if you don’t understand everything.\nSlides The slides for today’s lesson are available online as an HTML file. Use the buttons below to open the slides either as an interactive website or as a static PDF (for printing or storing for later). You can also click in the slides below and navigate through them with your left and right arrow keys.\n View all slides in new window  Download PDF of all slides\nIntroduction\r\rDrawing lines\r\rLines, Greek, and regression\r\rNull worlds and statistical significance\r\r\r\r\r\r\r\r\r\r\r\r\r\r\r\rFun fact: If you type ? (or shift + /) while going through the slides, you can see a list of special slide-specific commands.\r\rVideos Videos for each section of the lecture are available at this YouTube playlist.\n Introduction Drawing lines Lines, Greek, and regression Null worlds and statistical significance  You can also watch the playlist (and skip around to different sections) here:\n\r\r  Joshua D. Angrist and Jörn-Steffen Pischke, Mastering ’Metrics: The Path from Cause to Effect (Princeton, NJ: Princeton University Press, 2015). \u0026#x21a9;\u0026#xfe0e;\n Scott Cunningham, Causal Inference: The Mixtape (New Haven, CT: Yale University Press, 2021), https://mixtape.scunning.com/. \u0026#x21a9;\u0026#xfe0e;\n Chester Ismay and Albert Y. Kim, ModernDive: An Introduction to Statistical and Data Sciences via R, 2018, https://moderndive.com/. \u0026#x21a9;\u0026#xfe0e;\n Ibid. \u0026#x21a9;\u0026#xfe0e;\n David M. Diez, Christopher D. Barr, and Mine Çetinkaya-Rundel, OpenIntro Statistics, 3rd ed., 2017, https://www.openintro.org/stat/textbook.php?stat_book=os. \u0026#x21a9;\u0026#xfe0e;\n Ibid. \u0026#x21a9;\u0026#xfe0e;\n   ","date":1628467200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1628467200,"objectID":"41e89a34ca2fdd432a89cf4ae93cfcc3","permalink":"/content/02-content/","publishdate":"2021-08-09T00:00:00Z","relpermalink":"/content/02-content/","section":"content","summary":"Readings   Chapter 2 in Mastering ’Metrics1  Chapter 2, “Probability and Regression Review” in Causal Inference: The Mixtape (this is long; skim through this as a review)2  Recommended readings Look through your notes on regression from your last stats class.","tags":null,"title":"Herramientas para el uso de R","type":"docs"},{"authors":null,"categories":null,"content":"Evaluation research is tricky and costly. If you begin an intervention or launch a study prematurely, you can waste time and money—and potentially lives.\nEven if you have a well designed program with an impeccable logic model and a perfect DAG, you might discover (too late!) that you forgot to collect some critical variables or realize that your identification strategy will not work.\nFrom a more cynical perspective, you might (unethically) engage in the practice of p-hacking—running all sorts of different model specifications until you find the results you want, and then claim in your report that you had intended to run that model all along.\nOne increasingly popular method for (1) ensuring that your data and methods work before launching a study or intervention, and (2) declaring and committing to your hypotheses and methods and models before analyzing your data is to pre-register your research or evaluation. A pre-registered study contains all the background work—an introduction, literature review, theory, hypotheses, and proposed analysis—but without the actual data. Authors post their expectations and hypotheses publicly so they can be held publicly accountable for any deviations from their proposed design.^[See the Center for Open Science\u0026rsquo;s directory of preregistrations, or AsPredicted list for examples of this in real life. Here\u0026rsquo;s one by me!]\nThe best preregistered studies use simulated data that has the same structure as the data that will be collected (i.e. same columns, sometimes the same correlations and relationships researchers expect to see in the collected data, etc.). Because there\u0026rsquo;s no data yet (or just fake data), you have more freedom when developing a preregistered study. You can experiment with different models, play with different approaches, manipulate data in different ways, and so on. If you realize that you need a new variable, or that you need to rearrange questions on a survey, or make any other kinds of changes, you can—you haven\u0026rsquo;t collected the data yet!\n(Additionally, using synthetic data is extremely useful if you\u0026rsquo;re working with proprietary or private data that you cannot make public. You can make a synthetic version of the real data instead; see this too.)\nOnce you finalize your plan and know all the data you need to collect, and once you\u0026rsquo;ve written out the different models you\u0026rsquo;ll run, all you have to do is collect the real data, plop it into your script (replacing the fake data you\u0026rsquo;d been using), and run the analysis script again to generate the actual, real results. In the results section, you get to either say \u0026ldquo;As predicted, we found…\u0026rdquo;, or \u0026ldquo;Contrary to expectations, we found that…\u0026rdquo;.\nFor your final project in this class, you will write a pre-registered analysis of a public or nonprofit social program that you\u0026rsquo;re interested in. You don\u0026rsquo;t need to worry about collecting data—you\u0026rsquo;ll create a synthetic dataset for your pre-analysis.\nYou will submit three things via iCollege:\n A PDF of your preregistered report (see the outline below for details of what this needs to contain). You should compile this with R Markdown. You might want to write the prose-heavy sections in a word processor like Word or Google Docs and copy/paste the text into your R Markdown document, since RStudio doesn\u0026rsquo;t have a nice spell checker or grammar checker. This should have no visible R code, warnings, or messages in it (set echo = FALSE at the beginning of your document before you knit). The same PDF as above, but with all the R code in it (set echo = TRUE at the beginning of your document and reknit the file). A CSV file of your fake data.  This project is due by 7:00 PM on Monday, December 14, 2020. No late work will be accepted.\nYou can either run the analysis in RStudio locally on your computer (highly recommended(!!), since you won\u0026rsquo;t have to worry about keeping all your work on RStudio\u0026rsquo;s servers), or use an RStudio.cloud project. You can make a copy of this RStudio.cloud project—it doesn\u0026rsquo;t have anything in it, but I have preinstalled all the packages we\u0026rsquo;ve used over the course of the semester, so you don\u0026rsquo;t have to.\n  Empty RStudio.cloud project  Empty RStudio project you can download and unzip on your computer (doesn\u0026rsquo;t include any packages, since you\u0026rsquo;re responsible for installing those)  Resources Most importantly, do not hesitate to work with classmates. You all must choose different programs, but you can work in groups of up to 4 people on your own projects. Also, absolutely do not hesitate to ask me questions! I\u0026rsquo;m here to help!\nYou might find this evaluation (and its proposal) of a truancy program in the Provo School District in Utah helpful as an example for the first half of this assignment (program overview, theory, implementation, threats to validity, and outcomes). The PSD evaluation doesn\u0026rsquo;t have DAGs or fancy econometrics models like RCTs, diff-in-diff, RDD, IVs, or anything like that, so you can\u0026rsquo;t use it as an example of that part, but these should provide a good template for the program-specific sections. This is longer than expected for this class. I provide suggested word counts in the outline below.\n  psd-proposal-2011  psd-final-report-2012  Suggested outline Here\u0026rsquo;s an outline of what you\u0026rsquo;ll need to do. You did lots of this work in your evaluation assignments. Please don\u0026rsquo;t just copy/paste those assignments as is into this final project—you\u0026rsquo;ll want to polish it up for this final report. You can download this as an RMarkdown file and change the text if you want. I\u0026rsquo;ve also included this as an RMarkdown file in the empty RStudio.cloud project.\n  final-project-template.Rmd   Introduction Describe the motivation for this evaluation, briefly describe the program to be evaluated, and explain why it matters for society. (≈150 words)\nProgram overview Provide in-depth background about the program. Include details about (1) when it was started, (2) why it was started, (3) what it was designed to address in society. If the program hasn\u0026rsquo;t started yet, explain why it\u0026rsquo;s under consideration. (≈300 words)\nProgram theory and implementation Program theory and impact theory graph Explain and explore the program\u0026rsquo;s underlying theory. Sometimes programs will explain why they exist in a mission statement, but often they don\u0026rsquo;t and you have to infer the theory from what the program looks like when implemented. What did the program designers plan on occurring? Was this theory based on existing research? If so, cite it. (≈300 words)\nInclude a simple impact theory graph showing the program\u0026rsquo;s basic activities and outcomes. Recall from class and your reading that this is focused primarily on the theory and mechanisms, not on the implementation of the program.\nLogic model Describe the program\u0026rsquo;s inputs, activities, outputs, and outcomes. Pay careful attention to how they are linked—remember that every input needs to flow into an activity and every output must flow out of an activity. (≈150 words)\nUse flowchart software to connect the inputs, activities, outputs, and outcomes and create a complete logic model. Include this as a figure.\nOutcome and causation Main outcome Select one of the program\u0026rsquo;s outcomes to evaluate. Explain why you\u0026rsquo;ve chosen this (is it the most important? easiest to measure? has the greatest impact on society?) (≈50 words)\nMeasurement Using the concept of the \u0026ldquo;ladder of abstraction\u0026rdquo; that we discussed in class (e.g. identifying a witch, measuring poverty, etc.), make a list of all the possible attributes of the outcome. Narrow this list down to 3-4 key attributes. Discuss how you decided to narrow the concepts and justify why you think these attributes capture the outcome. Then, for each of these attributes, answer these questions:\n Measurable definition: How would you specifically define this attribute? (i.e. if the attribute is \u0026ldquo;reduced crime\u0026rdquo;, define it as \u0026ldquo;The percent change in crime in a specific neighborhood during a certain time frame\u0026rdquo; or something similar) Ideal measurement: How would you measure this attribute in an ideal world? Feasible measurement: How would you measure this given reality and given limitations in budget, time, etc.? Measurement of program effect: How would to connect this measure to people in the program? How would you check to see if the program itself had an effect?  (≈150 words in this section)\nCausal theory Given your measurement approach, describe and draw a causal diagram (DAG) that shows how your program causes the outcome. Note that this is not the same thing as the logic model—you\u0026rsquo;ll likely have nodes in the DAG that aren\u0026rsquo;t related to the program at all (like socioeconomic status, gender, experience, or other factors). The logic model provides the framework for the actual implementation of your program and connects all the moving parts to the outcomes. The DAG is how you can prove causation with statistical approaches. (≈150 words)\nHypotheses Make predictions of your program\u0026rsquo;s effect. Declare what you think will happen. (≈50 words)\nData and methods Identification strategy How will you measure the actual program effect? Will you rely on an RCT? Differences-in-differences? Regression discontinuity? Instrumental variables? How does your approach account for selection bias and endogeneity? How does your approach isolate the causal effect of the program on the outcome?\nAlso briefly describe what kinds of threats to internal and external validity you face in your study.\n(≈300 words)\nData Given your measurement approach, limits on feasibility, and identification strategy, describe the data you will use. Will you rely on administrative data collected by a government agency or nonprofit? Will you collect your own data? If so, what variables will you measure, and how? Will you conduct a survey or rely on outside observers or do something else? What does this data look like? What variables does it (or should it) include?\n(≈100 words)\nSynthetic analysis Generate a synthetic (fake) dataset in R with all the variables you\u0026rsquo;ll need for the real life analysis. Analyze the data using your identification strategy. For instance:\n If you\u0026rsquo;re relying on observational data, close all the backdoors with matching or inverse probability weighting, don\u0026rsquo;t adjust for colliders, and make a strong argument for isolation of the causal effect in the absence of treatment/control groups If you\u0026rsquo;re doing an RCT, test the differences in means in the treatment and control groups (and follow all other best practices listed in the World Bank book, checking for balance across groups, etc.) If you\u0026rsquo;re doing diff-in-diff, run a regression model with an interaction term to show the diff-in-diff If you\u0026rsquo;re doing regression discontinuity, check for a jump in the outcome variable at the cutoff in the running variable If you\u0026rsquo;re using instrumental variables, check the validity of your instrument and run a 2SLS model  Include robustness checks to ensure the validity of your effect (i.e. if you\u0026rsquo;re doing regression discontinuity, test different bandwidths and kernel types; etc.)\n(As many words as you need to fully describe your analysis and results)\nConclusion What would the findings from this analysis mean for your selected program? What would it mean if you found an effect? What would it mean if you didn\u0026rsquo;t find an effect? Why does any of this matter? (≈75 words)\n","date":1620000000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1620000000,"objectID":"6a858731bc294a85e66753f61ab21233","permalink":"/assignment/final-project/","publishdate":"2021-05-03T00:00:00Z","relpermalink":"/assignment/final-project/","section":"assignment","summary":"Evaluation research is tricky and costly. If you begin an intervention or launch a study prematurely, you can waste time and money—and potentially lives.\nEven if you have a well designed program with an impeccable logic model and a perfect DAG, you might discover (too late!","tags":null,"title":"Final project","type":"docs"},{"authors":null,"categories":null,"content":"This assignment will give you practice generating synthetic data and building in causal effects.\nThese two examples will be incredibly helpful:\n Generating random numbers The ultimate guide to generating synthetic data for causal inference  You\u0026rsquo;ll be doing all your R work in R Markdown. You can download a zipped file of a pre-made project here:\n  problem-set-9.zip  And as always, if you\u0026rsquo;re struggling, please talk to me. Work with classmates too (especially for this assignment!). Don\u0026rsquo;t suffer in silence!\nInstructions   If you\u0026rsquo;re using R on your own computer, download this file, unzip it, and double click on the file named problem-set-9.Rproj:  problem-set-9.zip\nYou\u0026rsquo;ll need to make sure you have these packages installed on your computer: tidyverse, broom, ggdag, and scales. If you try to load one of those packages with library(tidyverse) or library(ggdag), etc., and R gives an error that the package is missing, use the \u0026ldquo;Packages\u0026rdquo; panel in RStudio to install it.\n(Alternatively, you can open the project named \u0026ldquo;Problem Set 9\u0026rdquo; on RStudio.cloud and complete the assignment in your browser without needing to install anything. This link should take you to the project—if it doesn\u0026rsquo;t, log in and look for the project named \u0026ldquo;Problem Set 9.\u0026quot;)\n  Rename the R Markdown file named your-name_problem-set-9.Rmd to something that matches your name and open it in RStudio.\n  Complete the tasks given in the R Markdown file. You can remove any of the question text if you want.\nYou can definitely copy, paste, and adapt from other code in the document or the example guide—don\u0026rsquo;t try to write everything from scratch!.\nYou\u0026rsquo;ll need to insert your own code chunks. Rather than typing them by hand (that\u0026rsquo;s tedious!), use the \u0026ldquo;Insert\u0026rdquo; button at the top of the editing window, or press ⌥ + ⌘ + I on macOS, or ctrl + alt + I on Windows.\nRemember that you can run an entire chunk by clicking on the green play arrow in the top right corner of the chunk. You can also run lines of code line-by-line if you place your cursor on some R code and press ⌘ + enter (for macOS users) or ctrl + enter (for Windows users).\nMake sure you run each chunk sequentially. If you run a chunk in the middle of the document without running previous ones, it might not work, since previous chunks might do things that later chunks depend on.\n  When you\u0026rsquo;re all done, click on the \u0026ldquo;Knit\u0026rdquo; button at the top of the editing window and create an HTML or Word version (or PDF if you\u0026rsquo;ve installed tinytex) of your document. Upload that file to iCollege.\n  ","date":1618790400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1618790400,"objectID":"6e2fa3adc24a54ff7a509ba8feddc0c2","permalink":"/assignment/09-problem-set/","publishdate":"2021-04-19T00:00:00Z","relpermalink":"/assignment/09-problem-set/","section":"assignment","summary":"This assignment will give you practice generating synthetic data and building in causal effects.\nThese two examples will be incredibly helpful:\n Generating random numbers The ultimate guide to generating synthetic data for causal inference  You\u0026rsquo;ll be doing all your R work in R Markdown.","tags":null,"title":"Problem set 9","type":"docs"},{"authors":null,"categories":null,"content":"This assignment is a review of all the causal inference methods we\u0026rsquo;ve learned this semester: RCTs, matching and inverse probability weighting, diff-in-diff, RDD, and IVs. Refer to your past assignments and all the examples for guidance:\n RCTs: Example + Problem Set 3 Matching and inverse probability weighting: Example + Problem Set 3 Diff-in-diff: Example + Problem Sets 4 and 5 RDD: Example + Problem Set 6 Instrumental variables: Example + Problem Set 7  You\u0026rsquo;ll be doing all your R work in R Markdown. You can download a zipped file of a pre-made project here:\n  problem-set-8.zip  And as always, if you\u0026rsquo;re struggling, please talk to me. Work with classmates too (especially for this assignment!). Don\u0026rsquo;t suffer in silence!\nInstructions   If you\u0026rsquo;re using R on your own computer, download this file, unzip it, and double click on the file named problem-set-8.Rproj:  problem-set-8.zip\nYou\u0026rsquo;ll need to make sure you have these packages installed on your computer: tidyverse, broom, estimatr, modelsummary, MatchIt, rdrobust, rddensity, and haven. If you try to load one of those packages with library(tidyverse) or library(MatchIt), etc., and R gives an error that the package is missing, use the \u0026ldquo;Packages\u0026rdquo; panel in RStudio to install it.\n(Alternatively, you can open the project named \u0026ldquo;Problem Set 8\u0026rdquo; on RStudio.cloud and complete the assignment in your browser without needing to install anything. This link should take you to the project—if it doesn\u0026rsquo;t, log in and look for the project named \u0026ldquo;Problem Set 8.\u0026quot;)\n  Rename the R Markdown file named your-name_problem-set-8.Rmd to something that matches your name and open it in RStudio.\n  Complete the tasks given in the R Markdown file. You can remove any of the question text if you want.\nYou can definitely copy, paste, and adapt from other code in the document or the different example pages—don\u0026rsquo;t try to write everything from scratch!.\nYou\u0026rsquo;ll need to insert your own code chunks. Rather than typing them by hand (that\u0026rsquo;s tedious!), use the \u0026ldquo;Insert\u0026rdquo; button at the top of the editing window, or press ⌥ + ⌘ + I on macOS, or ctrl + alt + I on Windows.\nRemember that you can run an entire chunk by clicking on the green play arrow in the top right corner of the chunk. You can also run lines of code line-by-line if you place your cursor on some R code and press ⌘ + enter (for macOS users) or ctrl + enter (for Windows users).\nMake sure you run each chunk sequentially. If you run a chunk in the middle of the document without running previous ones, it might not work, since previous chunks might do things that later chunks depend on.\n  When you\u0026rsquo;re all done, click on the \u0026ldquo;Knit\u0026rdquo; button at the top of the editing window and create an HTML or Word version (or PDF if you\u0026rsquo;ve installed tinytex) of your document. Upload that file to iCollege.\n  ","date":1618185600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1618185600,"objectID":"0af4ecfded51c97611249152a11adab5","permalink":"/assignment/08-problem-set/","publishdate":"2021-04-12T00:00:00Z","relpermalink":"/assignment/08-problem-set/","section":"assignment","summary":"This assignment is a review of all the causal inference methods we\u0026rsquo;ve learned this semester: RCTs, matching and inverse probability weighting, diff-in-diff, RDD, and IVs. Refer to your past assignments and all the examples for guidance:","tags":null,"title":"Problem set 8","type":"docs"},{"authors":null,"categories":null,"content":"For this problem set, you\u0026rsquo;ll practice using instrumental variables with both real and simulated data. This example page will be incredibly useful for you:\n Instrumental variables  You\u0026rsquo;ll be doing all your R work in R Markdown. You can download a zipped file of a pre-made project here:\n  problem-set-7.zip  And as always, if you\u0026rsquo;re struggling, please talk to me. Work with classmates too (especially for this assignment!). Don\u0026rsquo;t suffer in silence!\nInstructions   If you\u0026rsquo;re using R on your own computer, download this file, unzip it, and double click on the file named problem-set-7.Rproj:  problem-set-7.zip\nYou\u0026rsquo;ll need to make sure you have these packages installed on your computer: tidyverse, broom, estimatr, and modelsummary. If you try to load one of those packages with library(tidyverse) or library(estimatr), etc., and R gives an error that the package is missing, use the \u0026ldquo;Packages\u0026rdquo; panel in RStudio to install it.\n(Alternatively, you can open the project named \u0026ldquo;Problem Set 7\u0026rdquo; on RStudio.cloud and complete the assignment in your browser without needing to install anything. This link should take you to the project—if it doesn\u0026rsquo;t, log in and look for the project named \u0026ldquo;Problem Set 7.\u0026quot;)\n  Rename the R Markdown file named your-name_problem-set-7.Rmd to something that matches your name and open it in RStudio.\n  Complete the tasks given in the R Markdown file. There are questions scattered throughout the document—your job is to answer those questions. You don\u0026rsquo;t need to put your answers in bold or ALL CAPS or anything, and you can remove the question text if you want.\nFill out code in the empty chunks provided (you can definitely copy, paste, and adapt from other code in the document or the example page on instrumental variables—don\u0026rsquo;t try to write everything from scratch!).\nYou\u0026rsquo;ll need to insert your own code chunks. Rather than typing them by hand (that\u0026rsquo;s tedious!), use the \u0026ldquo;Insert\u0026rdquo; button at the top of the editing window, or press ⌥ + ⌘ + I on macOS, or ctrl + alt + I on Windows.\nRemember that you can run an entire chunk by clicking on the green play arrow in the top right corner of the chunk. You can also run lines of code line-by-line if you place your cursor on some R code and press ⌘ + enter (for macOS users) or ctrl + enter (for Windows users).\nMake sure you run each chunk sequentially. If you run a chunk in the middle of the document without running previous ones, it might not work, since previous chunks might do things that later chunks depend on.\n  When you\u0026rsquo;re all done, click on the \u0026ldquo;Knit\u0026rdquo; button at the top of the editing window and create an HTML or Word version (or PDF if you\u0026rsquo;ve installed tinytex) of your document. Upload that file to iCollege.\n  ","date":1617580800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1617580800,"objectID":"0db81f198f56082de1a6af871b0e2922","permalink":"/assignment/07-problem-set/","publishdate":"2021-04-05T00:00:00Z","relpermalink":"/assignment/07-problem-set/","section":"assignment","summary":"For this problem set, you\u0026rsquo;ll practice using instrumental variables with both real and simulated data. This example page will be incredibly useful for you:\n Instrumental variables  You\u0026rsquo;ll be doing all your R work in R Markdown.","tags":null,"title":"Problem set 7","type":"docs"},{"authors":null,"categories":null,"content":"For this problem set, you\u0026rsquo;ll practice doing regression discontinuity analysis with simulated data from a hypothetical program. This example page will be incredibly useful for you:\n Regression discontinuity  You\u0026rsquo;ll be doing all your R work in R Markdown. You can download a zipped file of a pre-made project here:\n  problem-set-6.zip  And as always, if you\u0026rsquo;re struggling, please talk to me. Work with classmates too (especially for this assignment!). Don\u0026rsquo;t suffer in silence!\nInstructions   If you\u0026rsquo;re using R on your own computer, download this file, unzip it, and double click on the file named problem-set-6.Rproj:  problem-set-6.zip\nYou\u0026rsquo;ll need to make sure you have these packages installed on your computer: tidyverse, broom, rdrobust, rddensity, and modelsummary. If you try to load one of those packages with library(tidyverse) or library(rdrobust), etc., and R gives an error that the package is missing, use the \u0026ldquo;Packages\u0026rdquo; panel in RStudio to install it.\n(Alternatively, you can open the project named \u0026ldquo;Problem Set 6\u0026rdquo; on RStudio.cloud and complete the assignment in your browser without needing to install anything. This link should take you to the project—if it doesn\u0026rsquo;t, log in and look for the project named \u0026ldquo;Problem Set 6.\u0026quot;)\n  Rename the R Markdown file named your-name_problem-set-6.Rmd to something that matches your name and open it in RStudio.\n  Complete the tasks given in the R Markdown file. There are questions marked in bold. Your job is to answer those questions. You don\u0026rsquo;t need to put your answers in bold or ALL CAPS or anything, and you can remove the question text if you want.\nFill out code in the empty chunks provided (you can definitely copy, paste, and adapt from other code in the document or the example page on regression discontinuity—don\u0026rsquo;t try to write everything from scratch!).\nYou\u0026rsquo;ll need to insert your own code chunks. Rather than typing them by hand (that\u0026rsquo;s tedious!), use the \u0026ldquo;Insert\u0026rdquo; button at the top of the editing window, or press ⌥ + ⌘ + I on macOS, or ctrl + alt + I on Windows.\nRemember that you can run an entire chunk by clicking on the green play arrow in the top right corner of the chunk. You can also run lines of code line-by-line if you place your cursor on some R code and press ⌘ + enter (for macOS users) or ctrl + enter (for Windows users).\nMake sure you run each chunk sequentially. If you run a chunk in the middle of the document without running previous ones, it might not work, since previous chunks might do things that later chunks depend on.\n  When you\u0026rsquo;re all done, click on the \u0026ldquo;Knit\u0026rdquo; button at the top of the editing window and create an HTML or Word version (or PDF if you\u0026rsquo;ve installed tinytex) of your document. Upload that file to iCollege.\n  ","date":1616976000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1616976000,"objectID":"e5e6f92df01daa4f8a074400ebb41bb3","permalink":"/assignment/06-problem-set/","publishdate":"2021-03-29T00:00:00Z","relpermalink":"/assignment/06-problem-set/","section":"assignment","summary":"For this problem set, you\u0026rsquo;ll practice doing regression discontinuity analysis with simulated data from a hypothetical program. This example page will be incredibly useful for you:\n Regression discontinuity  You\u0026rsquo;ll be doing all your R work in R Markdown.","tags":null,"title":"Problem set 6","type":"docs"},{"authors":null,"categories":null,"content":"You’ll likely run into some errors when knitting with a modelsummary() table. That’s because the current version of the modelsummary package on CRAN doesn’t include support for robust standard errors from feols() models. You can install the latest version of modelsummary that does have support for robust standard errors by doing this:\n Install the remotes package using the Packages panel in RStudio Run this in your console to install the latest version of modelsummary directly from GitHub: remotes::install_github('vincentarelbundock/modelsummary')  \rFor this problem set, you’ll reproduce the results from one of the papers that you looked at in your threats to validity assignment:\n Rafael Di Tella and Ernesto Schargrodsky, “Do Police Reduce Crime? Estimates Using the Allocation of Police Forces After a Terrorist Attack,” American Economic Review 94, no. 1 (March 2004): 115–133, doi:10.1257/000282804322970733.\n The full published paper is posted on iCollege under “Content \u0026gt; Validity assignment” (since you used the paper for that assignment). It’s not posted publicly here because of copyright reasons. The data comes from Di Tella and Schargrodsky’s data appendix available at their study’s AER webpage, and I’ve included it in the .zip file for the assignment.\nThis paper uses difference-in-differences to estimate the causal effect of increased policing on car thefts. Now that you know all about diff-in-diff, you can create their same results!\nThe answer key from Problem Set 4 (where you also did diff-in-diff) + this example page will be incredibly useful for you:\n Difference-in-differences  You’ll be doing all your R work in R Markdown. You can download a zipped file of a pre-made project here:\n  problem-set-5.zip  And as always, if you’re struggling, please talk to me. Work with classmates too (especially for this assignment!). Don’t suffer in silence!\nInstructions   If you’re using R on your own computer, download this file, unzip it, and double click on the file named problem-set-5.Rproj:  problem-set-5.zip\nYou’ll need to make sure you have these packages installed on your computer: tidyverse, haven, broom, fixest, and modelsummary. If you try to load one of those packages with library(tidyverse) or library(haven), etc., and R gives an error that the package is missing, use the “Packages” panel in RStudio to install it.\n(Alternatively, you can open the project named “Problem Set 5” on RStudio.cloud and complete the assignment in your browser without needing to install anything. This link should take you to the project—if it doesn’t, log in and look for the project named “Problem Set 5.”)\n  Rename the R Markdown file named your-name_problem-set-5.Rmd to something that matches your name and open it in RStudio.\n  Complete the tasks given in the R Markdown file. There are questions marked in bold. Your job is to answer those questions. You don’t need to put your answers in bold or ALL CAPS or anything, and you can remove the question text if you want.\nFill out code in the empty chunks provided (you can definitely copy, paste, and adapt from other code in the document or the example page on diff-in-diff—don’t try to write everything from scratch!).\nYou’ll need to insert your own code chunks. Rather than typing them by hand (that’s tedious!), use the “Insert” button at the top of the editing window, or press ⌥ + ⌘ + I on macOS, or ctrl + alt + I on Windows.\nRemember that you can run an entire chunk by clicking on the green play arrow in the top right corner of the chunk. You can also run lines of code line-by-line if you place your cursor on some R code and press ⌘ + enter (for macOS users) or ctrl + enter (for Windows users).\nMake sure you run each chunk sequentially. If you run a chunk in the middle of the document without running previous ones, it might not work, since previous chunks might do things that later chunks depend on.\n  When you’re all done, click on the “Knit” button at the top of the editing window and create an HTML or Word version (or PDF if you’ve installed tinytex) of your document. Upload that file to iCollege.\n  ","date":1616371200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1616371200,"objectID":"093010dc7720d9206bc9872c695a5fbc","permalink":"/assignment/05-problem-set/","publishdate":"2021-03-22T00:00:00Z","relpermalink":"/assignment/05-problem-set/","section":"assignment","summary":"You’ll likely run into some errors when knitting with a modelsummary() table. That’s because the current version of the modelsummary package on CRAN doesn’t include support for robust standard errors from feols() models.","tags":null,"title":"Problem set 5","type":"docs"},{"authors":null,"categories":null,"content":"IMPORTANT: This looks like a lot of work, but again, it\u0026rsquo;s mostly copying/pasting chunks of code and changing things.\r\rFor this problem set, you\u0026rsquo;ll practice running difference-in-differences analysis with R, both manually and with regression. This example will be incredibly useful for you:\n Difference-in-differences  You\u0026rsquo;ll be doing all your R work in R Markdown. You can download a zipped file of a pre-made project here:\n  problem-set-4.zip  And as always, if you\u0026rsquo;re struggling, please talk to me. Work with classmates too (especially for this assignment!). Don\u0026rsquo;t suffer in silence!\nInstructions   If you\u0026rsquo;re using R on your own computer, download this file, unzip it, and double click on the file named problem-set-4.Rproj:  problem-set-4.zip\nYou\u0026rsquo;ll need to make sure you have these packages installed on your computer: tidyverse, haven, and broom. If you try to load one of those packages with library(tidyverse) or library(haven), etc., and R gives an error that the package is missing, use the \u0026ldquo;Packages\u0026rdquo; panel in RStudio to install it.\n(Alternatively, you can open the project named \u0026ldquo;Problem Set 4\u0026rdquo; on RStudio.cloud and complete the assignment in your browser without needing to install anything. If you don\u0026rsquo;t have access to the class RStudio.cloud account, please let me know as soon as possible. This link should take you to the project—if it doesn\u0026rsquo;t, log in and look for the project named \u0026ldquo;Problem Set 4.\u0026quot;)\n  Rename the R Markdown file named your-name_problem-set-4.Rmd to something that matches your name and open it in RStudio.\n  Complete the tasks given in the R Markdown file. There are questions marked in bold (e.g. **What is the ATE?**). Your job is to answer those questions. You don\u0026rsquo;t need to put your answers in bold, and you can remove the question text if you want.\nFill out code in the empty chunks provided (you can definitely copy, paste, and adapt from other code in the document or the example page on diff-in-diff—don\u0026rsquo;t try to write everything from scratch!).\nYou\u0026rsquo;ll need to insert your own code chunks. Rather than typing them by hand (that\u0026rsquo;s tedious!), use the \u0026ldquo;Insert\u0026rdquo; button at the top of the editing window, or press ⌥ + ⌘ + I on macOS, or ctrl + alt + I on Windows.\nRemember that you can run an entire chunk by clicking on the green play arrow in the top right corner of the chunk. You can also run lines of code line-by-line if you place your cursor on some R code and press ⌘ + enter (for macOS users) or ctrl + enter (for Windows users).\nMake sure you run each chunk sequentially. If you run a chunk in the middle of the document without running previous ones, it might not work, since previous chunks might do things that later chunks depend on.\n  When you\u0026rsquo;re all done, click on the \u0026ldquo;Knit\u0026rdquo; button at the top of the editing window and create an HTML or Word version (or PDF if you\u0026rsquo;ve installed tinytex) of your document. Upload that file to iCollege.\n  ","date":1615161600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1615161600,"objectID":"7a468823ba74f580c841ee57a3b89a7f","permalink":"/assignment/04-problem-set/","publishdate":"2021-03-08T00:00:00Z","relpermalink":"/assignment/04-problem-set/","section":"assignment","summary":"IMPORTANT: This looks like a lot of work, but again, it\u0026rsquo;s mostly copying/pasting chunks of code and changing things.\r\rFor this problem set, you\u0026rsquo;ll practice running difference-in-differences analysis with R, both manually and with regression.","tags":null,"title":"Problem set 4","type":"docs"},{"authors":null,"categories":null,"content":"IMPORTANT: This looks like a lot of work, but again, it\u0026rsquo;s mostly copying/pasting chunks of code and changing things.\r\rFor this problem set, you\u0026rsquo;ll practice analyzing RCTs and working with matching and inverse probability weighting. These two examples will be incredibly useful for you:\n RCTs Matching and IPW  You\u0026rsquo;ll be doing all your R work in R Markdown this time (and from now on). You can download a zipped file of a pre-made project here:\n  problem-set-3.zip  And as always, if you\u0026rsquo;re struggling, please talk to me. Work with classmates too (especially for this assignment!). Don\u0026rsquo;t suffer in silence!\nInstructions   If you\u0026rsquo;re using R on your own computer, download this file, unzip it, and double click on the file named problem-set-3.Rproj:  problem-set-3.zip\nYou\u0026rsquo;ll need to make sure you have these packages installed on your computer: tidyverse, MatchIt, modelsummary, and patchwork. If you try to load one of those packages with library(tidyverse) or library(MatchIt), etc., and R gives an error that the package is missing, use the \u0026ldquo;Packages\u0026rdquo; panel in RStudio to install it.\n(Alternatively, you can open the project named \u0026ldquo;Problem Set 3\u0026rdquo; on RStudio.cloud and complete the assignment in your browser without needing to install anything. If you don\u0026rsquo;t have access to the class RStudio.cloud account, please let me know as soon as possible. This link should take you to the project—if it doesn\u0026rsquo;t, log in and look for the project named \u0026ldquo;Problem Set 3.\u0026quot;)\n  Rename the R Markdown file named your-name_problem-set-3.Rmd to something that matches your name and open it in RStudio.\n  Complete the tasks given in the R Markdown file. There are questions marked in bold (e.g. **What is the ATE?**). Your job is to answer those questions. You don\u0026rsquo;t need to put your answers in bold, and you can remove the question text if you want.\nFill out code in the empty chunks provided (you can definitely copy, paste, and adapt from other code in the document or the example page on RCTs and the example page on matching and IPW—don\u0026rsquo;t try to write everything from scratch!).\nYou\u0026rsquo;ll need to insert your own code chunks. Rather than typing them by hand (that\u0026rsquo;s tedious!), use the \u0026ldquo;Insert\u0026rdquo; button at the top of the editing window, or press ⌥ + ⌘ + I on macOS, or ctrl + alt + I on Windows.\nRemember that you can run an entire chunk by clicking on the green play arrow in the top right corner of the chunk. You can also run lines of code line-by-line if you place your cursor on some R code and press ⌘ + enter (for macOS users) or ctrl + enter (for Windows users).\nMake sure you run each chunk sequentially. If you run a chunk in the middle of the document without running previous ones, it might not work, since previous chunks might do things that later chunks depend on.\n  When you\u0026rsquo;re all done, click on the \u0026ldquo;Knit\u0026rdquo; button at the top of the editing window and create an HTML or Word version (or PDF if you\u0026rsquo;ve installed tinytex) of your document. Upload that file to iCollege.\n  ","date":1614556800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1614556800,"objectID":"203dd31d9d8464141627e62e5903ffee","permalink":"/assignment/03-problem-set/","publishdate":"2021-03-01T00:00:00Z","relpermalink":"/assignment/03-problem-set/","section":"assignment","summary":"IMPORTANT: This looks like a lot of work, but again, it\u0026rsquo;s mostly copying/pasting chunks of code and changing things.\r\rFor this problem set, you\u0026rsquo;ll practice analyzing RCTs and working with matching and inverse probability weighting.","tags":null,"title":"Problem set 3","type":"docs"},{"authors":null,"categories":null,"content":"Instructions You need to complete the “Assessing validity” section below. Ideally you should type this in R Markdown and knit your document to HTML or Word or PDF, but you can also write in Word if you want (though your final project will need to be in R Markdown, and this would give you practice).\nI’ve created an R Markdown template you can use here:  threats-validity.zip. It’s also available on RStudio.cloud.\nSubmit this assignment as a PDF or Word file on iCollege.\n Internal validity scores One helpful way to assess an evaluation’s internal validity is to systematically go through each possible threat and evaluate if the research design addresses it. For each of the 13 types of internal validity that we discussed in class, assign 1 point if the study addresses it and 0 points if it fails to do so. Add up the total points and assign the study a final internal validity score.\nNot all types of validity will apply to every study. Testing, for example, is only an issue if there is a pre-test and it involves a skill that could feasibly be enhanced by practicing the test. If a threat doesn’t apply, don’t give it a score.\n Omitted variable bias  Selection Attrition   Trends  Maturation Secular trends Seasonality Testing Regression to the mean   Study calibration  Measurement error Time frame of study   Contamination  Hawthorne effects John Henry effects Spillovers Intervening events     Assessing validity Your textbook Impact Evaluation in Practice is full of short examples of real-world evaluations, experiments, and studies. For this assignment, you will assess the (1) internal validity, (2) external validity, and (3) construct validity for two of the examples from the book. If the summary in the book isn’t sufficient, you can skim through the original study for more details. The original studies can be found in the Content section of iCollege.\nYou will assess two (2) of these four studies. Pick whichever two seem the most interesting to you:\n The effect of conditional cash transfers on education in Mexico (Box 4.2, p. 70)  Original study: T. Paul Schultz, “School Subsidies for the Poor: Evaluating the Mexican Progresa Poverty Program,” Journal of Development Economics 74, no. 1 (June 2004): 199–250, doi:10.1016/j.jdeveco.2003.12.009.   The impact of Sesame Street on school readiness (Box 5.1, p. 91)  Original study: Melissa S. Kearney and Phillip B. Levine, Early Childhood Education by MOOC: Lessons from Sesame Street, Working Paper Series (National Bureau of Economic Research, June 2015), doi:10.3386/w21229.   The effects of police deployment on crime in Argentina (Box 7.2, p. 135)  Original study: Rafael Di Tella and Ernesto Schargrodsky, “Do Police Reduce Crime? Estimates Using the Allocation of Police Forces After a Terrorist Attack,” American Economic Review 94, no. 1 (March 2004): 115–133, doi:10.1257/000282804322970733.   Early childhood development and migration in Jamaica (Box 9.5, p. 170)  Original study: Paul Gertler et al., “Labor Market Returns to an Early Childhood Stimulation Intervention in Jamaica,” Science 344, no. 6187 (May 14, 2014): 998–1001, doi:10.1126/science.1251178.    For each of the two studies you choose, do the following:\n Go through the 13 types of internal validity and describe in 2–3 sentences how the study succeeds/fails to address each concern. Calculate a total internal validity score. Describe any threats to external validity and assess how generalizable the findings are. (≈75 words) Describe any threats to construct validity and assess if the researchers are measuring the thing they intended to measure. (≈75 words)  ","date":1613952000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1613952000,"objectID":"5eca3857bb9e7900d4edd4f273f442a7","permalink":"/assignment/04-eval-threats/","publishdate":"2021-02-22T00:00:00Z","relpermalink":"/assignment/04-eval-threats/","section":"assignment","summary":"Instructions You need to complete the “Assessing validity” section below. Ideally you should type this in R Markdown and knit your document to HTML or Word or PDF, but you can also write in Word if you want (though your final project will need to be in R Markdown, and this would give you practice).","tags":null,"title":"Threats to validity","type":"docs"},{"authors":null,"categories":null,"content":"For your final project, you will conduct an evaluation for a social program of your choosing. In this assignment, you will decide how to model the causal effect of your program on your primary outcome.\nIf you decide to use a different program for your final project, that\u0026rsquo;s okay! This assignment doesn\u0026rsquo;t have to be related to your final program, but it would be extraordinarily helpful—a more polished version of this assignment can be included as part of your final project.\nInstructions You need to complete the three sections listed below. Ideally you should type this in R Markdown and knit your document to HTML or Word or PDF, but you can also write in Word if you want (though your final project will need to be in R Markdown, and this would give you practice).\nI\u0026rsquo;ve created an R Markdown template you can use here:  causal-model.zip. It\u0026rsquo;s also available on RStudio.cloud.\nSubmit this assignment as a PDF or Word file on iCollege.\n 1: DAG I Find a news article that makes a causal claim and interpret that claim by drawing an appropriate diagram. The article likely won\u0026rsquo;t explain all the things the researchers controlled for, so you\u0026rsquo;ll need to create an ideal DAG. What should be included in the causal process to measure the effect of X on Y?\nExport the figure from dagitty and include it in your assignment, or use this code to draw the DAG with R:\nlibrary(tidyverse) library(ggdag) # Remember that you can change the variable names here--they can be basically # anything, but cannot include spaces. The labels can have spaces. Adjust the # variable names (y, x2, etc) and labels (\u0026#34;Outcome\u0026#34;, \u0026#34;Something\u0026#34;, etc.) as # necessary. my_dag \u0026lt;- dagify(y ~ x1 + x2 + z, z ~ x1, x2 ~ x1 + z, labels = c(\u0026#34;y\u0026#34; = \u0026#34;Outcome\u0026#34;, \u0026#34;x1\u0026#34; = \u0026#34;Something\u0026#34;, \u0026#34;x2\u0026#34; = \u0026#34;Something else\u0026#34;, \u0026#34;z\u0026#34; = \u0026#34;Yet another thing\u0026#34;), exposure = \u0026#34;z\u0026#34;, outcome = \u0026#34;y\u0026#34;) # If you set text = TRUE, you\u0026#39;ll see the variable names in the DAG points # The `seed` argument makes it so that the random layout is the same every time ggdag(my_dag, text = FALSE, use_labels = \u0026#34;label\u0026#34;, seed = 1234) + theme_dag() # If you want the treatment and outcomes colored differently,  # replace ggdag() with ggdag_status() ggdag_status(my_dag, text = FALSE, use_labels = \u0026#34;label\u0026#34;, seed = 1234) + theme_dag() + theme(legend.position = \u0026#34;bottom\u0026#34;) # Move legend to bottom for fun Summarize the causal claim. Describe what the authors controlled for and what else you included in the DAG. Justify the inclusion of each node (point) and connection (line) in the graph. (≈150 words)\nIdentify all the frontdoor and backdoor paths between your exposure and outcome. What variables need to be controlled for / adjusted to close the backdoors? Did this happen in the study or article? (≈100 words)\n2: DAG II Find a different news article with a causal claim and do the same thing as above.\nDraw and include a DAG.\nSummarize the causal claim. Describe what the authors controlled for and what else you included in the DAG. Justify the inclusion of each node (point) and connection (line) in the graph. (≈150 words)\nIdentify all the frontdoor and backdoor paths between your exposure and outcome. What variables need to be controlled for / adjusted to close the backdoors? Did this happen in the study or article? (≈100 words)\n3: DAG for your program Identify the outcome you care most about from your final project program. Draw a DAG that shows the causal effect of your program\u0026rsquo;s intervention on the outcome.\nSummarize the causal claim. Describe what needs to be controlled for and what else you included in the DAG. Justify the inclusion of each node (point) and connection (line) in the graph. (≈150 words)\nIdentify all the frontdoor and backdoor paths between your exposure and outcome. What variables need to be controlled for / adjusted to close the backdoors? How might you do this with your evaluation? (≈100 words)\n","date":1613347200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1613347200,"objectID":"61b9fae5f127a307513cb4bba81a9c9e","permalink":"/assignment/03-eval-dag/","publishdate":"2021-02-15T00:00:00Z","relpermalink":"/assignment/03-eval-dag/","section":"assignment","summary":"For your final project, you will conduct an evaluation for a social program of your choosing. In this assignment, you will decide how to model the causal effect of your program on your primary outcome.","tags":null,"title":"Causal model","type":"docs"},{"authors":null,"categories":null,"content":"For your final project, you will conduct an evaluation for a social program of your choosing. In this assignment, you will decide how to best measure two of the program\u0026rsquo;s outcomes.\nIf you decide to use a different program for your final project, that\u0026rsquo;s okay! This assignment doesn\u0026rsquo;t have to be related to your final program, but it would be extraordinarily helpful—a more polished version of this assignment can be included as part of your final project.\nInstructions You need to complete the two sections listed below. Ideally you should type this in R Markdown and knit your document to HTML or Word or PDF, but you can also write in Word if you want (though your final project will need to be in R Markdown, and this would give you practice).^[And if you want to be super brave, try using R Markdown\u0026rsquo;s citation system!]\nI\u0026rsquo;ve created an R Markdown template you can use here:  measurement.zip. It\u0026rsquo;s also available on RStudio.cloud.\nSubmit this assignment as a PDF or Word file on iCollege.\n Assignment outline 1: Measurement and abstraction for full-day kindergarten Read this article about half-day vs. full-day kindergarten in Utah. The article is 10 years old, and half-day kindergarten still remains standard practice in most Utah school districts.\nPretend you are the administrator of the Optional Extended Day Kindergarten initiative. Based on the Salt Lake Tribune article (which provides hints throughout, and especially in one of the final paragraphs), and based on your own knowledge of educational outcomes, make a list of two (2) possible outcomes of the full-day kindergarten program.\nThen, for each of those two outcomes, do the following: Using the concept of the \u0026ldquo;ladder of abstraction\u0026rdquo; that we discussed in class (e.g. identifying a witch, measuring poverty, etc.), make a list of all the possible attributes of the outcome. Narrow this list down to 3–4 key attributes. Discuss how you decided to narrow the concepts and justify why you think these attributes capture the outcome. (≈100 words)\nThen, for each of those attributes, answer these questions:\n Measurable definition: How would you specifically define this attribute? (i.e. if the attribute is \u0026ldquo;reduced crime\u0026rdquo;, define it as \u0026ldquo;The percent change in crime in a specific neighborhood during a certain time frame\u0026rdquo; or something similar) Ideal measurement: How would you measure this attribute in an ideal world? Feasible measurement: How would you measure this given reality and given limitations in budget, time, etc.? Measurement of program effect: How would to connect this measure to people in the program? How would you check to see if the program itself had an effect?  2: Measurement and abstraction for your program Make a list of two possible outcomes of your selected program. For each of those outcomes, make a list of all the possible attributes. Narrow this list down to 3–4 key attributes. Discuss how you decided to narrow the concepts and justify why you think these attributes capture the outcome. (≈100 words)\nThen, for each of those attributes, answer these questions:\n Measurable definition: How would you specifically define this attribute? (i.e. if the attribute is \u0026ldquo;reduced crime\u0026rdquo;, define it as \u0026ldquo;The percent change in crime in a specific neighborhood during a certain time frame\u0026rdquo; or something similar) Ideal measurement: How would you measure this attribute in an ideal world? Feasible measurement: How would you measure this given reality and given limitations in budget, time, etc.? Measurement of program effect: How would to connect this measure to people in the program? How would you check to see if the program itself had an effect?  ","date":1612742400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1612742400,"objectID":"8a8453dc4ee2c97ff1ef34774efe4076","permalink":"/assignment/02-eval-measurement/","publishdate":"2021-02-08T00:00:00Z","relpermalink":"/assignment/02-eval-measurement/","section":"assignment","summary":"For your final project, you will conduct an evaluation for a social program of your choosing. In this assignment, you will decide how to best measure two of the program\u0026rsquo;s outcomes.","tags":null,"title":"Measurement","type":"docs"},{"authors":null,"categories":null,"content":"For your final project, you will conduct an evaluation for a social program of your choosing. In this assignment, you will explore the program\u0026rsquo;s background, history, purpose, and theory.\nIf you decide to use a different program for your final project, that\u0026rsquo;s okay! This assignment doesn\u0026rsquo;t have to be related to your final program, but it would be helpful—a more polished version of this assignment can be included as part of your final project.\nInstructions You need to complete the four sections listed below. Ideally you should type this in R Markdown and knit your document to HTML or Word or PDF, but you can also write in Word if you want (though your final project will need to be in R Markdown, and this would give you practice).^[And if you want to be super brave, try using R Markdown\u0026rsquo;s citation system!]\nI\u0026rsquo;ve created an R Markdown template you can use here:  background-theory.zip. It\u0026rsquo;s also available on RStudio.cloud.\nYou can draw your impact theory and logic model charts by hand or with something like Diagrams.net, Lucidchart, or Creately. Export the image as a PNG, place it in the same directory as your R Markdown file, and include the image with Markdown.\nThe syntax for adding an image in Markdown is fairly simple. Importantly, it is not R code, so don\u0026rsquo;t try putting it in an R chunk. Just type this:\n![Image caption](/path/to/image.png) Submit this assignment as a PDF or Word file on iCollege.\n Assignment outline 1: Program background and purpose (≈350 words)\nProvide in-depth background about the program. Include details about (1) when it was started, (2) why it was started, (3) what it was designed to address in society. If the program hasn’t started yet, explain why it’s under consideration. Make sure you cite your sources appropriately! (In the past, some students have just copied/pasted text from a program\u0026rsquo;s website; don\u0026rsquo;t do that! Describe and analyze the program\u0026rsquo;s background!)\n2: Program theory (≈400 words)\nExplain and explore the program’s underlying theory. Sometimes programs will explain why they exist in a mission statement, but often they don’t and you have to infer the theory from what the program looks like when implemented. What did the program designers plan on occurring? Was this theory based on existing research? If so, cite it.\nInclude a simple impact theory graph showing the program’s basic activities and outcomes. Recall from class and your reading that this is focused primarily on the theory and mechanisms, not on the implementation of the program.\n3: Logic model List every possible input, activity, output, and outcome for the program and provide a brief 1–2 sentence description of each.\nInputs  Something Something else  Activities  Something Something else  Outputs  Something Something else  Outcomes  Something Something else  Diagram Use flowchart software to connect the inputs, activities, outputs, and outcomes and create a complete logic model. Remember that inputs will always feed into activities, and that activities always produce outputs (that\u0026rsquo;s the whole purpose of an activity: convert an input to an output). Include this as a figure.\n4: Analysis (≈150 words)\nEvaluate how well the logic model relates to the program theory. Do the inputs, activities, and outputs have a logical, well-grounded connection to the intended outcomes? Under ideal conditions, would the components of the program lead to changes or lasting effects?\n","date":1612137600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1612137600,"objectID":"e5d0a313d5edac0ca31d89b7b0b851c1","permalink":"/assignment/01-eval-background-theory/","publishdate":"2021-02-01T00:00:00Z","relpermalink":"/assignment/01-eval-background-theory/","section":"assignment","summary":"For your final project, you will conduct an evaluation for a social program of your choosing. In this assignment, you will explore the program\u0026rsquo;s background, history, purpose, and theory.\nIf you decide to use a different program for your final project, that\u0026rsquo;s okay!","tags":null,"title":"Background and theory","type":"docs"},{"authors":null,"categories":null,"content":"IMPORTANT: This looks like a lot of work, but it\u0026rsquo;s mostly copying/pasting chunks of code and changing things.\r\rGetting started For this problem set, you\u0026rsquo;ll practice running and interpreting regression models using data about penguins in Antarctica and data on food access and mortality in the US.\nYou\u0026rsquo;ll be doing all your R work in R Markdown this time (and from now on). You should use an RStudio Project to keep your files well organized (either on your computer or on RStudio.cloud). Either create a new project for this exercise only, or make a project for all your work in this class.\nYou\u0026rsquo;ll need to download these two CSV files and put them somewhere on your computer or upload them to RStudio.cloud—preferably in a folder named data in your project folder:\n  penguins.csv  food_health_politics.csv  You\u0026rsquo;ll also need to download this R Markdown file with a template for this problem set. Download that here and include it in your project:\n  problem-set-2.Rmd  In the end, the structure of your project directory should look something like this:\nyour-project-name\\ your-name_problem-set-2.Rmd your-project-name.Rproj data\\ penguins.csv food_health_politics.csv To check that you put everything in the right places, you can download and unzip this file, which contains everything in the correct structure:\n  problem-set-2.zip  You\u0026rsquo;ll need to make sure you have these packages installed on your computer: tidyverse and modelsummary. If you try to load one of those packages with library(tidyverse) or library(modelsummary), etc., and R gives an error that the package is missing, use the \u0026ldquo;Packages\u0026rdquo; panel in RStudio to install it.\n(Alternatively, you can open the project named \u0026ldquo;Problem Set 2\u0026rdquo; on RStudio.cloud and complete the assignment in your browser without needing to install anything. If you don\u0026rsquo;t have access to the class RStudio.cloud account, please let me know as soon as possible. This link should take you to the project—if it doesn\u0026rsquo;t, log in and look for the project named \u0026ldquo;Problem Set 2.\u0026quot;)\nRemember that you can run an entire chunk by clicking on the green play arrow in the top right corner of the chunk. You can also run lines of code line-by-line if you place your cursor on some R code and press ⌘ + enter (for macOS users) or ctrl + enter (for Windows users).\nMake sure you run each chunk sequentially. If you run a chunk in the middle of the document without running previous ones, it might not work, since previous chunks might do things that later chunks depend on.\nRemember, if you\u0026rsquo;re struggling, please talk to me. Work with classmates too. Don\u0026rsquo;t suffer in silence!\nInstructions For this problem set, we\u0026rsquo;re less interested in causal relationships and more interested in the mechanics of manipulating data and running regressions in R. We\u0026rsquo;ll start caring about identification and causal models in the next problem set. Because of this, don\u0026rsquo;t put too much causal weight into the interpretations of these different models—this is an actual case of correlation not implying causation.\nThe example for week 2 on regression will be incredibly helpful for this exercise. Reference it. Copy and paste from it liberally.\n  Rename the R Markdown file named your-name_problem-set-2.Rmd to something that matches your name and open it in RStudio.\n  Complete the tasks given in the R Markdown file. Fill out code in the empty chunks provided (you can definitely copy, paste, and adapt from other code in the document or from the regression example—don\u0026rsquo;t try to write everything from scratch!), and replace text in ALL CAPS with your own. (i.e. You\u0026rsquo;ll see a bunch of TYPE YOUR ANSWER HEREs. Type your answers there.). Again, you don\u0026rsquo;t need to type your answers in all caps.\n  Turning everything in When you\u0026rsquo;re all done, click on the \u0026ldquo;Knit\u0026rdquo; button at the top of the editing window and create an HTML or Word version (or PDF if you\u0026rsquo;ve installed tinytex) of your document. Upload that file to iCollege.\n","date":1611532800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1611532800,"objectID":"a2a382eafe3563877adff51ce461e03b","permalink":"/assignment/02-problem-set/","publishdate":"2021-01-25T00:00:00Z","relpermalink":"/assignment/02-problem-set/","section":"assignment","summary":"IMPORTANT: This looks like a lot of work, but it\u0026rsquo;s mostly copying/pasting chunks of code and changing things.\r\rGetting started For this problem set, you\u0026rsquo;ll practice running and interpreting regression models using data about penguins in Antarctica and data on food access and mortality in the US.","tags":null,"title":"Problem set 2","type":"docs"},{"authors":null,"categories":null,"content":"Task 1: Introduce yourself to R, RStudio, and the tidyverse   Go the the example page for this week, \u0026ldquo;Welcome to R, RStudio, and the tidyverse\u0026rdquo;, and work through the different primers and videos in the four parts of the page.\nIt seems like there\u0026rsquo;s a lot on the page, but they\u0026rsquo;re short and go fairly quickly (especially as you get the hang of the syntax). Also, I have no way of seeing what you do or what you get wrong or right, and that\u0026rsquo;s totally fine! If you get stuck and want to skip some (or if it gets too easy), go right ahead and skip them!\n  Task 2: Make an RStudio Project   Use either RStudio.cloud or RStudio on your computer (preferably RStudio on your computer! Follow these instructions to get started!) to create a new RStudio Project. Refer to the example page you read in Task 1 for instructions\n  Create a folder named \u0026ldquo;data\u0026rdquo; in the project folder you just made.\n  Download this CSV file and place it in that folder:\n  cars.csv    In RStudio, go to \u0026ldquo;File\u0026rdquo; \u0026gt; \u0026ldquo;New File…\u0026rdquo; \u0026gt; \u0026ldquo;R Markdown…\u0026rdquo; and click \u0026ldquo;OK\u0026rdquo; in the dialog without changing anything.\n  Delete all the placeholder text in that new file and replace it with this:\n--- title: \u0026#34;Problem set 1\u0026#34; author: \u0026#34;Put your name here\u0026#34; output: html_document --- ```{r load-libraries-data, warning=FALSE, message=FALSE} library(tidyverse) cars \u0026lt;- read_csv(\u0026#34;data/cars.csv\u0026#34;) ``` # Learning R Tell me that you worked through the primers and videos and examples at the example page for this week: WRITE SOMETHING HERE LIKE \u0026#34;I did all the primers and had the time of my life!\u0026#34; or whatever. # My first plots Insert a chunk below and use it to create a scatterplot (hint: `geom_point()`) with diplacement (`displ`) on the x-axis, city MPG (`cty`) on the y-axis, and with the points colored by drive (`drv`). PUT CHUNK HERE Insert a chunk below and use it to create a histogram (hint: `geom_histogram()`) with highway MPG (`hwy`) on the x-axis. Do not include anything on the y-axis (`geom_histogram()` will do that automatically for you). Choose an appropriate bin width. If you\u0026#39;re brave, facet by drive (`drv`). PUT CHUNK HERE # My first data manipulation Insert a chunk below and use it to calculate the average city MPG (`cty`) by class of car (`class`). This won\u0026#39;t be a plot---it\u0026#39;ll be a table. Hint: use a combination of `group_by()` and `summarize()`. PUT CHUNK HERE   Save the R Markdown file with some sort of name (without any spaces!)\n  Your project folder should look something like this:\n  Task 3: Work with R   Remove the text that says \u0026ldquo;PUT CHUNK HERE\u0026rdquo; and insert a new R code chunk. Either type ctrl + alt + i on Windows, or ⌘ + ⌥ + i on macOS, or use the \u0026ldquo;Insert Chunk\u0026rdquo; menu:\n  Follow the instructions for the three chunks of code.\n  Knit your document as a Word file (or PDF if you\u0026rsquo;re brave and installed LaTeX). Use the \u0026ldquo;Knit\u0026rdquo; menu:\n  Upload the knitted document to iCollege.\n  🎉 Party! 🎉\n  You\u0026rsquo;ll be doing this same process for all your future problem sets. Each problem set will involve an R Markdown file. You can either create a new RStudio Project directory for all your work:\nOr you can create individual projects for each assignment and project:\n\r","date":1611014400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1611014400,"objectID":"870e7b8a7a3cee10fba901cb0f845c00","permalink":"/assignment/01-problem-set/","publishdate":"2021-01-19T00:00:00Z","relpermalink":"/assignment/01-problem-set/","section":"assignment","summary":"Task 1: Introduce yourself to R, RStudio, and the tidyverse   Go the the example page for this week, \u0026ldquo;Welcome to R, RStudio, and the tidyverse\u0026rdquo;, and work through the different primers and videos in the four parts of the page.","tags":null,"title":"Problem set 1","type":"docs"},{"authors":null,"categories":null,"content":"Markdown is a special kind of markup language that lets you format text with simple syntax. You can then use a converter program like pandoc to convert Markdown into whatever format you want: HTML, PDF, Word, PowerPoint, etc. (see the full list of output types here)\nBasic Markdown formatting \r\r\r\rType…\r…or…\r…to get\r\r\r\rSome text in a paragraph.\nMore text in the next paragraph. Always\ruse empty lines between paragraphs.\r\rSome text in a paragraph.\nMore text in the next paragraph. Always\ruse empty lines between paragraphs.\n\r\r*Italic*\r_Italic_\rItalic\r\r**Bold**\r__Bold__\rBold\r\r# Heading 1\r\rHeading 1\r\r\r## Heading 2\r\rHeading 2\r\r\r### Heading 3\r\rHeading 3\r\r\r(Go up to heading level 6 with ######)\r\r\r\r[Link text](http://www.example.com)\r\rLink text\r\r![Image caption](/path/to/image.png)\r\r\r\r`Inline code` with backticks\r\rInline code with backticks\r\r\u0026gt; Blockquote\r\r\rBlockquote\n\r\r- Things in\r- an unordered\r- list\r* Things in\r* an unordered\r* list\r\rThings in\ran unordered\rlist\r\r\r1. Things in\r2. an ordered\r3. list\r1) Things in\r2) an ordered\r3) list\rThings in\ran ordered\rlist\r\r\rHorizontal line\r\u0026mdash;\nHorizontal line\r***\nHorizontal line\n\r\r\r\rMath Markdown uses LaTeX to create fancy mathematical equations. There are like a billion little options and features available for math equations—you can find helpful examples of the the most common basic commands here.\nYou can use math in two different ways: inline or in a display block. To use math inline, wrap it in single dollar signs, like \\$y = mx + b\\$:\n\r\r\r\rType…\r…to get\r\r\r\rBased on the DAG, the regression model for\restimating the effect of education on wages\ris \u0026dollar;\\hat{y} = \\beta_0 + \\beta_1 x_1 + \\epsilon\u0026dollar;, or\r\u0026dollar;\\text{Wages} = \\beta_0 + \\beta_1 \\text{Education} + \\epsilon\u0026dollar;.\rBased on the DAG, the regression model for\restimating the effect of education on wages\ris \\(\\hat{y} = \\beta_0 + \\beta_1 x_1 + \\epsilon\\), or\r\\(\\text{Wages} = \\beta_0 + \\beta_1 \\text{Education} + \\epsilon\\).\r\r\r\rTo put an equation on its own line in a display block, wrap it in double dollar signs, like this:\nType…\nThe quadratic equation was an important part of high school math: $$ x = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a} $$ But now we just use computers to solve for $x$. …to get…\n The quadratic equation was an important part of high school math:\n$$ x = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a} $$\nBut now we just use computers to solve for \\(x\\).\n  Because dollar signs are used to indicate math equations, you can\u0026rsquo;t just use dollar signs like normal if you\u0026rsquo;re writing about actual dollars. For instance, if you write This book costs \\$5.75 and this other costs \\$40, Markdown will treat everything that comes between the dollar signs as math, like so: \u0026ldquo;This book costs $5.75 and this other costs $40\u0026rdquo;.\nTo get around that, put a backslash (\\) in front of the dollar signs, so that This book costs \\\\\\$5.75 and this other costs \\\\\\$40 becomes \u0026ldquo;This book costs $5.75 and this other costs $40\u0026rdquo;.\nTables There are 4 different ways to hand-create tables in Markdown—I say \u0026ldquo;hand-create\u0026rdquo; because it\u0026rsquo;s normally way easier to use R to generate these things with packages like pander (use pandoc.table()) or knitr (use kable()). The two most common are simple tables and pipe tables. You should look at the full documentation here.\nFor simple tables, type…\nRight Left Center Default ------- ------ ---------- ------- 12 12 12 12 123 123 123 123 1 1 1 1 Table: Caption goes here …to get…\nRight Left Center Default\n  12 12 12 12\r123 123 123 123\r1 1 1 1\r Table: Caption goes here\nFor pipe tables, type…\n| Right | Left | Default | Center | |------:|:-----|---------|:------:| | 12 | 12 | 12 | 12 | | 123 | 123 | 123 | 123 | | 1 | 1 | 1 | 1 | Table: Caption goes here …to get…\n   Right Left Default Center     12 12 12 12   123 123 123 123   1 1 1 1    Table: Caption goes here\nFootnotes There are two different ways to add footnotes (see here for complete documentation): regular and inline.\nRegular notes need (1) an identifier and (2) the actual note. The identifier can be whatever you want. Some people like to use numbers like [^1], but if you ever rearrange paragraphs or add notes before #1, the numbering will be wrong (in your Markdown file, not in the output; everything will be correct in the output). Because of that, I prefer to use some sort of text label:\nType…\nHere is a footnote reference[^1] and here is another [^note-on-dags]. [^1]: This is a note. [^note-on-dags]: DAGs are neat. And here\u0026#39;s more of the document. …to get…\n Here is a footnote reference1 and here is another.2\nAnd here\u0026rsquo;s more of the document.\n\rThis is a note.↩︎\n\rDAGs are neat.↩︎\n\r\r\r  You can also use inline footnotes with ^[Text of the note goes here], which are often easier because you don\u0026rsquo;t need to worry about identifiers:\nType…\nCausal inference is neat.^[But it can be hard too!] …to get…\n Causal inference is neat.1\n\rBut it can be hard too!↩︎\n\r\r\r Front matter You can include a special section at the top of a Markdown document that contains metadata (or data about your document) like the title, date, author, etc. This section uses a special simple syntax named YAML (or \u0026ldquo;YAML Ain\u0026rsquo;t Markup Language\u0026rdquo;) that follows this basic outline: setting: value for setting. Here\u0026rsquo;s an example YAML metadata section. Note that it must start and end with three dashes (---).\n--- title: Title of your document date: \u0026#34;January 13, 2020\u0026#34; author: \u0026#34;Your name\u0026#34; --- You can put the values inside quotes (like the date and name in the example above), or you can leave them outside of quotes (like the title in the example above). I typically use quotes just to be safe—if the value you\u0026rsquo;re using has a colon (:) in it, it\u0026rsquo;ll confuse Markdown since it\u0026rsquo;ll be something like title: My cool title: a subtitle, which has two colons. It\u0026rsquo;s better to do this:\n--- title: \u0026#34;My cool title: a subtitle\u0026#34; --- If you want to use quotes inside one of the values (e.g. your document is An evaluation of \u0026quot;scare quotes\u0026quot;), you can use single quotes instead:\n--- title: \u0026#39;An evaluation of \u0026#34;scare quotes\u0026#34;\u0026#39; --- Citations One of the most powerful features of Markdown + pandoc is the ability to automatically cite things and generate bibliographies. to use citations, you need to create a BibTeX file (ends in .bib) that contains a database of the things you want to cite. You can do this with bibliography managers designed to work with BibTeX directly (like BibDesk on macOS), or you can use Zotero (macOS and Windows) to export a .bib file. You can download an example .bib file of all the readings from this class here.\nComplete details for using citations can be found here. In brief, you need to do three things:\n  Add a bibliography: entry to the YAML metadata:\n--- title: Title of your document date: \u0026#34;January 13, 2020\u0026#34; author: \u0026#34;Your name\u0026#34; bibliography: name_of_file.bib ---   Choose a citation style based on a CSL file. The default is Chicago author-date, but you can choose from 2,000+ at this repository. Download the CSL file, put it in your project folder, and add an entry to the YAML metadata (or provide a URL to the online version):\n--- title: Title of your document date: \u0026#34;January 13, 2020\u0026#34; author: \u0026#34;Your name\u0026#34; bibliography: name_of_file.bib csl: \u0026#34;https://raw.githubusercontent.com/citation-style-language/styles/master/apa.csl\u0026#34; --- Some of the most common CSLs are:\n Chicago author-date Chicago note-bibliography Chicago full note-bibliography (no shortened notes or ibids) APA 7th edition MLA 8th edition    Cite things in your document. Check the documentation for full details of how to do this. Essentially, you use @citationkey inside square brackets ([]):\n   Type… …to get…     Causal inference is neat [@Rohrer:2018; @AngristPischke:2015]. Causal inference is neat (Rohrer 2018; Angrist and Pischke 2015).   Causal inference is neat [see @Rohrer:2018, p. 34; also @AngristPischke:2015, chapter 1]. Causal inference is neat (see Rohrer 2018, 34; also Angrist and Pischke 2015, chap. 1).   Angrist and Pischke say causal inference is neat [-@AngristPischke:2015; see also @Rohrer:2018]. Angrist and Pischke say causal inference is neat (2015; see also Rohrer 2018).   @AngristPischke:2015 [chapter 1] say causal inference is neat, and @Rohrer:2018 agrees. Angrist and Pischke (2015, chap. 1) say causal inference is neat, and Rohrer (2018) agrees.    After compiling, you should have a perfectly formatted bibliography added to the end of your document too:\n Angrist, Joshua D., and Jörn-Steffen Pischke. 2015. Mastering ’Metrics: The Path from Cause to Effect. Princeton, NJ: Princeton University Press.\nRohrer, Julia M. 2018. “Thinking Clearly About Correlations and Causation: Graphical Causal Models for Observational Data.” Advances in Methods and Practices in Psychological Science 1 (1): 27–42. https://doi.org/10.1177/2515245917745629.\n   Other references These websites have additional details and examples and practice tools:\n CommonMark\u0026rsquo;s Markdown tutorial: A quick interactive Markdown tutorial. Markdown tutorial: Another interactive tutorial to practice using Markdown. Markdown cheatsheet: Useful one-page reminder of Markdown syntax. The Plain Person’s Guide to Plain Text Social Science: A comprehensive explanation and tutorial about why you should write data-based reports in Markdown.  ","date":1578873600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1578873600,"objectID":"50f208c66dd6a2ac7e263653db4153ee","permalink":"/resource/markdown/","publishdate":"2020-01-13T00:00:00Z","relpermalink":"/resource/markdown/","section":"resource","summary":"Markdown is a special kind of markup language that lets you format text with simple syntax. You can then use a converter program like pandoc to convert Markdown into whatever format you want: HTML, PDF, Word, PowerPoint, etc.","tags":null,"title":"Using Markdown","type":"docs"},{"authors":null,"categories":null,"content":"R Markdown is regular Markdown with R code and output sprinkled in. You can do everything you can with regular Markdown, but you can incorporate graphs, tables, and other R output directly in your document. You can create HTML, PDF, and Word documents, PowerPoint and HTML presentations, websites, books, and even interactive dashboards with R Markdown. This whole course website is created with R Markdown (and a package named blogdown).\nThe documentation for R Markdown is extremely comprehensive, and their tutorials and cheatsheets are excellent—rely on those.\nHere are the most important things you\u0026rsquo;ll need to know about R Markdown in this class:\nKey terms   Document: A Markdown file where you type stuff\n  Chunk: A piece of R code that is included in your document. It looks like this:\n```{r} # Code goes here ``` There must be an empty line before and after the chunk. The final three backticks must be the only thing on the line—if you add more text, or if you forget to add the backticks, or accidentally delete the backticks, your document will not knit correctly.\n  Knit: When you \u0026ldquo;knit\u0026rdquo; a document, R runs each of the chunks sequentially and converts the output of each chunk into Markdown. R then runs the knitted document through pandoc to convert it to HTML or PDF or Word (or whatever output you\u0026rsquo;ve selected).\nYou can knit by clicking on the \u0026ldquo;Knit\u0026rdquo; button at the top of the editor window, or by pressing ⌘⇧K on macOS or control + shift + K on Windows.\n  Add chunks There are three ways to insert chunks:\n  Press ⌘⌥I on macOS or control + alt + I on Windows\n  Click on the \u0026ldquo;Insert\u0026rdquo; button at the top of the editor window\n  Manually type all the backticks and curly braces (don\u0026rsquo;t do this)\n  Chunk names You can add names to chunks to make it easier to navigate your document. If you click on the little dropdown menu at the bottom of your editor in RStudio, you can see a table of contents that shows all the headings and chunks. If you name chunks, they\u0026rsquo;ll appear in the list. If you don\u0026rsquo;t include a name, the chunk will still show up, but you won\u0026rsquo;t know what it does.\nTo add a name, include it immediately after the {r in the first line of the chunk. Names cannot contain spaces, but they can contain underscores and dashes. All chunk names in your document must be unique.\n```{r name-of-this-chunk} # Code goes here ``` Chunk options There are a bunch of different options you can set for each chunk. You can see a complete list in the RMarkdown Reference Guide or at knitr\u0026rsquo;s website.\nOptions go inside the {r} section of the chunk:\n```{r name-of-this-chunk, warning=FALSE, message=FALSE} # Code goes here ``` The most common chunk options are these:\n fig.width=5 and fig.height=3 (or whatever number you want): Set the dimensions for figures echo=FALSE: The code is not shown in the final document, but the results are message=FALSE: Any messages that R generates (like all the notes that appear after you load a package) are omitted warning=FALSE: Any warnings that R generates are omitted include=FALSE: The chunk still runs, but the code and results are not included in the final document  You can also set chunk options by clicking on the little gear icon in the top right corner of any chunk:\nInline chunks You can also include R output directly in your text, which is really helpful if you want to report numbers from your analysis. To do this, use `r r_code_here`.\nIt\u0026rsquo;s generally easiest to calculate numbers in a regular chunk beforehand and then use an inline chunk to display the value in your text. For instance, this document…\n```{r find-avg-mpg, echo=FALSE} avg_mpg \u0026lt;- mean(mtcars$mpg) ``` The average fuel efficiency for cars from 1974 was `r round(avg_mpg, 1)` miles per gallon. … would knit into this:\n The average fuel efficiency for cars from 1974 was 20.1 miles per gallon.\n Output formats You can specify what kind of document you create when you knit in the YAML front matter.\ntitle: \u0026#34;My document\u0026#34; output: html_document: default pdf_document: default word_document: default You can also click on the down arrow on the \u0026ldquo;Knit\u0026rdquo; button to choose the output and generate the appropriate YAML. If you click on the gear icon next to the \u0026ldquo;Knit\u0026rdquo; button and choose \u0026ldquo;Output options\u0026rdquo;, you change settings for each specific output type, like default figure dimensions or whether or not a table of contents is included.\nThe first output type listed under output: will be what is generated when you click on the \u0026ldquo;Knit\u0026rdquo; button or press the keyboard shortcut (⌘⇧K on macOS; control + shift + K on Windows). If you choose a different output with the \u0026ldquo;Knit\u0026rdquo; button menu, that output will be moved to the top of the output section.\nThe indentation of the YAML section matters, especially when you have settings nested under each output type. Here\u0026rsquo;s what a typical output section might look like:\n--- title: \u0026#34;My document\u0026#34; author: \u0026#34;My name\u0026#34; date: \u0026#34;January 13, 2020\u0026#34; output: html_document: toc: yes fig_caption: yes fig_height: 8 fig_width: 10 pdf_document: latex_engine: xelatex  # More modern PDF typesetting engine toc: yes word_document: toc: yes fig_caption: yes fig_height: 4 fig_width: 5 --- ","date":1578873600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1578873600,"objectID":"4c95f77a8fb69d2c5ee2e6b67bcae08b","permalink":"/resource/rmarkdown/","publishdate":"2020-01-13T00:00:00Z","relpermalink":"/resource/rmarkdown/","section":"resource","summary":"R Markdown is regular Markdown with R code and output sprinkled in. You can do everything you can with regular Markdown, but you can incorporate graphs, tables, and other R output directly in your document.","tags":null,"title":"Using R Markdown","type":"docs"},{"authors":null,"categories":null,"content":"\r\r\rClases (): Esta página contiene las presentaciones, sesiones grabadas y lecturas correspondientes al tema.\n\rPrácticos (): Esta página contiene los ejercicios prácticos asociados a cada sesión de clase. Podrás encontrar tanto el código de R cómo, en algunos casos, los videos grabados del live coding. Esta página será muy importante para la entrega de las tareas.\n\rTareas (): Esta página contiene las instrucciones para cada tarea. Las tareas pueden ser entregadas hasta el día viernes a las 23.59 de la semana correspondiente.\n\r\rPuedes suscribir la planificación del curso con la URL del calendario en Outlook, Google Calendar o Apple Calendar:\n \r\r\rAnálisis Estadístico en R\rClases\rPrácticos\rTareas\r\r9-16 de agosto\rUnidad 1. Elementos y herramientas básicos de R\r\r\r\r\r\r\r\r9 Agosto\n(Sesión 1)\r1.1 R enviroment: interfaz de RStudio, elementos de script, workspace\n1.2 Prácticas y herramientas de consulta: CRAN, stackoverflow, Rcommunity\n1.3 Herramientas para la colaboración y comunicación: Rprojects, GitHub y Slack\r\r\r\r\r16 Agosto\n(Sesión 2)\r1.4 Librerías y funciones para las ciencias sociales: tidyverse y sj (sjmisc y sjPlot)\n1.5 Construcción de reportes reproducibles e integrados con código: Rmarkdown\r\r\r\r\r23 Agosto-6 Septiembre\rUnidad 2. Manipulación y limpieza de datos\r\r\r\r\r\r\r\r23 Agosto\n(Sesión 3)\r2.1 Importar/exportar datos en diferentes formatos\n2.2 Validación y limpieza de variables\r\r\r\r\r\r30 Agosto\n(Sesión 4)\r2.3 Transformación y selección de variables\r\r\r\r\r\r\r6 Septiembre\n(Sesión 5)\r2.4 Transformación de datos en tidydata\r\r\r\r\r13-20 Septiembre\rUnidad 3. Análisis estadístico descriptivo en R\r\r\r\r\r\r\r\r13 Septiembre\n(Sesión 6)\r3.1 Análisis descriptivos univariados: medidas de tendencia central, dispersión y frecuencias\n3.2 Análisis bivariado: tablas de contingencia, correlaciones y ANOVA\r\r\r\r\r20 Septiembre\rSemana de receso\r\r\r\r\r\r\r\r27 Septiembre-8 Noviembre\rUnidad 4. Análisis estadístico inferencial en R\r\r\r\r\r\r\r\r27 Septiembre\n(Sesión 7)\r4.1 Muestras complejas y precisión de inferencia estadística\r\r\r\r\r4 Octubre\n(Sesión 8)\r4.2 Pruebas de hipótesis y representación gráfica\r\r\r\r\r\r11 Octubre\rFeriado\r\r\r\r\r\r\r\r18 Octubre\n(Sesión 9)\r4.3 Regresiones lineales, predictores categóricos y representación gráfica\r\r\r\r\r\r25 Octubre\n(Sesión 10)\r4.4 Regresiones logísticas, exponenciación y representación gráfica\r\r\r\r\r1 Noviembre\rFeriado\r\r\r\r\r\r\r\r8 Noviembre\n(Sesión 11)\r4.5 Calidad de modelos y otras técnicas de estimación\r\r\r\r\r\r15 Noviembre\rReceso\r\r\r\r\r\r\r\r22 Noviembre\r Entrega final\r\r\r\r\r\r\r\rResumen Evaluaciones\rClases\rPrácticos\rTareas\r\r9 Agosto-13 Agosto\r Tarea 0: Colaborando en Github\r\r\r\r\r\r\r16 Agosto-20 Agosto\r Tarea 1: Reporte en Rmarkdown\r\r\r\r\r\r\r6 Septiembre-10 septiembre\r Tarea 2: Procesamiento de datos\r\r\r\r\r\r\r13 Septiembre-17 septiembre\r Tarea 3: Análisis descriptivo\r\r\r\r\r\r\r27 Septiembre-1 octubre\r Tarea 4: Muestras complejas\r\r\r\r\r\r\r25 octubre-29 octubre\r Tarea 5: Regresiones en R\r\r\r\r\r\r\r22 Noviembre\r Entrega investigación final\r\r\r\r\r\r\r\r\r","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"3e223d7ba58b0122b42458e4cf52e04c","permalink":"/schedule/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/schedule/","section":"","summary":"Clases (): Esta página contiene las presentaciones, sesiones grabadas y lecturas correspondientes al tema.\n\rPrácticos (): Esta página contiene los ejercicios prácticos asociados a cada sesión de clase.","tags":null,"title":"Schedule","type":"page"},{"authors":null,"categories":null,"content":"\r\r\rPresentación\r\rDescripción\rObjetivos\r\rObjetivo general\rObjetivos específicos\r\r\rI. Contenidos\r\rUnidad 1. Elementos y herramientas básicos de R\rUnidad 2. Manipulación y limpieza de datos\rUnidad 3. Análisis estadístico descriptivo en R\rUnidad 4. Análisis estadístico inferencial en R\r\rII. Metodología del curso\r\rRecursos principales de aprendizaje\r\rIII. Evaluación de aprendizajes\rIV. Calendario de actividades\rV. Recursos pedagógicos\r\r1. Referencias bibliográficas\r2. Sitios de consulta\r\rDescargar programa en pdf\r\r\rDocente\r Valentina Andrade\rvalentinaandrade.netlify.app/\r valentinaandrade@uchile.cl\r @valentiandrade\r Agendar reunión\r\r\rCourse details\r Clases: Lunes 16.30-17.40\r Práctico: Lunes 18.00-19.20\r Online, en link de Zoom Slack\r\r\rContacto\rEl canal de contacto de preferencia es Slack, y en segunda instancia, email.\n\r\rPresentación\rR es uno de los lenguajes de programación estadística más populares y demandados del último tiempo. Una de las razones principales es que tanto el software como la comunidad que hace uso de este se maneja en un marco de la ciencia abierta (r Open Science Tools o rOpensci), permitiendo el desarrollo constante de una serie de herramientas que permiten mejorar nuestras investigaciones, de manera libre, gratis y colaborativa.\nPara las ciencias sociales estos elementos no son triviales. Es frecuente encontrarse con personas de nuestro campo que se están iniciando en R y desisten de este intento dada la frustración y pensamientos de incapacidad. Aprender R puede ser difícil al iniciar – es como aprender cualquier otro lenguaje o idioma -, y poco se ha pensado en cómo implementar enseñanza a grupos más expuestos a estos prejuicios, como en las ciencias sociales y humanidades.\nSe propone que un curso de introducción de R para análisis de datos sociales puede abordar estos desafíos basado en enseñar herramientas concretas que faciliten y potencien las investigaciones sociales. Basada en la experiencia de enseñanza de R en contextos similares, esto implica introducir prácticas del rOpensci como la colaboración y reproducibilidad, que facilitan sin duda el aprendizaje colectivo y la calidad de los productos desarrollados durante el curso.\n¡Iniciemos este desafío juntas/os!\nDescripción\rEste curso busca introducir a estudiante en el uso de R y RStudio para el análisis de datos sociales. En ese sentido, será una introducción al entorno R y a la interfaz de RStudio para su uso en contextos académicos. Al mismo tiempo, el curso profundiza en temas específicos para las ciencias sociales como el análisis estadístico y la presentación de resultados (visualización de datos).\n\rObjetivos\rObjetivo general\rAprender las principales herramientas para el análisis de datos sociales en RStudio, con el fin de lograr un uso autónomo del software\n\rObjetivos específicos\rEl curso no es un curso de programación en R , sino que una aplicación concreta del uso de R para ciencias sociales. Por ello, el curso tiene como propósito específico que los estudiantes sean capaces de\nManejar R y herramientas asociadas a su utilización (rOpensci), utilizando prácticas que les permitan avanzar en su aprendizaje de manera autónoma (Unidad 1)\n\rManipular, procesar y limpiar datos sociales utilizando R (Unidad 2)\n\rAplicar herramientas para análisis estadísticos descriptivos en R (Unidad 3)\n\rAplicar herramientas para análisis estadísticos inferenciales en R (Unidad 4)\n\rPresentar de resultados, a partir de la visualización de datos y construcción de documentos (transversal a todas las unidades)\n\r\r\r\r\rI. Contenidos\rUnidad 1. Elementos y herramientas básicos de R\r1.1 R enviroment: interfaz de RStudio, elementos de script, workspace\n1.2 Prácticas y herramientas de consulta: CRAN, stackoverflow, Rcommunity\n1.3 Herramientas para la colaboración y comunicación: Rprojects, GitHub y Slack\n1.4 Librerías y funciones para las ciencias sociales: tidyverse y sj (sjmisc y sjPlot)\n1.5 Construcción de reportes reproducibles e integrados con código: RMarkdown\n\rUnidad 2. Manipulación y limpieza de datos\r2.1 Importar y exportar datos en diferentes formatos\n2.2 Validación y limpieza de variables (missing values)\n2.3 Transformación y selección de variables\n2.4 Transformación de datos en tidydata con tidyr\n\rUnidad 3. Análisis estadístico descriptivo en R\r3.1 Análisis descriptivos univariados: medidas de tendencia central, dispersión y frecuencias\n3.2 Análisis descriptivo bivariado: tablas de contingencia, correlaciones y ANOVA\n3.3 Representación gráfica con sjPlot: Likert, proporciones agrupadas y distribuciones\n\rUnidad 4. Análisis estadístico inferencial en R\r4.1 Muestras complejas y precisión de inferencia estadística con survey y srvyr\n4.2 Pruebas de hipótesis y representación gráfica\n4.3 Regresiones lineales, predictores categóricos y representación gráfica\n4.4 Regresiones logísticas, exponenciación y representación gráfica\n4.5 Ajuste de modelos (performance) y otras representaciones gráficas (predicción, efectos marginales e interacciones)\n\r\rII. Metodología del curso\rDado el contexto de pandemia se tendrán tres espacios principales de aprendizaje:\nSesiones de clases lectivas (), donde se presentarán los aspectos centrales de los contenidos correspondientes a la semana vía Zoom. Tanto el documento de presentación como el video de la clase se encontrará disponible en la pestaña de Clases de este sitio web del curso.\n\rPrácticas guiadas (): cada tema de las sesiones se acompaña de una guía práctica de aplicación de contenidos, y que estarán disponibles en la pestaña Prácticos. Estas guías están diseñadas para ser desarrolladas de manera autónoma por cada estudiante semana a semana. También serán desarrolladas y revisadas cada semana en grupos pequeños con supervisión de ayudantes para dar mayor oportunidad de participación y resolver las dudas respectivas. Existe un reporte de progreso asociado a estas guías que deberá ser completado semanalmente con fines de monitoreo y retroalimentación.\n\rTareas: se desarrollarán 6 tareas que les permitirán aplicar contenidos y replicar lo aprendido en los prácticos en base a una base de datos seleccionada por ustedes al inicio de semestre. Esto permitirá no solo recibir retroalimentación constante, sino que aprender con datos que puedan ser útiles para otros proyectos de investigación que sean de su interés.\n\r\rRecursos principales de aprendizaje\r1. Sitio web\nEl curso tiene disponible este sitio web, que he programado pues permite integrar texto y código de R, junto con hacer interactuar con otras plataformas como el foro Disqus y Github.\n2. R, RStudio y RStudio Cloud \nEl software que se utilizará principalmente será R y su interfaz RStudio. Ahora bien, muchos usuario/as de R presentan problemas de instalación dada la capacidad de sus computadores y sistemas operativos. Por ello se promoverá el uso del servicio gratuito de RStudio.cloud , que permite ejecutar la interfaz de RStudio en el navegador web y compartir el código de manera sincrónica con la docente. Dado el enfoque rOpensci, las plantillas para ejemplos, ejercicios y mini proyectos podrán ser implementados en esta plataforma. Si bien no es ideal pues tiene limitantes de memoria, mientras ustedes asimilan el programa será una buena herramienta.\n3. Slack \nSlack es una herramienta de uso frecuente en equipos de trabajo que utilizan R pues permite integrar script (o código) de distintos lenguajes en el chat. Se tendrá un espacio de trabajo en la app Slack que permite que cualquier persona del curso pueda hacer preguntas y cualquiera pueda responder. Esta es una de las prácticas que se promoverán en el curso pues es probable que los estudiantes tengan dudas similares a las de sus compañeros, por lo que las respuestas de la docente, ayudante y otros compañeros serán de libre disposición de todo el curso. Dentro del Slack se tendrán canales para hacer preguntas sobre las sesiones, tareas y proyectos, y el link que permite unirse a este estará disponible en el sitio del curso.\n Guía de uso de slack\n Unirse a slack de curso\n4. GitHub \nGithub es una plataforma online que permite depositar archivos y el control de versiones (VCS), por lo que se ha transformado una herramienta fácil y popular para corregir, colaborar y compartir códigos de distintos lenguajes (no solo R). Utilizaremos esta plataforma para subir las tareas, ayudarlos/as de manera directa con su código y darles feedback.\n5. Zotero \nZotero es un gestor bibliográfico que permite sistematizar las referencias y archivos utilizados en una investigación o informe. Enseñaremos este de manera complementaria pues este software se puede integrar en los documentos escritos hechos en R.\n\r\rIII. Evaluación de aprendizajes\rLas evaluaciones del curso se componen de tareas (70% de la nota final) y la entrega de una investigación (30% de la nota final), en dónde en ambos casos la/el estudiante deberá seleccionar datos y temas de interés de modo de acercar la aplicación del software a contextos de investigación propios de la/el estudiante. En concreto, cada evaluación consiste en:\n1. Tareas (70% de la nota final): consisten en evaluaciones parciales temáticas que buscan poner en práctica los aprendizajes expuestos en la sesión de clases y herramientas reforzadas en los prácticos. El promedio de notas las de tareas será calculado solo con las cuatro mejores entregas a partir de la Tarea 1 (25% c/u)1.\n2. Investigación final (30% de la nota final): consiste en una evaluación final que aplica los conocimientos y herramientas entregadas a lo largo de curso, a un proyecto de investigación de elección por el/la estudiante.\n\rIV. Calendario de actividades\rEl calendario de actividades se puede revisar con detención en la pestaña planificación. Un resumen breve de las tareas es\n\r\rEvaluación\rFormato\rFecha\rPonderación Nota Final\r\r\r\rTarea\r4 tareas2\rHasta Viernes de la semana informada\r70% (25% c/u)\r\rInvestigación\rÚnica entrega\r22 de noviembre\r30%\r\r\r\r\rV. Recursos pedagógicos\r1. Referencias bibliográficas\r\rWickham, H., \u0026amp; Grolemund, G. (2016). R for data science: import, tidy, transform, visualize, and model data (First edition). Sebastopol: O’Reilly. Libro con enfoque en el aprendizaje de R. Disponible en español como “R para ciencia de datos”\n\rDaniel Lüdecke (2021) Data Visualization for Statistics in Social Science R package version 2.8.7\n\rWickham et al., (2019). Welcome to the tidyverse. Journal of Open Source Software, 4(43), 1686\n\rYihui Xie, J. J. Allaire, Garrett Grolemund (2021) R Markdown: The Definitive Guide\n\rBryan, Jenny (2019) Happy Git in R\n\r\r\r2. Sitios de consulta\r\rrOpensci (R Open Scicnce Tools)\rLaboratorio de Ciencia Social Abierta, Centro de Estudios de Conflicto y Cohesión Social (LISA-COES)\rStackoverflow\rRStudio Community\rRMarkdown\rsjPlot\rtidyverse\r\r\r\rDescargar programa en pdf\r\n\r\rEs decir, a lo largo del semestre deberá entregar 6 tareas: 5 calificadas (Tarea 1 a Tarea 5) y 1 no (Tarea 0). De las 5 tareas restantes, solo las 4 mejores serán consideradas en su promedio de las tareas.↩︎\n\rEs decir, a lo largo del semestre deberá entregar 6 tareas: 5 calificadas (Tarea 1 a Tarea 5) y 1 no (Tarea 0). De las 5 tareas restantes, solo las 4 mejores serán consideradas en su promedio de las tareas.↩︎\n\r\r\r","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"e4d5a4a79239f08c6ad0d7cbf1be756c","permalink":"/syllabus/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/syllabus/","section":"","summary":"Presentación\r\rDescripción\rObjetivos\r\rObjetivo general\rObjetivos específicos\r\r\rI. Contenidos\r\rUnidad 1. Elementos y herramientas básicos de R\rUnidad 2. Manipulación y limpieza de datos\rUnidad 3.","tags":null,"title":"Syllabus","type":"page"},{"authors":null,"categories":null,"content":"Every week, after you finish working through the content, I want to hear about what you learned and what questions you still have. To facilitate this, and to encourage engagement with the course content, you\u0026rsquo;ll need to fill out a short response on iCollege. This should be ≈150 words. That\u0026rsquo;s fairly short: there are ≈250 words on a typical double-spaced page in Microsoft Word (500 when single-spaced).\nYou should answer these two questions each week:\n What was the most exciting thing you learned from the session? Why? What was the muddiest thing from the session this week? What are you still wondering about?  I will grade these check-ins using a check system:\n ✔+: (11.5 points (115%) in gradebook) Response shows phenomenal thought and engagement with the course content. I will not assign these often. ✔: (10 points (100%) in gradebook) Response is thoughtful, well-written, and shows engagement with the course content. This is the expected level of performance. ✔−: (5 points (50%) in gradebook) Response is hastily composed, too short, and/or only cursorily engages with the course content. This grade signals that you need to improve next time. I will hopefully not assign these often.  Notice that is essentially a pass/fail or completion-based system. I\u0026rsquo;m not grading your writing ability, I\u0026rsquo;m not counting the exact number of words you\u0026rsquo;re writing, and I\u0026rsquo;m not looking for encyclopedic citations of every single reading to prove that you did indeed read everything. I\u0026rsquo;m looking for thoughtful engagement, that\u0026rsquo;s all. Do good work and you\u0026rsquo;ll get a ✓.\nYou will submit these responses via iCollege.\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"3f10dcc1a092351371781cc2f093e144","permalink":"/assignment/weekly-check-in/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/assignment/weekly-check-in/","section":"assignment","summary":"Every week, after you finish working through the content, I want to hear about what you learned and what questions you still have. To facilitate this, and to encourage engagement with the course content, you\u0026rsquo;ll need to fill out a short response on iCollege.","tags":null,"title":"Weekly check-in","type":"docs"}]